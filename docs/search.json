[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "介绍",
    "section": "",
    "text": "0.1 介绍\n《用 Python 介绍数据科学》课程草稿书\n这本书是由 The GRAPH Courses 提供的一门为期 3 个月的在线课程的课程笔记汇编。要访问课程视频、练习文件和在线测验，请访问我们的网站 thegraphcourses.org。\nThe GRAPH Courses 是 Global Research and Analyses for Public Health (GRAPH) Network 的一个项目，该网络是一个非营利组织，致力于通过经济实惠的现场训练营和免费的自学课程让编程和数据技能变得触手可及。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>介绍</span>"
    ]
  },
  {
    "objectID": "index.html#贡献者",
    "href": "index.html#贡献者",
    "title": "介绍",
    "section": "0.2 贡献者",
    "text": "0.2 贡献者\n我们非常感谢以下多年来为这些资料开发做出贡献的各位：\nAmanda McKinley, Andree Valle Campos, Aziza Merzouki, Benedict Nguimbis, Bennour Hsin, Camille Beatrice Valera, Daniel Camara, Eduardo Araujo, Elton Mukonda, Guy Wafeu, Imad El Badisy, Imane Bensouda Korachi, Joy Vaz, Kene David Nwosu, Lameck Agasa, Laure Nguemo, Laure Vancauwenberghe, Matteo Franza, Michal Shrestha, Olivia Keiser, Sabina Rodriguez Velasquez, Sara Botero Mesa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>介绍</span>"
    ]
  },
  {
    "objectID": "index.html#合作伙伴与资助方",
    "href": "index.html#合作伙伴与资助方",
    "title": "介绍",
    "section": "0.3 合作伙伴与资助方",
    "text": "0.3 合作伙伴与资助方\n\n日内瓦大学\n牛津大学\n世界卫生组织\n全球基金\nErnst Goehner 基金会",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>介绍</span>"
    ]
  },
  {
    "objectID": "index.html#欢迎视频",
    "href": "index.html#欢迎视频",
    "title": "介绍",
    "section": "0.4 欢迎视频",
    "text": "0.4 欢迎视频",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>介绍</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html",
    "href": "p_foundations_google_colab.html",
    "title": "2  Introduction to Google Colab",
    "section": "",
    "text": "2.1 Learning Objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#learning-objectives",
    "href": "p_foundations_google_colab.html#learning-objectives",
    "title": "2  Introduction to Google Colab",
    "section": "",
    "text": "Understand what Google Colab is and its advantages for data science and AI\nLearn how to access and navigate Google Colab\nCreate and manage notebooks in Google Colab\nRun Python code in Colab cells\nUse text cells for explanations and formatting\nImport and use pre-installed libraries for data analysis\nImport and use data to perform analysis\nShare Colab notebooks",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#introduction",
    "href": "p_foundations_google_colab.html#introduction",
    "title": "2  Introduction to Google Colab",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nGoogle Collaboratory, or Colab for short, is a free online platform that allows you to work with Python or R code in your browser. It’s a great way to get started with Python, as you don’t have to install anything on your computer.\nSome limitations if you’re running heavy workload though. Can get timeout. But for beginner data analysts, it’s perfect and free.\nNote that this document is a summary of the video, rather than a replacement. You should watch the video for a more complete experience.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#getting-started-with-colab",
    "href": "p_foundations_google_colab.html#getting-started-with-colab",
    "title": "2  Introduction to Google Colab",
    "section": "2.3 Getting Started With Colab",
    "text": "2.3 Getting Started With Colab\n\nSearch for “Google Colab” in your favorite search engine.\nUsually the first result will be the correct one. Currently, it’s colab.research.google.com, but it may change in the future.\nSign in with your Google account (create a Gmail account if you don’t have one)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#creating-and-managing-notebooks",
    "href": "p_foundations_google_colab.html#creating-and-managing-notebooks",
    "title": "2  Introduction to Google Colab",
    "section": "2.4 Creating and Managing Notebooks",
    "text": "2.4 Creating and Managing Notebooks\n\nNotebooks are the main way to organize work in Colab. They contain code cells and text cells.\nCreate a new notebook: File &gt; New Notebook\nRename your notebook for better organization",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#working-with-code-cells",
    "href": "p_foundations_google_colab.html#working-with-code-cells",
    "title": "2  Introduction to Google Colab",
    "section": "2.5 Working With Code Cells",
    "text": "2.5 Working With Code Cells\n\nThere should be a button at the top left that allows you to add code or text cells\nCode cells are where you write and execute Python code\nType ``1 + 1 in a cell then run it\nRun a cell by clicking the play button or using keyboard shortcuts:\n\nCommand + Enter on Mac or Ctrl + Enter on Windows: Run the current cell\nShift + Enter: Run the current cell and create a new one below\n\nTry to get comfortable with the keyboard shortcuts\nYour code cell may take a while to run the first time, as the Python engine needs to be initialized\nWhen you run a cell, the final output is displayed below the cell\nTo see multiple outputs, explicitly print them with print() For example:\n\nprint(1)\nprint(2)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#text-cells",
    "href": "p_foundations_google_colab.html#text-cells",
    "title": "2  Introduction to Google Colab",
    "section": "2.6 Text Cells",
    "text": "2.6 Text Cells\n\nUse text cells for explanations and titles\nThe toolbar above the text cell allows you to format text, but pay attention to the generated markdown",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#example-of-working-with-data",
    "href": "p_foundations_google_colab.html#example-of-working-with-data",
    "title": "2  Introduction to Google Colab",
    "section": "2.7 Example of Working With Data",
    "text": "2.7 Example of Working With Data\n\nClick on the Files tab to see the “sample_data” folder\nImport the California housing test dataset:\n\nimport pandas\nhousing_data = pandas.read_csv(\"/content/sample_data/california_housing_test.csv\")\nhousing_data.describe()\n\nView the dataset by typing housing_data in a cell and running it",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#practice-q-importing-data",
    "href": "p_foundations_google_colab.html#practice-q-importing-data",
    "title": "2  Introduction to Google Colab",
    "section": "2.8 Practice Q: Importing Data",
    "text": "2.8 Practice Q: Importing Data\n\nImport the “california_housing_train” dataset\nUse the describe() function to get a summary of the dataset",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#getting-data-from-your-drive",
    "href": "p_foundations_google_colab.html#getting-data-from-your-drive",
    "title": "2  Introduction to Google Colab",
    "section": "2.9 Getting Data From Your Drive",
    "text": "2.9 Getting Data From Your Drive\n\nIn the Files tab, click the button to mount your drive\nCreate a folder and upload a CSV file from your computer to the folder\nWe can import the data with pandas as before",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#practice-q-importing-data-1",
    "href": "p_foundations_google_colab.html#practice-q-importing-data-1",
    "title": "2  Introduction to Google Colab",
    "section": "2.10 Practice Q: Importing Data",
    "text": "2.10 Practice Q: Importing Data\n\nImport a CSV file you uploaded to your drive\nUse the describe() function to get a summary of the dataset",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#where-is-your-notebook-saved",
    "href": "p_foundations_google_colab.html#where-is-your-notebook-saved",
    "title": "2  Introduction to Google Colab",
    "section": "2.11 Where Is Your Notebook Saved?",
    "text": "2.11 Where Is Your Notebook Saved?\n\nAll work is automatically saved to your Google Drive\nAccess your notebooks at drive.google.com in the “Colab Notebooks” folder",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#sharing-and-collaborating",
    "href": "p_foundations_google_colab.html#sharing-and-collaborating",
    "title": "2  Introduction to Google Colab",
    "section": "2.12 Sharing and Collaborating",
    "text": "2.12 Sharing and Collaborating\n\nShare notebooks with a link, giving viewer or editor access\nAccess notebooks later from your Google Drive\nDownload notebooks in various formats (ipynb, py)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#conclusion",
    "href": "p_foundations_google_colab.html#conclusion",
    "title": "2  Introduction to Google Colab",
    "section": "2.13 Conclusion",
    "text": "2.13 Conclusion\nGoogle Colab provides a powerful, accessible platform for data science and AI projects. Its pre-configured environment and easy sharing features make it an excellent way to get started with data analysis.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html",
    "href": "p_foundations_coding_basics.html",
    "title": "3  Coding basics",
    "section": "",
    "text": "3.1 Learning Objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#learning-objectives",
    "href": "p_foundations_coding_basics.html#learning-objectives",
    "title": "3  Coding basics",
    "section": "",
    "text": "You can write and use comments in Python (single-line and multi-line).\nYou know how to use Python as a calculator for basic arithmetic operations and understand the order of operations.\nYou can use the math library for more complex mathematical operations.\nYou understand how to use proper spacing in Python code to improve readability.\nYou can create, manipulate, and reassign variables of different types (string, int, float).\nYou can get user input and perform calculations with it.\nYou understand the basic rules and best practices for naming variables in Python.\nYou can identify and fix common errors related to variable usage and naming.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#introduction",
    "href": "p_foundations_coding_basics.html#introduction",
    "title": "3  Coding basics",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nIn this lesson, you will learn the basics of using Python.\nTo get started, open your preferred Python environment (e.g., Jupyter Notebook, VS Code, or PyCharm), and create a new Python file or notebook.\nNext, save the file with a name like “coding_basics.py” or “coding_basics.ipynb” depending on your environment.\nYou should now type all the code from this lesson into that file.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#comments",
    "href": "p_foundations_coding_basics.html#comments",
    "title": "3  Coding basics",
    "section": "3.3 Comments",
    "text": "3.3 Comments\nComments are text that is ignored by Python. They are used to explain what the code is doing.\nYou use the symbol #, pronounced “hash” or “pound”, to start a comment. Anything after the # on the same line is ignored. For example:\n\n# Addition\n2 + 2\n\n4\n\n\nIf we just tried to write Addition above the code, it would cause an error:\n\nAddition\n2 + 2\n\nNameError: name 'Addition' is not defined\nWe can put the comment on the same line as the code, but it needs to come after the code.\n\n2 + 2  # Addition\n\n4\n\n\nTo write multiple lines of comments, you can either add more # symbols:\n\n# Addition\n# Add two numbers\n2 + 2\n\n4\n\n\nOr you can use triple quotes ''' or \"\"\":\n\n'''\nAddition:\nBelow we add two numbers\n'''\n2 + 2\n\n4\n\n\nOr:\n\n\"\"\"\nAddition:\nBelow we add two numbers\n\"\"\"\n2 + 2\n\n4\n\n\n\n\n\n\n\n\nVocab\n\n\n\nComment: A piece of text in your code that is ignored by Python. Comments are used to explain what the code is doing and are meant for human readers.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.4 Practice Q: Commenting in Python\nWhich of the following code chunks are valid ways to comment code in Python?\n# add two numbers\n2 + 2\n2 + 2 # add two numbers\n''' add two numbers\n2 + 2\n# add two numbers 2 + 2\nCheck your answer by trying to run each code chunk.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-commenting-in-python",
    "href": "p_foundations_coding_basics.html#practice-q-commenting-in-python",
    "title": "3  Coding basics",
    "section": "3.4 Practice Q: Commenting in Python",
    "text": "3.4 Practice Q: Commenting in Python\nWhich of the following code chunks are valid ways to comment code in Python?\n# add two numbers\n2 + 2\n2 + 2 # add two numbers\n''' add two numbers\n2 + 2\n# add two numbers 2 + 2\nCheck your answer by trying to run each code chunk.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#python-as-a-calculator",
    "href": "p_foundations_coding_basics.html#python-as-a-calculator",
    "title": "3  Coding basics",
    "section": "3.5 Python as a Calculator",
    "text": "3.5 Python as a Calculator\nAs you have already seen, Python works as a calculator in standard ways.\nBelow are some other examples of basic arithmetic operations:\n\n2 - 2 # two minus two\n\n0\n\n\n\n2 * 2  # two times two \n\n4\n\n\n\n2 / 2  # two divided by two\n\n1.0\n\n\n\n2 ** 2  # two raised to the power of two\n\n4\n\n\nThere are a few other operators you may come across. For example, % is the modulo operator, which returns the remainder of the division.\n\n10 % 3  # ten modulo three\n\n1\n\n\n// is the floor division operator, which divides then rounds down to the nearest whole number.\n\n10 // 3  # ten floor division three\n\n3\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.6 Practice Q: Modulo and Floor Division\nGuess the result of the following code chunks then run them to check your answer:\n\n5 % 4\n\n\n5 // 4",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-modulo-and-floor-division",
    "href": "p_foundations_coding_basics.html#practice-q-modulo-and-floor-division",
    "title": "3  Coding basics",
    "section": "3.6 Practice Q: Modulo and Floor Division",
    "text": "3.6 Practice Q: Modulo and Floor Division\nGuess the result of the following code chunks then run them to check your answer:\n\n5 % 4\n\n\n5 // 4",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#order-of-operations",
    "href": "p_foundations_coding_basics.html#order-of-operations",
    "title": "3  Coding basics",
    "section": "3.7 Order of Operations",
    "text": "3.7 Order of Operations\nPython obeys the standard PEMDAS order of operations (Parentheses, Exponents, Multiplication, Division, Addition, Subtraction).\nFor example, multiplication is evaluated before addition, so below the result is 6.\n\n2 + 2 * 2   \n\n6\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.8 Practice Q: Evaluating Arithmetic Expressions\nWhich, if any, of the following code chunks will evaluate to 10?\n\n2 + 2 * 4\n\n\n6 + 2 ** 2",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-evaluating-arithmetic-expressions",
    "href": "p_foundations_coding_basics.html#practice-q-evaluating-arithmetic-expressions",
    "title": "3  Coding basics",
    "section": "3.8 Practice Q: Evaluating Arithmetic Expressions",
    "text": "3.8 Practice Q: Evaluating Arithmetic Expressions\nWhich, if any, of the following code chunks will evaluate to 10?\n\n2 + 2 * 4\n\n\n6 + 2 ** 2",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#using-the-math-library",
    "href": "p_foundations_coding_basics.html#using-the-math-library",
    "title": "3  Coding basics",
    "section": "3.9 Using the Math Library",
    "text": "3.9 Using the Math Library\nWe can also use the math library to do more complex mathematical operations. For example, we can use the math.sqrt function to calculate the square root of a number.\n\nimport math\nmath.sqrt(100)  # square root\n\n10.0\n\n\nOr we can use the math.log function to calculate the natural logarithm of a number.\n\nimport math\nmath.log(100)  # logarithm\n\n4.605170185988092\n\n\nmath.sqrt and math.log are examples of Python functions, where an argument (e.g., 100) is passed to the function to perform a calculation.\nWe will learn more about functions later.\n\n\n\n\n\n\nVocab\n\n\n\nFunction: A reusable block of code that performs a specific task. Functions often take inputs (called arguments) and return outputs.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\n3.10 Practice Q: Using the Math Library\nUsing the math library, calculate the square root of 81.\nWrite your code below and run it to check your answers:\n\n# Your code here\n\n\n\n3.11 Practice Q: Describing the Use of the Random Library\nConsider the following code, which generates a random number between 1 and 10:\n\nimport random\nrandom.randint(1, 10)\n\n4\n\n\nIn that code, identify the library, the function, and the argument(s) to the function.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-using-the-math-library",
    "href": "p_foundations_coding_basics.html#practice-q-using-the-math-library",
    "title": "3  Coding basics",
    "section": "3.10 Practice Q: Using the Math Library",
    "text": "3.10 Practice Q: Using the Math Library\nUsing the math library, calculate the square root of 81.\nWrite your code below and run it to check your answers:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-describing-the-use-of-the-random-library",
    "href": "p_foundations_coding_basics.html#practice-q-describing-the-use-of-the-random-library",
    "title": "3  Coding basics",
    "section": "3.11 Practice Q: Describing the Use of the Random Library",
    "text": "3.11 Practice Q: Describing the Use of the Random Library\nConsider the following code, which generates a random number between 1 and 10:\n\nimport random\nrandom.randint(1, 10)\n\n4\n\n\nIn that code, identify the library, the function, and the argument(s) to the function.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#spacing-in-code",
    "href": "p_foundations_coding_basics.html#spacing-in-code",
    "title": "3  Coding basics",
    "section": "3.12 Spacing in Code",
    "text": "3.12 Spacing in Code\nGood spacing makes your code easier to read. In Python, two simple spacing practices can greatly improve your code’s readability: using blank lines and adding spaces around operators.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#indentation",
    "href": "p_foundations_coding_basics.html#indentation",
    "title": "3  Coding basics",
    "section": "3.13 Indentation",
    "text": "3.13 Indentation\nPython uses indentation to indicate the start and end of loops, functions, and other blocks of code. We’ll look at this more in later lessons.\nFor now, one thing to watch out for is to avoid accidentally including a space before your code\nFor example, consider the following code chunk:\n\nimport math\n# Get the square root of 100\n math.sqrt(100)\n\nTrying to run this code will cause an error:\nIndentationError: unexpected indent\nThis is due to the space before the math.sqrt function. We can fix this by removing the space.\n\nimport math\n# Get the square root of 100\nmath.sqrt(100)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#blank-lines",
    "href": "p_foundations_coding_basics.html#blank-lines",
    "title": "3  Coding basics",
    "section": "3.14 Blank Lines",
    "text": "3.14 Blank Lines\nUse blank lines to separate different parts of your code.\nFor example, consider the following code chunk:\n\n# Set up numbers\nx = 5\ny = 10\n# Perform calculation\nresult = x + y\n# Display result\nprint(result)\n\n15\n\n\nWe can add blank lines to separate the different parts of the code:\n\n# Set up numbers\nx = 5\ny = 10\n\n# Perform calculation\nresult = x + y\n\n# Display result\nprint(result)\n\n15\n\n\nBlank lines help organize your code into logical sections, similar to paragraphs in writing.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#spaces-around-operators",
    "href": "p_foundations_coding_basics.html#spaces-around-operators",
    "title": "3  Coding basics",
    "section": "3.15 Spaces Around Operators",
    "text": "3.15 Spaces Around Operators\nAdding spaces around mathematical operators improves readability:\n\n# Hard to read\nx=5+3*2\n\n# Easy to read\nx = 5 + 3 * 2\n\nWhen listing items, add a space after each comma:\n\n# Hard to read\nprint(1,2,3)\n\n# Easy to read\nprint(1, 2, 3)\n\nThis practice follows the convention in written English, where we put a space after a comma. It makes lists of items in your code easier to read.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#variables-in-python",
    "href": "p_foundations_coding_basics.html#variables-in-python",
    "title": "3  Coding basics",
    "section": "3.16 Variables in Python",
    "text": "3.16 Variables in Python\nAs you have seen, to store a value for future use in Python, we assign it to a variable with the assignment operator, =.\n\nmy_var = 2 + 2  # assign the result of `2 + 2 ` to the variable called `my_var`\nprint(my_var)  # print my_var\n\n4\n\n\nNow that you’ve created the variable my_var, Python knows about it and will keep track of it during this Python session.\nYou can open your environment to see what variables you have created. This looks different depending on your IDE.\nSo what exactly is a variable? Think of it as a named container that can hold a value. When you run the code below:\n\nmy_var = 20\n\nyou are telling Python, “store the number 20 in a variable named ‘my_var’”.\nOnce the code is run, we would say, in Python terms, that “the value of variable my_var is 20”.\nTry to come up with a similar sentence for this code chunk:\n\nfirst_name = \"Joanna\"\n\nAfter we run this code, we would say, in Python terms, that “the value of the first_name variable is Joanna”.\n\n\n\n\n\n\nVocab\n\n\n\nA text value like “Joanna” is called a string, while a number like 20 is called an integer. If the number has a decimal point, it is called a float, which is short for “floating-point number”.\nBelow are these three types of variables:\n\n# string variable\nfirst_name = \"Joanna\"\n\n# integer variable\nage = 5\n\n# float variable\nheight = 1.4\n\nYou can check the type of a variable using the type() function.\n\nprint(type(first_name))\nprint(type(age))\nprint(type(height))\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\n\n\n\n\n\n\n\n\nVocab\n\n\n\nVariable: A named container that can hold a value. In Python, variables can store different types of data, including numbers, strings, and more complex objects.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#reassigning-variables",
    "href": "p_foundations_coding_basics.html#reassigning-variables",
    "title": "3  Coding basics",
    "section": "3.17 Reassigning Variables",
    "text": "3.17 Reassigning Variables\nReassigning a variable is like changing the contents of a container.\nFor example, previously we ran this code to store the value “Joanna” inside the first_name variable:\n\nfirst_name = \"Joanna\"\n\nTo change this to a different value, simply run a new assignment statement with a new value:\n\nfirst_name = \"Luigi\"\n\nYou can print the variable to observe the change:\n\nfirst_name\n\n'Luigi'",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#working-with-variables",
    "href": "p_foundations_coding_basics.html#working-with-variables",
    "title": "3  Coding basics",
    "section": "3.18 Working with Variables",
    "text": "3.18 Working with Variables\nMost of your time in Python will be spent manipulating variables. Let’s see some quick examples.\nYou can run simple commands on variables. For example, below we store the value 100 in a variable and then take the square root of the variable:\n\nimport math\n\nmy_number = 100\nmath.sqrt(my_number)\n\n10.0\n\n\nPython “sees” my_number as the number 100, and so is able to evaluate its square root.\n\nYou can also combine existing variables to create new variables. For example, type out the code below to add my_number to itself, and store the result in a new variable called my_sum:\n\nmy_sum = my_number + my_number\nmy_sum\n\n200\n\n\nWhat should be the value of my_sum? First take a guess, then check it by printing it.\n\nPython also allows us to concatenate strings with the + operator. For example, we can concatenate the first_name and last_name variables to create a new variable called full_name:\n\nfirst_name = \"Joanna\"\nlast_name = \"Luigi\"\nfull_name = first_name + \" \" + last_name\nfull_name\n\n'Joanna Luigi'\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.19 Practice Q: Variable Assignment and Manipulation\nConsider the code below. What is the value of the answer variable? Think about it, then run the code to check your answer.\n\neight = 9\nanswer = eight - 8\nanswer",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-variable-assignment-and-manipulation",
    "href": "p_foundations_coding_basics.html#practice-q-variable-assignment-and-manipulation",
    "title": "3  Coding basics",
    "section": "3.19 Practice Q: Variable Assignment and Manipulation",
    "text": "3.19 Practice Q: Variable Assignment and Manipulation\nConsider the code below. What is the value of the answer variable? Think about it, then run the code to check your answer.\n\neight = 9\nanswer = eight - 8\nanswer",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#getting-user-input",
    "href": "p_foundations_coding_basics.html#getting-user-input",
    "title": "3  Coding basics",
    "section": "3.20 Getting User Input",
    "text": "3.20 Getting User Input\nThough it’s not used often in data analysis, the input() function from Python is a cool Python feature that you should know about. It allows you to get input from the user.\nHere’s a simple example. We can request user input and store it in a variable called name.\n\nname = input()\n\nAnd then we can print a greeting to the user.\n\nprint(\"Hello,\", name)\n\nWe can also include a question for the input prompt:\n\nname = input('What is your name? ')\nprint(\"Hello,\", name)\n\nLet’s see another example. We’ll tell the user how many letters are in their name.\n\nname = input('What is your name? ')\nprint(\"There are\", len(name), \"letters in your name\")\n\nFor instance, if you run this code and enter “Kene”, you might see:\nWhat is your name? Kene\nThere are 4 letters in your name\n\n\n\n\n\n\nPractice\n\n\n\n3.21 Practice Q: Using Input()\nWrite a short program that asks the user for their favorite color and then prints a message saying “xx color is my favorite color too!”, where xx is the color they entered. Test your program by running it and entering a color.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-using-input",
    "href": "p_foundations_coding_basics.html#practice-q-using-input",
    "title": "3  Coding basics",
    "section": "3.21 Practice Q: Using Input()",
    "text": "3.21 Practice Q: Using Input()\nWrite a short program that asks the user for their favorite color and then prints a message saying “xx color is my favorite color too!”, where xx is the color they entered. Test your program by running it and entering a color.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#common-error-with-variables",
    "href": "p_foundations_coding_basics.html#common-error-with-variables",
    "title": "3  Coding basics",
    "section": "3.22 Common Error with Variables",
    "text": "3.22 Common Error with Variables\nOne of the most common errors you’ll encounter when working with variables in Python is the NameError. This occurs when you try to use a variable that hasn’t been defined yet. For example:\n\nmy_number = 48  # define `my_number`\nMy_number + 2  # attempt to add 2 to `my_number`\n\nIf you run this code, you’ll get an error message like this:\nNameError: name 'My_number' is not defined\nHere, Python returns an error message because we haven’t created (or defined) the variable My_number yet. Recall that Python is case-sensitive; we defined my_number but tried to use My_number.\nTo fix this, make sure you’re using the correct variable name:\n\nmy_number = 48\nmy_number + 2  # This will work and return 50\n\n50\n\n\nAlways double-check your variable names to avoid this error. Remember, in Python, my_number, My_number, and MY_NUMBER are all different variables.\n\nWhen you first start learning Python, dealing with errors can be frustrating. They’re often difficult to understand.\nBut it’s important to get used to reading and understanding errors, because you’ll get them a lot through your coding career.\nLater, we will show you how to use Large Language Models (LLMs) like ChatGPT to debug errors.\nAt the start though, it’s good to try to spot and fix errors yourself.\n\n\n\n\n\n\nPractice\n\n\n\n3.23 Practice Q: Debugging Variable Errors\nThe code below returns an error. Why? (Look carefully)\n\nmy_1st_name = \"Kene\"\nmy_last_name = \"Nwosu\"\n\nprint(my_Ist_name, my_last_name)\n\nHint: look at the variable names. Are they consistent?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-debugging-variable-errors",
    "href": "p_foundations_coding_basics.html#practice-q-debugging-variable-errors",
    "title": "3  Coding basics",
    "section": "3.23 Practice Q: Debugging Variable Errors",
    "text": "3.23 Practice Q: Debugging Variable Errors\nThe code below returns an error. Why? (Look carefully)\n\nmy_1st_name = \"Kene\"\nmy_last_name = \"Nwosu\"\n\nprint(my_Ist_name, my_last_name)\n\nHint: look at the variable names. Are they consistent?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#naming-variables",
    "href": "p_foundations_coding_basics.html#naming-variables",
    "title": "3  Coding basics",
    "section": "3.24 Naming Variables",
    "text": "3.24 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n— Phil Karlton.\n\nBecause much of your work in Python involves interacting with variables you have created, picking intelligent names for these variables is important.\nNaming variables is difficult because names should be both short (so that you can type them quickly) and informative (so that you can easily remember what the variable contains), and these two goals are often in conflict.\nSo names that are too long, like the one below, are bad because they take forever to type.\n\nsample_of_the_ebola_outbreak_dataset_from_sierra_leone_in_2014\n\nAnd a name like data is bad because it is not informative; the name does not give a good idea of what the variable contains.\nAs you write more Python code, you will learn how to write short and informative names.\n\nFor names with multiple words, there are a few conventions for how to separate the words:\n\nsnake_case = \"Snake case uses underscores\"\ncamelCase = \"Camel case capitalizes new words (but not the first word)\"\nPascalCase = \"Pascal case capitalizes all words including the first\"\n\nWe recommend snake_case, which uses all lower-case words, and separates words with _.\n\nNote too that there are some limitations on variable names:\n\nNames must start with a letter or underscore. So 2014_data is not a valid name (because it starts with a number). Try running the code chunk below to see what error you get.\n\n\n2014_data = \"This is not a valid name\"\n\n\nNames can only contain letters, numbers, and underscores (_). So ebola-data or ebola~data or ebola data with a space are not valid names.\n\n\nebola-data = \"This is not a valid name\"\n\n\nebola~data = \"This is not a valid name\"\n\n\n\n\n\n\n\nSide note\n\n\n\nWhile we recommend snake_case for variable names in Python, you might see other conventions like camelCase or PascalCase, especially when working with code from other languages or certain Python libraries. It’s important to be consistent within your own code and follow the conventions of any project or team you’re working with.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.25 Practice Q: Valid Variable Naming Conventions\nWhich of the following variable names are valid in Python? Try to determine this without running the code, then check your answers by attempting to run each line.\nThen fix the invalid variable names.\n\n1st_name = \"John\"\nlast_name = \"Doe\"\nfull-name = \"John Doe\"\nage_in_years = 30\ncurrent@job = \"Developer\"\nPhoneNumber = \"555-1234\"\n_secret_code = 42",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#practice-q-valid-variable-naming-conventions",
    "href": "p_foundations_coding_basics.html#practice-q-valid-variable-naming-conventions",
    "title": "3  Coding basics",
    "section": "3.25 Practice Q: Valid Variable Naming Conventions",
    "text": "3.25 Practice Q: Valid Variable Naming Conventions\nWhich of the following variable names are valid in Python? Try to determine this without running the code, then check your answers by attempting to run each line.\nThen fix the invalid variable names.\n\n1st_name = \"John\"\nlast_name = \"Doe\"\nfull-name = \"John Doe\"\nage_in_years = 30\ncurrent@job = \"Developer\"\nPhoneNumber = \"555-1234\"\n_secret_code = 42",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#wrap-up",
    "href": "p_foundations_coding_basics.html#wrap-up",
    "title": "3  Coding basics",
    "section": "3.26 Wrap-Up",
    "text": "3.26 Wrap-Up\nIn this lesson, we’ve covered the fundamental building blocks of Python programming:\n\nComments: Using # for single-line and triple quotes for multi-line comments.\nBasic Arithmetic: Using Python as a calculator and understanding order of operations.\nMath Library: Performing complex mathematical operations.\nCode Spacing: Improving readability with proper spacing.\nVariables: Creating, manipulating, and reassigning variables of different types.\nGetting User Input: Using the input() function to get input from the user.\nVariable Naming: Following rules and best practices for naming variables.\nCommon Errors: Identifying and fixing errors related to variables.\n\nThese concepts form the foundation of Python programming. As you continue your journey, you’ll build upon these basics to create more complex and powerful programs. Remember, practice is key to mastering these concepts!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html",
    "href": "p_foundations_functions_methods.html",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "",
    "text": "4.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#learning-objectives",
    "href": "p_foundations_functions_methods.html#learning-objectives",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "",
    "text": "You understand what functions and methods are in Python.\nYou can identify and use arguments (parameters) in functions and methods.\nYou know how to call built-in functions and methods on objects.\nYou understand what libraries are in Python and how to import them.\nYou know how to install a simple external library and use it in your code.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#introduction",
    "href": "p_foundations_functions_methods.html#introduction",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nIn this lesson, you will learn about functions, methods, and libraries in Python, building on the basics we covered in the previous lesson.\nTo get started, open your preferred Python environment (e.g., Jupyter Notebook, VS Code, or PyCharm), and create a new Python file or notebook.\nNext, save the file with a name like “functions_and_libraries.py” or “functions_and_libraries.ipynb” depending on your environment.\nYou should now type all the code from this lesson into that file.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#functions",
    "href": "p_foundations_functions_methods.html#functions",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.3 Functions",
    "text": "4.3 Functions\nA function is a block of code that performs a specific task. It can take inputs (arguments) and return outputs. Here’s an example of a built-in function with just one argument:\n\n# Using the len() function to get the length of a string\nlen(\"Python\")\n\n6\n\n\nThe round() function takes two arguments: the number to round and the number of decimal places to round to.\n\n# Using the round() function to round a number\nround(3.1415, 2)\n\n3.14\n\n\n\n\n\n\n\n\nPractice\n\n\n\n4.3.1 Q: Using built-in functions\nUse the abs() function to get the absolute value of -5.\nWrite your code below and run it to check your answer:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#arguments-parameters",
    "href": "p_foundations_functions_methods.html#arguments-parameters",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.4 Arguments (Parameters)",
    "text": "4.4 Arguments (Parameters)\nArguments (also called parameters) are the values that you pass to a function (or method) when you call it.\nThere are different ways to pass arguments to a function.\nConsider again the round() function.\nIf we look at the documentation for the round() function, with :\n\nround?\n\nWe see that it takes two arguments:\n\nnumber: The number to round.\nndigits: The number of decimal places to round to.\n\nThere are two main ways to pass arguments to this function.\n\nPositional arguments: Passed in the order they are defined. Since the default order of the arguments is number then ndigits, we can pass the arguments in that order without specifying the argument names, as we did above.\n\n\nround(3.1415, 2)\n\n3.14\n\n\nIf we swap the order of the arguments, we get an error:\n\nround(2, 3.1415)\n\n\nKeyword arguments: Passed by specifying the argument name followed by a = and the argument value.\n\n\nround(number=3.1415, ndigits=2)\n\n3.14\n\n\nWith this method, we can pass the arguments in any order, as long as we use the argument names.\n\nround(ndigits=2, number=3.1415)\n\n3.14\n\n\nSpecifying the keyword is usually recommended, except for simple functions with very few arguments, or when the order of the arguments is obvious from context.\n\n\n\n\n\n\nPractice\n\n\n\n\n4.4.1 Q: Using Positional Arguments with pow()\nUse the pow() function to calculate 2 raised to the power of 7 by passing positional arguments. You may need to consult the documentation for the pow() function to see how it works.\nWrite your code below and run it to check your answer:\n\n# Your code here\n\n\n\n4.4.2 Q: Using Keyword Arguments with round()\nUse the round() function to round the number 9.87652 to 3 decimal places by specifying keyword arguments.\nWrite your code below and run it to check your answer:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#methods",
    "href": "p_foundations_functions_methods.html#methods",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.5 Methods",
    "text": "4.5 Methods\nMethods are similar to functions, but they are associated with specific objects or data types. They are called using dot notation.\nFor example, every string object comes with a range of built-in methods, like upper() to convert to uppercase, lower() to convert to lowercase, replace() to replace substrings, and many more.\nLet’s see how to use these:\n\nname = \"python\"\nprint(name.upper())\nprint(name.lower())\nprint(name.replace(\"p\", \"🐍\"))\n\nPYTHON\npython\n🐍ython\n\n\nWe can also call the methods directly on the string object, without assigning it to a variable:\n\n# Using the upper() method on a string\nprint(\"python\".upper())\nprint(\"PYTHON\".lower())\nprint(\"python\".replace(\"p\", \"🐍\"))\n\nPYTHON\npython\n🐍ython\n\n\nSimilarly, numbers in Python come with some built-in methods. For example, the as_integer_ratio() (added in Python 3.8) method converts a decimalnumber to a ratio of two integers.\n\n# Using the as_integer_ratio() method on a float\nexample_decimal = 1.5\nexample_decimal.as_integer_ratio()\n\n(3, 2)\n\n\n\n\n\n\n\n\nPractice\n\n\n\n4.5.1 Q: Definitions\nCome up with simple definitions for the following terms that are clear to YOU (even if not technically exactly accurate):\n\nFunction\nMethod\nArgument (Parameter)\nDot Notation\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n4.5.2 Q: Using methods\n\nCall the replace() method on the string “Helo” to replace the single l with double l.\nCall the split() method on the string “Hello World” to split the string into a list of words.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#libraries-in-python",
    "href": "p_foundations_functions_methods.html#libraries-in-python",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.6 Libraries in Python",
    "text": "4.6 Libraries in Python\nLibraries are collections of pre-written code that you can use in your programs. They extend the functionality of Python by providing additional functions and tools.\nFor example, the math library provides mathematical functions like sqrt() for square roots and sin() for sine.\nIf we try to use the sqrt() function without importing the math library, we get an error:\n\n# This will cause a NameError\nsqrt(16)\n\nWe can import the math library and use the sqrt() function like this:\n\n# Import the library\nimport math\n\nThen we can use the sqrt() function like this:\n\n# Use the sqrt() function\nmath.sqrt(16)\n\n4.0\n\n\nWe can get help on a function in a similar way, calling both the function and the library it’s in:\n\n# Get help on the sqrt() function\nmath.sqrt?\n\nWe can also import libraries with aliases. For example, we can import the math library with the alias m:\n\n# Import the entire library with an alias\nimport math as m\n# Then we can use the alias to call the function\nm.sqrt(16)\n\n4.0\n\n\nFinally, if you want to skip the alias/library name, you can either import the functions individually:\n\n# Import specific functions from a library\nfrom math import sqrt, sin\n# Then we can use the function directly\nsqrt(16)\nsin(0)\n\n0.0\n\n\nOr import everything from the library:\n\n# Import everything from the library\nfrom math import *\n# Then we can all functions directly, such as sqrt() and sin()\nsqrt(16)\ncos(0)\ntan(0)\nsin(0)\n\n0.0\n\n\nPhew that’s a lot of ways to import libraries! You’ll mostly see the import ... as ... syntax, and sometimes the from ... import ... syntax.\nNote that we typically import all required libraries at the top of the file, in a single code chunk. This is a good practice to follow.\n\n\n\n\n\n\nPractice\n\n\n\n4.6.1 Q: Definitions\nCome up with simple definitions for the following terms that are clear to YOU (even if not technically exactly accurate):\n\nLibrary (Module)\nImport\nAlias\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n4.6.2 Q: Using functions from imported libraries\n\nImport the random library and use the randint() function to generate a random integer between 1 and 10. You can use the ? operator to get help on the function after importing it.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#installing-libraries",
    "href": "p_foundations_functions_methods.html#installing-libraries",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.7 Installing Libraries",
    "text": "4.7 Installing Libraries\nWhile Python comes with many built-in libraries, there are thousands of additional libraries available that you can install to extend Python’s functionality even further. Let’s look at how to install and use a simple external library, with the cowsay library as an example.\nIf we try to import this library without first installing it, we get an error:\n\nimport cowsay\n\nTo install the library, you can use the !pip install command in a code cell in Google Colab. For cowsay, you would run:\n\n!pip install cowsay\n\nPip installs packages from a remote repository called PyPI. Anyone can create and upload a package to PyPI. After a few checks, it’s then available for anyone to install.\n\n\n\n\n\n\nSide-note\n\n\n\nFor those working on local Python instances, you can install cowsay using pip in your terminal:\npip install cowsay\n\n\nOnce installed, we can now import and use the cowsay library:\n\nimport cowsay\n\n# Make the cow say something\ncowsay.cow('Moo!')\n\n  ____\n| Moo! |\n  ====\n    \\\n     \\\n       ^__^\n       (oo)\\_______\n       (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n\n\nThis should display an ASCII art cow saying “Moo!”.\n\n\n\n\n\n\nPractice\n\n\n\n4.7.1 Q: Using the emoji library\n\nInstall the emoji library.\nImport the emoji library.\nConsult the help for the emojize() function in the emoji library.\nUse the emojize() function to display an emoji for “thumbs up”.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#wrap-up",
    "href": "p_foundations_functions_methods.html#wrap-up",
    "title": "4  Functions, Methods, and Libraries in Python",
    "section": "4.8 Wrap-up",
    "text": "4.8 Wrap-up\nIn this lesson, we’ve covered:\n\nFunctions and methods in Python\nArguments (parameters) and how to use them\nImporting and using libraries\nInstalling and using an external library\n\nThese concepts are fundamental to Python programming and will be used extensively as you continue to develop your skills. Practice using different functions, methods, and libraries to become more comfortable with these concepts.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html",
    "href": "p_foundations_data_structures.html",
    "title": "5  Data Structures in Python",
    "section": "",
    "text": "5.1 Intro\nSo far in our Python explorations, we’ve been working with simple, single values, like numbers and strings. But, as you know, data usually comes in the form of larger structures. The structure most familiar to you is a table, with rows and columns.\nIn this lesson, we’re going to explore the building blocks for organizing data in Python, building up through lists, dictionaries, series, and finally tables, or, more formally,dataframes.\nLet’s dive in!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#learning-objectives",
    "href": "p_foundations_data_structures.html#learning-objectives",
    "title": "5  Data Structures in Python",
    "section": "5.2 Learning objectives",
    "text": "5.2 Learning objectives\n\nCreate and work with Python lists and dictionaries\nUnderstand and use Pandas Series\nExplore Pandas DataFrames for organizing structured data",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#imports",
    "href": "p_foundations_data_structures.html#imports",
    "title": "5  Data Structures in Python",
    "section": "5.3 Imports",
    "text": "5.3 Imports\nWe need pandas for this lesson. You can import it like this:\n\nimport pandas as pd\n\nIf you get an error, you probably need to install it. You can do this by running !pip install pandas in a cell.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#python-lists",
    "href": "p_foundations_data_structures.html#python-lists",
    "title": "5  Data Structures in Python",
    "section": "5.4 Python Lists",
    "text": "5.4 Python Lists\nLists are like ordered containers that can hold different types of information. For example, you might have a list of things to buy:\n\nshopping = [\"apples\", \"bananas\", \"milk\", \"bread\"] \nshopping\n\n['apples', 'bananas', 'milk', 'bread']\n\n\nIn Python, we use something called “zero-based indexing” to access items in a list. This means we start counting positions from 0, not 1.\nLet’s see some examples:\n\nprint(shopping[0])  # First item (remember, we start at 0!)\nprint(shopping[1])  # Second item\nprint(shopping[2])  # Third item\n\napples\nbananas\nmilk\n\n\nIt might seem odd at first, but it’s a common practice in many programming languages. It has to do with how computers store information, and the ease of writing algorithms.\nWe can change the contents of a list after we’ve created it, using the same indexing system.\n\nshopping[1] = \"oranges\"  # Replace the second item (at index 1)\nshopping\n\n['apples', 'oranges', 'milk', 'bread']\n\n\nThere are many methods accessible to lists. For example, we can add elements to a list using the append() method.\n\nshopping.append(\"eggs\")\nshopping\n\n['apples', 'oranges', 'milk', 'bread', 'eggs']\n\n\nIn the initial stages of your Python data journey, you may not work with lists too often, so we’ll keep this intro brief.\n\n\n\n\n\n\nPractice\n\n\n\n5.4.1 Practice: Working with Lists\n\nCreate a list called temps with these values: 1,2,3,4\nPrint the first element of the list\nChange the last element to 6\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#python-dictionaries",
    "href": "p_foundations_data_structures.html#python-dictionaries",
    "title": "5  Data Structures in Python",
    "section": "5.5 Python Dictionaries",
    "text": "5.5 Python Dictionaries\nDictionaries are like labeled storage boxes for your data. Each piece of data (value) has a unique label (key). Below, we have a dictionary of grades for some students.\n\ngrades = {\"Alice\": 90, \"Bob\": 85, \"Charlie\": 92}\ngrades\n\n{'Alice': 90, 'Bob': 85, 'Charlie': 92}\n\n\nAs you can see, dictionaries are defined using curly braces {}, with keys and values separated by colons :, and the key-value pairs are separated by commas.\nWe use the key to get the associated value.\n\ngrades[\"Bob\"]\n\n85\n\n\n\n5.5.1 Adding/Modifying Entries\nWe can easily add new information or change existing data in a dictionary.\n\ngrades[\"David\"] = 88  # Add a new student\ngrades\n\n{'Alice': 90, 'Bob': 85, 'Charlie': 92, 'David': 88}\n\n\n\ngrades[\"Alice\"] = 95  # Update Alice's grade\ngrades\n\n{'Alice': 95, 'Bob': 85, 'Charlie': 92, 'David': 88}\n\n\n\n\n\n\n\n\nPractice\n\n\n\n5.5.2 Practice: Working with Dictionaries\n\nCreate a dictionary called prices with these pairs: “apple”: 0.50, “banana”: 0.25, “orange”: 0.75\nPrint the price of an orange by using the key\nAdd a new fruit “grape” with a price of 1.5\nChange the price of “banana” to 0.30\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#pandas-series",
    "href": "p_foundations_data_structures.html#pandas-series",
    "title": "5  Data Structures in Python",
    "section": "5.6 Pandas Series",
    "text": "5.6 Pandas Series\nPandas provides a data structure called a Series that is similar to a list, but with additional features that are particularly useful for data analysis.\nLet’s create a simple Series:\n\ntemps = pd.Series([1, 2, 3, 4, 5])\ntemps\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\nWe can use built-in Series methods to calculate summary statistics.\n\ntemps.mean()\ntemps.median()\ntemps.std()\n\nnp.float64(1.5811388300841898)\n\n\nAn important feature of Series is that they can have a custom index for intuitive access.\n\ntemps_labeled = pd.Series([1, 2, 3, 4], index=['Mon', 'Tue', 'Wed', 'Thu'])\ntemps_labeled\ntemps_labeled['Wed']\n\nnp.int64(3)\n\n\nThis makes them similar to dictionaries.\n\n\n\n\n\n\nPractice\n\n\n\n5.6.1 Practice: Working with Series\n\nCreate a Series called rain with these values: 5, 4, 3, 2\nGet the mean and median rainfall\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#pandas-dataframes",
    "href": "p_foundations_data_structures.html#pandas-dataframes",
    "title": "5  Data Structures in Python",
    "section": "5.7 Pandas DataFrames",
    "text": "5.7 Pandas DataFrames\nNext up, let’s consider Pandas DataFrames, which are like Series but in two dimensions - think spreadsheets or database tables.\nThis is the most important data structure for data analysis.\nA DataFrame is like a spreadsheet in Python. It has rows and columns, making it perfect for organizing structured data.\nMost of the time, you will be importing external data frames, but you should know how to data frames from scratch within Python as well.\nLet’s create three lists first:\n\n# Create three lists\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 28]\ncities = [\"Lagos\", \"London\", \"Lima\"]\n\nThen we combined them into a dictionary, and finally into a dataframe.\n\ndata = {'name': names,\n        'age': ages,\n        'city': cities}\n\npeople_df = pd.DataFrame(data)\npeople_df\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nLagos\n\n\n1\nBob\n30\nLondon\n\n\n2\nCharlie\n28\nLima\n\n\n\n\n\n\n\nNote that we could have created the dataframe without the intermediate series:\n\npeople_df = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"age\": [25, 30, 28],\n        \"city\": [\"Lagos\", \"London\", \"Lima\"],\n    }\n)\npeople_df\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nLagos\n\n\n1\nBob\n30\nLondon\n\n\n2\nCharlie\n28\nLima\n\n\n\n\n\n\n\nWe can select specific columns or rows from our DataFrame.\n\npeople_df[\"city\"]  # Selecting a column. Note that this returns a Series.\npeople_df.loc[0]  # Selecting a row by its label. This also returns a Series.\n\nname    Alice\nage        25\ncity    Lagos\nName: 0, dtype: object\n\n\nWe can call methods on the dataframe.\n\npeople_df.describe() # This is a summary of the numerical columns\npeople_df.info() # This is a summary of the data types\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   name    3 non-null      object\n 1   age     3 non-null      int64 \n 2   city    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\n\n\nAnd we can call methods on the Series objects that result from selecting columns.\nFor example, we can get summary statistics on the “city” column.\n\npeople_df[\"city\"].describe()  # This is a summary of the \"city\" column\npeople_df[\"age\"].mean()  # This is the mean of the \"age\" column\n\nnp.float64(27.666666666666668)\n\n\nIn a future series of lessons, we’ll dive deeper into slicing and manipulating DataFrames. Our goal in this lesson is just to get you familiar with the basic syntax and concepts.\n\n\n\n\n\n\nPractice\n\n\n\n5.7.1 Practice: Working with DataFrames\n\nCreate a DataFrame called students with this information:\n\nColumns: “Name”, “Age”, “Grade”\nAlice’s grade is 90, Bob’s grade is 85, and Charlie’s grade is 70. You pick the ages.\n\nShow only the “Grade” column\nCalculate and show the average age of the students\nDisplay the row for Bob.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#wrap-up",
    "href": "p_foundations_data_structures.html#wrap-up",
    "title": "5  Data Structures in Python",
    "section": "5.8 Wrap-up",
    "text": "5.8 Wrap-up\nWe’ve explored the main data structures for Python data analysis. From basic lists and dictionaries to Pandas Series and DataFrames, these tools are essential for organizing and analyzing data. They will be the foundation for more advanced data work in future lessons.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html",
    "href": "p_foundations_for_loops.html",
    "title": "6  Intro to Loops in Python",
    "section": "",
    "text": "6.1 Introduction\nAt the heart of programming is the concept of repeating a task multiple times. A for loop is one fundamental way to do that. Loops enable efficient repetition, saving time and effort.\nMastering this concept is essential for writing intelligent Python code.\nLet’s dive in and enhance your coding skills!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#learning-objectives",
    "href": "p_foundations_for_loops.html#learning-objectives",
    "title": "6  Intro to Loops in Python",
    "section": "6.2 Learning Objectives",
    "text": "6.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nUse basic for loops in Python\nUse index variables to iterate through lists in a loop\nFormat output using f-strings within loops\nApply loops to generate multiple plots for data visualization",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#packages",
    "href": "p_foundations_for_loops.html#packages",
    "title": "6  Intro to Loops in Python",
    "section": "6.3 Packages",
    "text": "6.3 Packages\nIn this lesson, we will use the following Python libraries:\n\nimport pandas as pd\nimport plotly.express as px\nfrom vega_datasets import data",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#intro-to-for-loops",
    "href": "p_foundations_for_loops.html#intro-to-for-loops",
    "title": "6  Intro to Loops in Python",
    "section": "6.4 Intro to for Loops",
    "text": "6.4 Intro to for Loops\nLet’s start with a simple example. Suppose we have a list of children’s ages in years, and we want to convert these to months:\n\nages = [7, 8, 9]  # List of ages in years\n\nWe could try to directly multiply the list by 12:\n\nages * 12\n\n[7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9]\n\n\nBut this does not do what we want. It repeats the list 12 times.\nRather, we need to loop through each element in the list and multiply it by 12:\n\nfor age in ages:\n    print(age * 12)\n\n84\n96\n108\n\n\nfor and in are required keywords in the loop. The colon and the indentation on the second line are also required.\nIn this loop, age is a temporary variable that takes the value of each element in ages during each iteration. First, age is 7, then 8, then 9.\nYou can choose any name for this variable:\n\nfor random_name in ages:\n    print(random_name * 12)\n\n84\n96\n108\n\n\nNote that we need the print statement since the loop does not automatically print the result:\n\nfor age in ages:\n    age * 12\n\n\n\n\n\n\n\nPractice\n\n\n\n6.4.1 Hours to Minutes Basic Loop\nTry converting hours to minutes using a for loop. Start with this list of hours:\n\nhours = [3, 4, 5]  # List of hours\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#printing-with-f-strings",
    "href": "p_foundations_for_loops.html#printing-with-f-strings",
    "title": "6  Intro to Loops in Python",
    "section": "6.5 Printing with f-strings",
    "text": "6.5 Printing with f-strings\nWe might want to print both the result and the original age. We could do this by concatenating strings with the + operator. But we need to convert the age to a string with str().\n\nfor age in ages:\n    print(str(age) + \" years is \" + str(age * 12) + \" months\" )\n\n7 years is 84 months\n8 years is 96 months\n9 years is 108 months\n\n\nAlternatively, we can use something called an f-string. This is a string that allows us to embed variables directly.\n\nfor age in ages:\n    print(f\"{age} years is {age * 12} months\")\n\n7 years is 84 months\n8 years is 96 months\n9 years is 108 months\n\n\nWithin the f-string, we use curly braces {} to embed the variables.\n\n\n\n\n\n\nPractice\n\n\n\n6.5.1 Practice: F-String\nAgain convert the list of hours below to minutes. Use f-strings to print both the original hours and the converted minutes.\n\nhours = [3, 4, 5]  # List of hours\n# Your code here\n# Example output \"3 hours is 180 minutes\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#are-for-loops-useful-in-python",
    "href": "p_foundations_for_loops.html#are-for-loops-useful-in-python",
    "title": "6  Intro to Loops in Python",
    "section": "6.6 Are for Loops Useful in Python?",
    "text": "6.6 Are for Loops Useful in Python?\nWhile for loops are useful, in many cases there are more efficient ways to perform operations over collections of data.\nFor example, our initial age conversion could be achieved using pandas Series:\n\nimport pandas as pd\n\nages = pd.Series([7, 8, 9])\nmonths = ages * 12\nprint(months)\n\n0     84\n1     96\n2    108\ndtype: int64\n\n\nBut while libraries like pandas offer powerful ways to work with data, for loops are essential for tasks that can’t be easily vectorized or when you need fine-grained control over the iteration process.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#looping-with-an-index-and-value",
    "href": "p_foundations_for_loops.html#looping-with-an-index-and-value",
    "title": "6  Intro to Loops in Python",
    "section": "6.7 Looping with an Index and Value",
    "text": "6.7 Looping with an Index and Value\nSometimes, we want to access both the position (index) and the value of items in a list. The enumerate() function helps us do this easily.\nLet’s look at our ages list again:\n\nages = [7, 8, 9]  # List of ages in years\n\nFirst, let’s see what enumerate() actually does:\n\nfor item in enumerate(ages):\n    print(item)\n\n(0, 7)\n(1, 8)\n(2, 9)\n\n\nAs you can see, enumerate() gives us pairs of (index, value).\nWe can unpack these pairs directly in the for loop:\n\nfor i, age in enumerate(ages):\n    print(f\"The person at index {i} is aged {age}\")\n\nThe person at index 0 is aged 7\nThe person at index 1 is aged 8\nThe person at index 2 is aged 9\n\n\nHere, i is the index, and age is the value at that index.\nNow, let’s create a more detailed output using both the index and value:\n\nfor i, age in enumerate(ages):\n    print(f\"The person at index {i} is aged {age} years which is {age * 12} months\")\n\nThe person at index 0 is aged 7 years which is 84 months\nThe person at index 1 is aged 8 years which is 96 months\nThe person at index 2 is aged 9 years which is 108 months\n\n\nThis is particularly useful when you need both the position and the value in your loop.\n\n\n\n\n\n\nPractice\n\n\n\n6.7.1 Practice: Enumerate with F-strings\nUse enumerate() and f-strings to print a sentence for each hour in the list:\n\nhours = [3, 4, 5]  # List of hours\n\n# Your code here\n# Example output: \"Hour 3 at index 0 is equal to 180 minutes\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html",
    "href": "p_foundations_writing_functions.html",
    "title": "7  Intro to Functions and Conditionals",
    "section": "",
    "text": "7.1 Intro\nSo far in this course you have mostly used functions written by others. In this lesson, you will learn how to write your own functions in Python.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#learning-objectives",
    "href": "p_foundations_writing_functions.html#learning-objectives",
    "title": "7  Intro to Functions and Conditionals",
    "section": "7.2 Learning Objectives",
    "text": "7.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate and use your own functions in Python.\nDesign function arguments and set default values.\nUse conditional logic like if, elif, and else within functions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#packages",
    "href": "p_foundations_writing_functions.html#packages",
    "title": "7  Intro to Functions and Conditionals",
    "section": "7.3 Packages",
    "text": "7.3 Packages\nRun the following code to install and load the packages needed for this lesson:\n\n# Import packages\nimport pandas as pd\nimport numpy as np\nimport vega_datasets as vd",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#basics-of-a-function",
    "href": "p_foundations_writing_functions.html#basics-of-a-function",
    "title": "7  Intro to Functions and Conditionals",
    "section": "7.4 Basics of a Function",
    "text": "7.4 Basics of a Function\nLet’s start by creating a very simple function. Consider the following function that converts pounds (a unit of weight) to kilograms (another unit of weight):\n\ndef pounds_to_kg(pounds):\n    return pounds * 0.4536\n\nIf you execute this code, you will create a function named pounds_to_kg, which can be used directly in a script or in the console:\n\nprint(pounds_to_kg(150))\n\n68.04\n\n\nLet’s break down the structure of this first function step by step.\nFirst, a function is created using the def keyword, followed by a pair of parentheses and a colon.\n\ndef function_name():\n    # Function body\n\nInside the parentheses, we indicate the arguments of the function. Our function only takes one argument, which we have decided to name pounds. This is the value that we want to convert from pounds to kilograms.\n\ndef pounds_to_kg(pounds):\n    # Function body\n\nOf course, we could have named this argument anything we wanted. E.g. p or weight.\nThe next element, after the colon, is the body of the function. This is where we write the code that we want to execute when the function is called.\n\ndef pounds_to_kg(pounds):\n    return pounds * 0.4536\n\nWe use the return statement to specify what value the function should output.\nYou could also assign the result to a variable and then return that variable:\n\ndef pounds_to_kg(pounds):\n    kg = pounds * 0.4536\n    return kg\n\nThis is a bit more wordy, but it makes the function clearer.\nWe can now use our function like this with a named argument:\n\npounds_to_kg(pounds=150)\n\n68.04\n\n\nOr without a named argument:\n\npounds_to_kg(150)\n\n68.04\n\n\nTo use this in a DataFrame, you can create a new column:\n\npounds_df = pd.DataFrame({'pounds': [150, 200, 250]})\npounds_df['kg'] = pounds_to_kg(pounds_df['pounds'])\npounds_df\n\n\n\n\n\n\n\n\npounds\nkg\n\n\n\n\n0\n150\n68.04\n\n\n1\n200\n90.72\n\n\n2\n250\n113.40\n\n\n\n\n\n\n\nAnd that’s it! You have just created and usedyour first function in Python.\n\n\n\n\n\n\nPractice\n\n\n\n7.4.1 Age in Months Function\nCreate a simple function called years_to_months that transforms age in years to age in months.\nUse it on the riots_df DataFrame imported below to create a new column called age_months:\n\nriots_df = vd.data.la_riots()\nriots_df \n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n60\nElbert O.\nWilkins\n33.0\nMale\nBlack\n1992-04-30\nWestern Avenue & 92nd Street\nGramercy Park\nHomicide\n-118.310004\n33.952767\n\n\n61\nJohn H.\nWillers\n37.0\nMale\nWhite\n1992-04-29\n10621 Sepulveda Blvd.\nMission Hills\nHomicide\n-118.467770\n34.263184\n\n\n62\nWillie Bernard\nWilliams\n29.0\nMale\nBlack\n1992-04-29\nGage & Western avenues\nChesterfield Square\nDeath\n-118.308952\n33.982363\n\n\n\n\n63 rows × 11 columns",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#functions-with-multiple-arguments",
    "href": "p_foundations_writing_functions.html#functions-with-multiple-arguments",
    "title": "7  Intro to Functions and Conditionals",
    "section": "7.5 Functions with Multiple Arguments",
    "text": "7.5 Functions with Multiple Arguments\nMost functions take multiple arguments rather than just one. Let’s look at an example of a function that takes three arguments:\n\ndef calc_calories(carb_grams, protein_grams, fat_grams):\n    result = (carb_grams * 4) + (protein_grams * 4) + (fat_grams * 9)\n    return result\n\ncalc_calories(carb_grams=50, protein_grams=25, fat_grams=10)\n\n390\n\n\nThe calc_calories function computes the total calories based on the grams of carbohydrates, protein, and fat. Carbohydrates and proteins are estimated to be 4 calories per gram, while fat is estimated to be 9 calories per gram.\nIf you attempt to use the function without supplying all the arguments, it will yield an error.\n\ncalc_calories(carb_grams=50, protein_grams=25)\n\nTypeError: calc_calories() missing 1 required positional argument: 'fat_grams'\nYou can define default values for your function’s arguments. If an argument is called without a value assigned to it, then this argument assumes its default value. Let’s make all arguments optional by giving them all default values:\n\ndef calc_calories(carb_grams=0, protein_grams=0, fat_grams=0):\n    result = (carb_grams * 4) + (protein_grams * 4) + (fat_grams * 9)\n    return result\n\nNow, we can call the function with only some arguments without getting an error:\n\ncalc_calories(carb_grams=50, protein_grams=25)\n\n300\n\n\nLet’s use this on a sample dataset:\n\nfood_df = pd.DataFrame({\n    'food': ['Apple', 'Avocado'],\n    'carb_grams': [25, 10],\n    'protein_grams': [0, 1],\n    'fat_grams': [0, 14]\n})\nfood_df['calories'] = calc_calories(food_df['carb_grams'], food_df['protein_grams'], food_df['fat_grams'])\nfood_df\n\n\n\n\n\n\n\n\nfood\ncarb_grams\nprotein_grams\nfat_grams\ncalories\n\n\n\n\n0\nApple\n25\n0\n0\n100\n\n\n1\nAvocado\n10\n1\n14\n170\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.5.1 BMI Function\nCreate a function named calc_bmi that calculates the Body Mass Index (BMI) for one or more individuals, then apply the function by running the code chunk further below. The formula for BMI is weight (kg) divided by height (m) squared.\n\n# Your code here\n\n\nbmi_df = pd.DataFrame({\n    'Weight': [70, 80, 100],  # in kg\n    'Height': [1.7, 1.8, 1.2]  # in meters\n})\nbmi_df['BMI'] = calc_bmi(bmi_df['Weight'], bmi_df['Height'])\nbmi_df",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#intro-to-conditionals-if-elif-and-else",
    "href": "p_foundations_writing_functions.html#intro-to-conditionals-if-elif-and-else",
    "title": "7  Intro to Functions and Conditionals",
    "section": "7.6 Intro to Conditionals: if, elif, and else",
    "text": "7.6 Intro to Conditionals: if, elif, and else\nConditional statements allow you to execute code only when certain conditions are met. The basic syntax in Python is:\n\nif condition:\n    # Code to execute if condition is True\nelif another_condition:\n    # Code to execute if the previous condition was False and this condition is True\nelse:\n    # Code to execute if all previous conditions were False\n\nLet’s look at an example of using conditionals within a function. Suppose we want to write a function that classifies a number as positive, negative, or zero.\n\ndef class_num(num):\n    if num &gt; 0:\n        return \"Positive\"\n    elif num &lt; 0:\n        return \"Negative\"\n    else:\n        return \"Zero\"\n\nprint(class_num(10))    # Output: Positive\nprint(class_num(-5))    # Output: Negative\nprint(class_num(0))     # Output: Zero\n\nPositive\nNegative\nZero\n\n\nIf you try to use this function the way we have done above for, for example the BMI function, you will get an error:\n\nnum_df = pd.DataFrame({'num': [10, -5, 0]})\nnum_df\n\n\n\n\n\n\n\n\nnum\n\n\n\n\n0\n10\n\n\n1\n-5\n\n\n2\n0\n\n\n\n\n\n\n\n\nnum_df['category'] = class_num(num_df['num'])\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nThe reason for this is that if statements are not built to work with series (they are not inherently vectorized); but rather work with single values. To get around this, we can use the np.vectorize function to create a vectorized version of the function:\n\nclass_num_vec = np.vectorize(class_num)\nnum_df['category'] = class_num_vec(num_df['num'])\nnum_df\n\n\n\n\n\n\n\n\nnum\ncategory\n\n\n\n\n0\n10\nPositive\n\n\n1\n-5\nNegative\n\n\n2\n0\nZero\n\n\n\n\n\n\n\nTo get more practice with conditionals, let’s write a function that categorizes grades into simple categories:\n\nIf the grade is 85 or above, the category is ‘Excellent’.\nIf the grade is between 60 and 84, the category is ‘Pass’.\nIf the grade is below 60, the category is ‘Fail’.\nIf the grade is negative or invalid, return ‘Invalid grade’.\n\n\ndef categorize_grade(grade):\n    if grade &gt;= 85 and grade &lt;= 100:\n        return 'Excellent'\n    elif grade &gt;= 60 and grade &lt; 85:\n        return 'Pass'\n    elif grade &gt;= 0 and grade &lt; 60:\n        return 'Fail'\n    else:\n        return 'Invalid grade'\n\ncategorize_grade(95)  # Output: Excellent\n\n'Excellent'\n\n\nWe can apply this function to a column in a DataFrame but first we need to vectorize it:\n\ncategorize_grade = np.vectorize(categorize_grade)\n\n\ngrades_df = pd.DataFrame({'grade': [95, 82, 76, 65, 58, -5]})\ngrades_df['grade_cat'] = categorize_grade(grades_df['grade'])\ngrades_df\n\n\n\n\n\n\n\n\ngrade\ngrade_cat\n\n\n\n\n0\n95\nExcellent\n\n\n1\n82\nPass\n\n\n2\n76\nPass\n\n\n3\n65\nPass\n\n\n4\n58\nFail\n\n\n5\n-5\nInvalid grade\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.6.1 Age Categorization Function\nNow, try writing a function that categorizes age into different life stages as described earlier. You should use the following criteria:\n\nIf the age is under 18, the category is ‘Minor’.\nIf the age is greater than or equal to 18 and less than 65, the category is ‘Adult’.\nIf the age is greater than or equal to 65, the category is ‘Senior’.\nIf the age is negative or invalid, return ‘Invalid age’.\n\nUse it on the riots_df DataFrame printed below to create a new column called Age_Category.\n\n# Your code here\n\nriots_df = vd.data.la_riots()\nriots_df\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n60\nElbert O.\nWilkins\n33.0\nMale\nBlack\n1992-04-30\nWestern Avenue & 92nd Street\nGramercy Park\nHomicide\n-118.310004\n33.952767\n\n\n61\nJohn H.\nWillers\n37.0\nMale\nWhite\n1992-04-29\n10621 Sepulveda Blvd.\nMission Hills\nHomicide\n-118.467770\n34.263184\n\n\n62\nWillie Bernard\nWilliams\n29.0\nMale\nBlack\n1992-04-29\nGage & Western avenues\nChesterfield Square\nDeath\n-118.308952\n33.982363\n\n\n\n\n63 rows × 11 columns\n\n\n\n\n\n\n\n\n\n\n\nSide Note\n\n\n\n7.6.2 Apply vs Vectorize\nAnother way to use functions with if statements on a dataframe is to use the apply method. Here is how you can do the grade categorization function with apply:\n\ngrades_df['grade_cat'] = grades_df['grade'].apply(categorize_grade)\ngrades_df\n\n\n\n\n\n\n\n\ngrade\ngrade_cat\n\n\n\n\n0\n95\nExcellent\n\n\n1\n82\nPass\n\n\n2\n76\nPass\n\n\n3\n65\nPass\n\n\n4\n58\nFail\n\n\n5\n-5\nInvalid grade\n\n\n\n\n\n\n\nThe vectorize method is easier to use with multiple arguments, but you will encounter the apply method further down the road.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#conclusion",
    "href": "p_foundations_writing_functions.html#conclusion",
    "title": "7  Intro to Functions and Conditionals",
    "section": "7.7 Conclusion",
    "text": "7.7 Conclusion\nIn this lesson, we’ve introduced the basics of writing functions in Python and how to use conditional statements within those functions. Functions are essential building blocks in programming that allow you to encapsulate code for reuse and better organization. Conditional statements enable your functions to make decisions based on input values or other conditions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html",
    "href": "p_data_on_display_data_viz_types.html",
    "title": "8  Data Visualization Types",
    "section": "",
    "text": "8.1 Slides\nYou can review the slides below, which was used to record the video. Or you can jump directly to the following sections, which contain more detail about each type of visualization.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#intro",
    "href": "p_data_on_display_data_viz_types.html#intro",
    "title": "8  Data Visualization Types",
    "section": "8.2 Intro",
    "text": "8.2 Intro\nIn data visualization, there are often three main steps:\n\nWrangle and clean your data.\n\nPick the right type of visualization for your question.\n\nWrite the code to implement that visualization.\n\nIn this lesson, we focus on step #2: understanding which types of plots best suit particular kinds of questions. We’ll look at examples from the built-in tips dataset in Plotly, exploring:\n\nUnivariate (single variable) numeric data\n\nUnivariate (single variable) categorical data\n\nBivariate numeric vs numeric\n\nBivariate numeric vs categorical\n\nBivariate categorical vs categorical\n\nTime series\n\nWe then practice with another dataset (gapminder) to reinforce how to choose an effective plot for your data and your question.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#setup",
    "href": "p_data_on_display_data_viz_types.html#setup",
    "title": "8  Data Visualization Types",
    "section": "8.3 Setup",
    "text": "8.3 Setup\nRun the following code to set up the plotting environment and load the tips dataset from Plotly Express.\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\n\ntips = px.data.tips()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#univariate-numeric",
    "href": "p_data_on_display_data_viz_types.html#univariate-numeric",
    "title": "8  Data Visualization Types",
    "section": "9.1 1.1 Univariate Numeric",
    "text": "9.1 1.1 Univariate Numeric\nQuestions of the form: &gt; What is the distribution of a numeric variable?\n&gt; What’s its range? Its median? Where do most values lie?\nPossible visualizations include:\n\nHistogram – good for seeing the overall distribution of values.\n\nBox Plot – highlights the median, quartiles, and outliers.\n\nViolin Plot – combines a box plot with a “smoothed” histogram (density) shape.\n\nStrip/Jitter Plot – shows each individual point, slightly “jittered” so they’re not on top of each other.\n\n\n9.1.1 Histogram\n\nfig_hist = px.histogram(tips, x='tip')\nfig_hist.show()\n\n                                                \n\n\n\n\n9.1.2 Box Plot\n\nfig_box = px.box(tips, x='tip')\nfig_box.show()\n\n                                                \n\n\n\n\n9.1.3 Violin Plot (with Box + Points)\n\nfig_violin = px.violin(tips, x='tip', box=True, points='all')\nfig_violin.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#univariate-categorical",
    "href": "p_data_on_display_data_viz_types.html#univariate-categorical",
    "title": "8  Data Visualization Types",
    "section": "9.2 1.2 Univariate Categorical",
    "text": "9.2 1.2 Univariate Categorical\nQuestions of the form: &gt; How many observations fall into each category? &gt; In the case of the tips dataset, how many observations are there for each sex?\nPossible visualizations include:\n\nBar Chart – typically counts how many rows fall into each category.\n\nPie Chart – can be used if you have only a few categories and want to highlight proportions.\n\n\n9.2.1 Bar Chart\n\nfig_bar_cat = px.histogram(tips, x='sex', color='sex', color_discrete_sequence=['#deb221', '#2f828a'])\nfig_bar_cat.update_layout(showlegend=False)\nfig_bar_cat.show()\n\n                                                \n\n\n\n\n9.2.2 Pie Chart\n\nfig_pie_cat = px.pie(tips, names='sex', color='sex', values='tip',\n                     color_discrete_sequence=['#deb221', '#2f828a'])\nfig_pie_cat.update_layout(showlegend=False)\nfig_pie_cat.update_traces(textposition='none')\nfig_pie_cat.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#numeric-vs-numeric",
    "href": "p_data_on_display_data_viz_types.html#numeric-vs-numeric",
    "title": "8  Data Visualization Types",
    "section": "10.1 2.1 Numeric vs Numeric",
    "text": "10.1 2.1 Numeric vs Numeric\nTypical question: &gt; Do two numeric variables correlate? &gt; In the case of the tips dataset, does the total bill correlate with the tip?\nA scatter plot is usually the first choice.\n\nfig_scatter = px.scatter(tips, x='total_bill', y='tip')\nfig_scatter.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#numeric-vs-categorical",
    "href": "p_data_on_display_data_viz_types.html#numeric-vs-categorical",
    "title": "8  Data Visualization Types",
    "section": "10.2 2.2 Numeric vs Categorical",
    "text": "10.2 2.2 Numeric vs Categorical\nTypical questions: &gt; Which group has higher values on average?\n&gt; In the case of the tips dataset, do men or women tip more?\nVisual options:\n\nGrouped Histograms – separate histograms for each category (often overlaid).\n\nGrouped Box/Violin/Strip – one distribution plot per category.\n\nSummary Bar Chart – show mean (or median) for each category, optionally with error bars.\n\n\n10.2.1 Grouped Histogram\n\nfig_grouped_hist = px.histogram(tips, x='tip', color='sex', barmode='overlay',\n                                color_discrete_sequence=['#deb221', '#2f828a'])\nfig_grouped_hist.show()\n\n                                                \n\n\n\n\n10.2.2 Grouped Violin/Box/Strip\n\nfig_grouped_violin = px.violin(tips, y='sex', x='tip', color='sex',\n                               box=True, points='all',\n                               color_discrete_sequence=['#deb221', '#2f828a'])\nfig_grouped_violin.update_layout(showlegend=False)\nfig_grouped_violin.show()\n\n                                                \n\n\n\n\n10.2.3 Summary Bar (Mean + Std)\n\nsummary_df = tips.groupby('sex').agg({'tip': ['mean', 'std']}).reset_index()\nsummary_df.columns = ['sex', 'mean_tip', 'std_tip']\n\nfig_mean_bar = px.bar(\n    summary_df,\n    y='sex',\n    x='mean_tip',\n    error_x='std_tip',\n    color='sex',\n    color_discrete_sequence=['#deb221', '#2f828a'],\n)\nfig_mean_bar.update_layout(showlegend=False)\nfig_mean_bar.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#categorical-vs-categorical",
    "href": "p_data_on_display_data_viz_types.html#categorical-vs-categorical",
    "title": "8  Data Visualization Types",
    "section": "10.3 2.3 Categorical vs Categorical",
    "text": "10.3 2.3 Categorical vs Categorical\nTypical question: &gt; How do two categorical variables overlap?\n&gt; In the case of the tips dataset, how does the sex ratio differ by day of the week?\nVisual options:\n\nStacked Bar Chart – total counts, stacked by category.\n\nGrouped/Clustered Bar Chart – side-by-side bars per category.\n\nPercent Stacked Bar Chart – each bar scaled to 100% so you see proportions.\n\n\n10.3.1 Grouped Bar\n\nfig_grouped_bar = px.histogram(tips, x='day', color='sex', barmode='group',\n                               color_discrete_sequence=['#deb221', '#2f828a'])\nfig_grouped_bar.show()\n\n                                                \n\n\n\n\n10.3.2 Stacked Bar\n\nfig_stacked_bar = px.histogram(tips, x='day', color='sex',\n                               color_discrete_sequence=['#deb221', '#2f828a'])\nfig_stacked_bar.show()\n\n                                                \n\n\n\n\n10.3.3 Percent Stacked\n\nfig_percent_stacked = px.histogram(tips, x='day', color='sex',\n                                   barmode='stack', barnorm='percent',\n                                   color_discrete_sequence=['#deb221', '#2f828a'])\nfig_percent_stacked.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#how-does-gdp-per-capita-vary-across-continents",
    "href": "p_data_on_display_data_viz_types.html#how-does-gdp-per-capita-vary-across-continents",
    "title": "8  Data Visualization Types",
    "section": "12.1 4.1 How does GDP per capita vary across continents?",
    "text": "12.1 4.1 How does GDP per capita vary across continents?\n\nGDP per capita = numeric\n\nContinent = categorical\n\nA violin or box plot grouped by continent works well.\n\nfig_gdp_violin = px.violin(\n    gap_2007, \n    x=\"gdpPercap\", \n    y=\"continent\", \n    color=\"continent\", \n    box=True, \n    points=\"all\",\n    color_discrete_sequence=px.colors.qualitative.G10\n)\nfig_gdp_violin.update_layout(showlegend=False)\nfig_gdp_violin.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#is-there-a-relationship-between-a-countrys-gdp-per-capita-and-life-expectancy",
    "href": "p_data_on_display_data_viz_types.html#is-there-a-relationship-between-a-countrys-gdp-per-capita-and-life-expectancy",
    "title": "8  Data Visualization Types",
    "section": "12.2 4.2 Is there a relationship between a country’s GDP per capita and life expectancy?",
    "text": "12.2 4.2 Is there a relationship between a country’s GDP per capita and life expectancy?\n\nGDP per capita = numeric\n\nLife expectancy = numeric\n\nA scatter plot is typically best.\n\nfig_scatter_gdp_life = px.scatter(gap_2007, x='gdpPercap', y='lifeExp')\nfig_scatter_gdp_life.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#how-does-life-expectancy-vary-between-the-income-groups",
    "href": "p_data_on_display_data_viz_types.html#how-does-life-expectancy-vary-between-the-income-groups",
    "title": "8  Data Visualization Types",
    "section": "12.3 4.3 How does life expectancy vary between the income groups?",
    "text": "12.3 4.3 How does life expectancy vary between the income groups?\n\nLife expectancy = numeric\n\nIncome group = categorical\n\nUse grouped box, violin, or strip plots.\n\nfig_life_violin = px.violin(gap_2007, x=\"income_group\", y=\"lifeExp\", box=True, points=\"all\")\nfig_life_violin.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#what-is-the-relationship-between-continent-and-income-group",
    "href": "p_data_on_display_data_viz_types.html#what-is-the-relationship-between-continent-and-income-group",
    "title": "8  Data Visualization Types",
    "section": "12.4 4.4 What is the relationship between continent and income group?",
    "text": "12.4 4.4 What is the relationship between continent and income group?\n\nContinent = categorical\n\nIncome group = categorical\n\nA stacked or percent-stacked bar chart is a good choice.\n\nfig_continent_income = px.histogram(\n    gap_2007,\n    x='continent',\n    color='income_group',\n    barmode='stack',\n    color_discrete_sequence=['#deb221', '#2f828a']\n)\nfig_continent_income.show()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html",
    "href": "p_data_on_display_univariate.html",
    "title": "9  Univariate Graphs with Plotly Express",
    "section": "",
    "text": "9.1 Intro\nIn this lesson, you’ll learn how to create univariate graphs using Plotly Express. Univariate graphs are essential for understanding the distribution of a single variable, whether it’s categorical or quantitative.\nLet’s get started!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#learning-objectives",
    "href": "p_data_on_display_univariate.html#learning-objectives",
    "title": "9  Univariate Graphs with Plotly Express",
    "section": "9.2 Learning objectives",
    "text": "9.2 Learning objectives\n\nCreate bar charts, pie charts, and treemaps for categorical data using Plotly Express\nGenerate histograms for quantitative data using Plotly Express\nCustomize graph appearance and labels",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#imports",
    "href": "p_data_on_display_univariate.html#imports",
    "title": "9  Univariate Graphs with Plotly Express",
    "section": "9.3 Imports",
    "text": "9.3 Imports\nThis lesson requires plotly.express, pandas, and vega_datasets. Install them if you haven’t already.\n\nimport plotly.express as px\nimport pandas as pd\nfrom vega_datasets import data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#quantitative-data",
    "href": "p_data_on_display_univariate.html#quantitative-data",
    "title": "9  Univariate Graphs with Plotly Express",
    "section": "9.4 Quantitative Data",
    "text": "9.4 Quantitative Data\n\n9.4.1 Histogram\nHistograms are used to visualize the distribution of continuous variables.\nLet’s make a histogram of the tip amounts in the tips dataset.\n\ntips = px.data.tips()\ntips.head() # view the first 5 rows\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\nWe can see that the highest bar, corresponding to tips between 1.75 and 2.24, has a frequency of 55. This means that there were 55 tips between 1.75 and 2.24.\n\n\n\n\n\n\nSide-note\n\n\n\nNotice that plotly charts are interactive. You can hover over the bars to see the exact number of tips in each bin.\nTry playing with the buttons at the top right. The button to download the chart as a png is especially useful.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.4.2 Practice Q: Speed Distribution Histogram\nFollowing the example of the histogram of tips, create a histogram of the speed distribution (Speed_IAS_in_knots) using the birdstrikes dataset.\n\nbirdstrikes = data.birdstrikes()\nbirdstrikes.head()\n# Your code here\n\n\n\n\n\n\n\n\nAirport__Name\nAircraft__Make_Model\nEffect__Amount_of_damage\nFlight_Date\nAircraft__Airline_Operator\nOrigin_State\nWhen__Phase_of_flight\nWildlife__Size\nWildlife__Species\nWhen__Time_of_day\nCost__Other\nCost__Repair\nCost__Total_$\nSpeed_IAS_in_knots\n\n\n\n\n0\nBARKSDALE AIR FORCE BASE ARPT\nT-38A\nNone\n1/8/90 0:00\nMILITARY\nLouisiana\nClimb\nLarge\nTurkey vulture\nDay\n0\n0\n0\n300.0\n\n\n1\nBARKSDALE AIR FORCE BASE ARPT\nKC-10A\nNone\n1/9/90 0:00\nMILITARY\nLouisiana\nApproach\nMedium\nUnknown bird or bat\nNight\n0\n0\n0\n200.0\n\n\n2\nBARKSDALE AIR FORCE BASE ARPT\nB-52\nNone\n1/11/90 0:00\nMILITARY\nLouisiana\nTake-off run\nMedium\nUnknown bird or bat\nDay\n0\n0\n0\n130.0\n\n\n3\nNEW ORLEANS INTL\nB-737-300\nSubstantial\n1/11/90 0:00\nSOUTHWEST AIRLINES\nLouisiana\nTake-off run\nSmall\nRock pigeon\nDay\n0\n0\n0\n140.0\n\n\n4\nBARKSDALE AIR FORCE BASE ARPT\nKC-10A\nNone\n1/12/90 0:00\nMILITARY\nLouisiana\nClimb\nMedium\nUnknown bird or bat\nDay\n0\n0\n0\n160.0\n\n\n\n\n\n\n\n\n\nWe can view the help documentation for the function by typing px.histogram? in a cell and running it.\n\npx.histogram?\n\nFrom the help documentation, we can see that the px.histogram function has many arguments that we can use to customize the graph.\nLet’s make the histogram a bit nicer by adding a title, customizing the x axis label, and changing the color.\n\npx.histogram(\n    tips,\n    x=\"tip\",\n    labels={\"tip\": \"Tip Amount ($)\"},\n    title=\"Distribution of Tips\", \n    color_discrete_sequence=[\"lightseagreen\"]\n)\n\n                                                \n\n\nColor names are based on standard CSS color naming from Mozilla. You can see the full list here.\nAlternatively, you can use hex color codes, like #1f77b4. You can get these easily by using a color picker. Search for “color picker” on Google.\n\npx.histogram(\n    tips,\n    x=\"tip\",\n    labels={\"tip\": \"Tip Amount ($)\"},\n    title=\"Distribution of Tips\", \n    color_discrete_sequence=[\"#6a5acd\"]\n)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.4.3 Practice Q: Bird Strikes Histogram Custom\nUpdate your birdstrikes histogram to use a hex code color, add a title, and change the x-axis label to “Speed (Nautical Miles Per Hour)”.\n\n# Your code here",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#categorical-data",
    "href": "p_data_on_display_univariate.html#categorical-data",
    "title": "9  Univariate Graphs with Plotly Express",
    "section": "9.5 Categorical Data",
    "text": "9.5 Categorical Data\n\n9.5.1 Bar chart\nBar charts can be used to display the frequency of a single categorical variable.\nPlotly has a px.bar function that we will see later. But for single categorical variables, the function plotly wants you to use is actually px.histogram. (Statisticians everywhere are crying; histograms are supposed to be used for just quantitative data!)\nLet’s create a basic bar chart showing the distribution of sex in the tips dataset:\n\npx.histogram(tips, x='sex')   \n\n                                                \n\n\nLet’s add counts to the bars.\n\npx.histogram(tips, x='sex', text_auto= True)\n\n                                                \n\n\nWe can enhance the chart by adding a color axis, and customizing the labels and title.\n\npx.histogram(tips, x='sex', text_auto=True, color='sex', \n             labels={'sex': 'Gender'},\n             title='Distribution of Customers by Gender')\n\n                                                \n\n\nArguably, in this plot, we do not need the color axis, since the sex variable is already represented by the x axis. But public audiences like colors, so it may still be worth including.\nHowever, we should remove the legend. Let’s also use custom colors.\nFor this, we can first create a figure object, then use the .layout.update method from that object to update the legend.\n\ntips_by_sex = px.histogram(\n    tips,\n    x=\"sex\",\n    text_auto=True,\n    color=\"sex\",\n    labels={\"sex\": \"Gender\"},\n    title=\"Distribution of Customers by Gender\",\n    color_discrete_sequence=[\"#1f77b4\", \"#ff7f0e\"],\n)\n\ntips_by_sex.update_layout(showlegend=False)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.5.2 Practice Q: Bird Strikes by Phase of Flight\nCreate a bar chart showing the frequency of bird strikes by the phase of flight, When__Phase_of_flight. Add appropriate labels and a title. Use colors of your choice, and remove the legend.\n\n# Your code here\n\n\n\n:::\n\n9.5.2.1 Sorting categories\nIt is sometimes useful to dictate a specific order for the categories in a bar chart.\nConsider this bar chart of the election winners by district in the 2013 Montreal mayoral election.\n\nelection = px.data.election()\nelection.head()\n\n\n\n\n\n\n\n\ndistrict\nCoderre\nBergeron\nJoly\ntotal\nwinner\nresult\ndistrict_id\n\n\n\n\n0\n101-Bois-de-Liesse\n2481\n1829\n3024\n7334\nJoly\nplurality\n101\n\n\n1\n102-Cap-Saint-Jacques\n2525\n1163\n2675\n6363\nJoly\nplurality\n102\n\n\n2\n11-Sault-au-Récollet\n3348\n2770\n2532\n8650\nCoderre\nplurality\n11\n\n\n3\n111-Mile-End\n1734\n4782\n2514\n9030\nBergeron\nmajority\n111\n\n\n4\n112-DeLorimier\n1770\n5933\n3044\n10747\nBergeron\nmajority\n112\n\n\n\n\n\n\n\n\npx.histogram(election, x='winner')\n\n                                                \n\n\nLet’s define a custom order for the categories. “Bergeron” will be first, then “Joly” then “Coderre”.\n\ncustom_order = [\"Bergeron\", \"Joly\", \"Coderre\"]\nelection_chart = px.histogram(election, x='winner', category_orders={'winner': custom_order})\nelection_chart\n\n                                                \n\n\nWe can also sort the categories by frequency.\nWe can sort the categories by frequency using the categoryorder attribute of the x axis.\n\nelection_chart = px.histogram(election, x=\"winner\")\nelection_chart.update_xaxes(categoryorder=\"total descending\")\n\n                                                \n\n\nOr in ascending order:\n\nelection_chart = px.histogram(election, x=\"winner\")\nelection_chart.update_xaxes(categoryorder=\"total ascending\")\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.5.3 Practice Q: Sorted Origin State Bar Chart\nCreate a sorted bar chart showing the distribution of bird strikes by origin state. Sort the bars in descending order of frequency.\n\n# Your code here\n\n\n\n\n\n\n9.5.4 Horizontal bar chart\nWhen you have many categories, horizontal bar charts are often easier to read than vertical bar charts. To make a horizontal bar chart, simply use the y axis instead of the x axis.\n\npx.histogram(tips, y='day')\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.5.5 Practice Q: Horizontal Bar Chart of Origin State\nCreate a horizontal bar chart showing the distribution of bird strikes by origin state.\n\n# Your code here\n\n\n\n\n\n9.5.6 Pie chart\nPie charts are also useful for showing the proportion of categorical variables. They are best used when you have a small number of categories. For larger numbers of categories, pie charts are hard to read.\nLet’s make a pie chart of the distribution of tips by day of the week.\n\npx.pie(tips, names=\"day\")\n\n                                                \n\n\nWe can add labels to the pie chart to make it easier to read.\n\ntips_by_day = px.pie(tips, names=\"day\")\ntips_by_day_with_labels = tips_by_day.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\ntips_by_day_with_labels\n\n                                                \n\n\nThe legend is no longer needed, so we can remove it.\n\ntips_by_day_with_labels.update_layout(showlegend=False)\n\n                                                \n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nIf you forget how to make simple changes like this, don’t hesitate to consult the plotly documentation, Google or ChatGPT.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.5.7 Practice Q: Wildlife Size Pie Chart\nCreate a pie chart showing the distribution of bird strikes by wildlife size. Include percentages and labels inside the pie slices.\n\n# Your code here\n\n\n\n:::",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#summary",
    "href": "p_data_on_display_univariate.html#summary",
    "title": "9  Univariate Graphs with Plotly Express",
    "section": "9.6 Summary",
    "text": "9.6 Summary\nIn this lesson, you learned how to create univariate graphs using Plotly Express. You should now feel confident in your ability to create bar charts, pie charts, and histograms. You should also feel comfortable customizing the appearance of your graphs.\nSee you in the next lesson.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html",
    "href": "p_data_on_display_multivariate.html",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "",
    "text": "10.1 Introduction\nIn this lesson, you’ll learn how to create bivariate and multivariate graphs using Plotly Express. These types of graphs are essential for exploring relationships between two or more variables, whether they are quantitative or categorical. Understanding these relationships can provide deeper insights into your data.\nLet’s dive in!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#learning-objectives",
    "href": "p_data_on_display_multivariate.html#learning-objectives",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.2 Learning Objectives",
    "text": "10.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate scatter plots for quantitative vs. quantitative data\nGenerate grouped histograms and violin plots for quantitative vs. categorical data\nCreate grouped, stacked, and percent-stacked bar charts for categorical vs. categorical data\nVisualize time series data using bar charts and line charts\nCreate bubble charts to display relationships between three or more variables\nUse faceting to compare distributions across subsets of data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#imports",
    "href": "p_data_on_display_multivariate.html#imports",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.3 Imports",
    "text": "10.3 Imports\nThis lesson requires plotly.express, pandas, numpy, and vega_datasets. Install them if you haven’t already.\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nfrom vega_datasets import data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#numeric-vs.-numeric-data",
    "href": "p_data_on_display_multivariate.html#numeric-vs.-numeric-data",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.4 Numeric vs. Numeric Data",
    "text": "10.4 Numeric vs. Numeric Data\nWhen both variables are quantitative, scatter plots are an excellent way to visualize their relationship.\n\n10.4.1 Scatter Plot\nLet’s create a scatter plot to examine the relationship between total_bill and tip in the tips dataset. The tips dataset is included in Plotly Express and contains information about restaurant bills and tips that were collected by a waiter in a US restaurant.\nFirst, we’ll load the dataset and view the first five rows:\n\ntips = px.data.tips()\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nNext, we’ll create a basic scatter plot. We do this with the px.scatter function.\n\npx.scatter(tips, x='total_bill', y='tip')\n\n                                                \n\n\nFrom the scatter plot, we can observe that as the total bill increases, the tip amount tends to increase as well.\nLet’s enhance the scatter plot by adding labels and a title.\n\npx.scatter(\n    tips,\n    x=\"total_bill\",\n    y=\"tip\",\n    labels={\"total_bill\": \"Total Bill ($)\", \"tip\": \"Tip ($)\"},\n    title=\"Relationship Between Total Bill and Tip Amount\",\n)\n\n                                                \n\n\nRecall that you can see additional information about the function by typing px.scatter? in a cell and executing the cell.\n\npx.scatter?\n\n\n\n10.4.2 Practice Q: Life Expectancy vs. GDP Per Capita\n\n\n\n\n\n\nPractice\n\n\n\nUsing the Gapminder dataset (the 2007 subset, g_2007, defined below), create a scatter plot showing the relationship between gdpPercap (GDP per capita) and lifeExp (life expectancy).\nAccording to the plot, what is the relationship between GDP per capita and life expectancy?\n\ngapminder = px.data.gapminder()\ng_2007 = gapminder.query('year == 2007')\ng_2007.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n11\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.580338\nAFG\n4\n\n\n23\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.029526\nALB\n8\n\n\n35\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.367465\nDZA\n12\n\n\n47\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.231267\nAGO\n24\n\n\n59\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.379640\nARG\n32\n\n\n\n\n\n\n\nAccording to the plot, there is a positive relationship between GDP per capita and life expectancy, though it appears to level off at higher GDP values.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#numeric-vs.-categorical-data",
    "href": "p_data_on_display_multivariate.html#numeric-vs.-categorical-data",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.5 Numeric vs. Categorical Data",
    "text": "10.5 Numeric vs. Categorical Data\nWhen one variable is quantitative and the other is categorical, we can use grouped histograms, violin plots, or box plots to visualize the distribution of the quantitative variable across different categories.\n\n10.5.1 Grouped Histograms\nFirst, here’s how you can create a regular histogram of all tips:\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\nTo create a grouped histogram, use the color parameter to specify the categorical variable. Here, we’ll color the histogram by sex:\n\npx.histogram(tips, x='tip', color='sex')\n\n                                                \n\n\nBy default, the histograms for each category are stacked. To change this behavior, you can use the barmode parameter. For example, barmode='overlay' will create an overlaid histogram:\n\npx.histogram(tips, x=\"tip\", color=\"sex\", barmode=\"overlay\")\n\n                                                \n\n\nThis creates two semi-transparent histograms overlaid on top of each other, allowing for direct comparison of the distributions.\n\n\n10.5.2 Practice Q: Age Distribution by Gender\n\n\n\n\n\n\nPractice\n\n\n\nUsing the la_riots dataset from vega_datasets, create a grouped histogram of age by gender. Compare the age distributions between different genders.\nAccording to the plot, was the oldest victim male or female?\n\nla_riots = data.la_riots()\nla_riots.head()\n# Your code here\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n3\nBrian E.\nAndrew\n30.0\nMale\nBlack\n1992-04-30\nRosecrans & Chester avenues\nCompton\nOfficer-involved shooting\n-118.215390\n33.903457\n\n\n4\nVivian\nAustin\n87.0\nFemale\nBlack\n1992-05-03\n1600 W. 60th St.\nHarvard Park\nDeath\n-118.304741\n33.985667\n\n\n\n\n\n\n\nAccording to the plot, the oldest victim was female.\n\n\n\n\n10.5.3 Violin & Box Plots\nViolin plots are useful for comparing the distribution of a quantitative variable across different categories. They show the probability density of the data at different values and can include a box plot to summarize key statistics.\nFirst, let’s create a violin plot of all tips:\n\npx.violin(tips, y=\"tip\")\n\n                                                \n\n\nWe can add a box plot to the violin plot by setting the box parameter to True:\n\npx.violin(tips, y=\"tip\", box=True)\n\n                                                \n\n\nFor just the box plot, we can use px.box:\n\npx.box(tips, y=\"tip\")\n\n                                                \n\n\nTo add jitter points to the violin or box plots, we can use the points = 'all' parameter.\n\npx.violin(tips, y=\"tip\", points=\"all\")\n\n                                                \n\n\nNow, to create a violin plot of tips by gender, use the x parameter to specify the categorical variable:\n\npx.violin(tips, y=\"tip\", x=\"sex\", box=True)\n\n                                                \n\n\nWe can also add a color axis to differentiate the violins:\n\npx.violin(tips, y=\"tip\", x=\"sex\", color=\"sex\", box=True)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n10.5.4 Practice Q: Life Expectancy by Continent\nUsing the g_2007 dataset, create a violin plot showing the distribution of lifeExp by continent.\nAccording to the plot, which continent has the highest median country life expectancy?\n\ng_2007 = gapminder.query(\"year == 2007\")\ng_2007.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n11\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.580338\nAFG\n4\n\n\n23\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.029526\nALB\n8\n\n\n35\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.367465\nDZA\n12\n\n\n47\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.231267\nAGO\n24\n\n\n59\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.379640\nARG\n32\n\n\n\n\n\n\n\nAccording to the plot, Oceania has the highest median country life expectancy.\n\n\n\n\n10.5.5 Summary Bar Charts (Mean and Standard Deviation)\nSometimes it’s useful to display the mean and standard deviation of a quantitative variable across different categories. This can be visualized using a bar chart with error bars.\nFirst, let’s calculate the mean and standard deviation of tips for each gender. You have not yet learned how to do this, but you will in a later lesson.\n\n# Calculate the mean and standard deviation\nsummary_df = (\n    tips.groupby(\"sex\")\n    .agg(mean_tip=(\"tip\", \"mean\"), std_tip=(\"tip\", \"std\"))\n    .reset_index()\n)\nsummary_df\n\n\n\n\n\n\n\n\nsex\nmean_tip\nstd_tip\n\n\n\n\n0\nFemale\n2.833448\n1.159495\n\n\n1\nMale\n3.089618\n1.489102\n\n\n\n\n\n\n\nNext, we’ll create a bar chart using px.bar and add error bars using the error_y parameter:\n\n# Create the bar chart\npx.bar(summary_df, x=\"sex\", y=\"mean_tip\", error_y=\"std_tip\")\n\n                                                \n\n\nThis bar chart displays the average tip amount for each gender, with error bars representing the standard deviation.\n\n\n\n\n\n\nPractice\n\n\n\n10.5.6 Practice Q: Average Total Bill by Day\nUsing the tips dataset, create a bar chart of mean total_bill by day with standard deviation error bars. You should copy and paste the code from the example above and modify it to create this plot.\nAccording to the plot, which day has the highest average total bill?\n\ntips.head()  # View the tips dataset\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\nAccording to the plot, Sunday has the highest average bill.\n\n\n\n\n\n\n\n\nSide Note: Difference between px.bar and px.histogram\n\n\n\nNotice that this is the first time we are using the px.bar function. For past plots, we have used px.histogram to make bar charts.\nThe bar chart function generally expects that the numeric variable being plotted is already in it’s own column, while the histogram function does the grouping for you.\nFor example, in the cell below, we use px.histogram to make a bar chart of the sex column. The resulting plot compares the number of male and female customers in the dataset.\n\npx.histogram(tips, x='sex')\n\n                                                \n\n\nTo make the same plot using px.bar, we first need to group by the sex column and count the number of rows for each sex.\n\nsex_counts = tips['sex'].value_counts().reset_index()\nsex_counts\n\n\n\n\n\n\n\n\nsex\ncount\n\n\n\n\n0\nMale\n157\n\n\n1\nFemale\n87\n\n\n\n\n\n\n\nWe can then plot the day column using px.bar:\n\npx.bar(sex_counts, x=\"sex\", y=\"count\")\n\n                                                \n\n\nThis produces a bar chart with one bar for each sex.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#categorical-vs.-categorical-data",
    "href": "p_data_on_display_multivariate.html#categorical-vs.-categorical-data",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.6 Categorical vs. Categorical Data",
    "text": "10.6 Categorical vs. Categorical Data\nWhen both variables are categorical, bar charts with a color axis are effective for visualizing the frequency distribution across categories. We will focus on three types of bar charts: stacked bar charts, percent-stacked bar charts, and grouped/clustered bar charts.\n\n10.6.1 Stacked Bar Charts\nStacked bar charts show the total counts and the breakdown within each category. To make a stacked bar chart, use the color parameter to specify the categorical variable:\n\npx.histogram(\n    tips,\n    x='day',\n    color='sex'\n)\n\n                                                \n\n\nLet’s add numbers to the bars to show the exact counts, and also improve the color palette with custom colors.\n\npx.histogram(\n    tips,\n    x=\"day\",\n    color=\"sex\",\n    text_auto=True,\n    color_discrete_sequence=[\"#deb221\", \"#2f828a\"],\n)\n\n                                                \n\n\nThis stacked bar chart shows the total number of customers each day, broken down by gender.\n\n\n\n\n\n\nPractice\n\n\n\n10.6.2 Practice Q: High and Low Income Countries by Continent\nUsing the g_2007_income dataset, create a stacked bar chart showing the count of high and low income countries in each continent.\n\ngap_dat = px.data.gapminder()\n\ng_2007_income = (\n    gap_dat.query(\"year == 2007\")\n    .drop(columns=[\"year\", \"iso_alpha\", \"iso_num\"])\n    .assign(\n        income_group=lambda df: np.where(\n            df.gdpPercap &gt; 15000, \"High Income\", \"Low & Middle Income\"\n        )\n    )\n)\n\ng_2007_income.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nlifeExp\npop\ngdpPercap\nincome_group\n\n\n\n\n11\nAfghanistan\nAsia\n43.828\n31889923\n974.580338\nLow & Middle Income\n\n\n23\nAlbania\nEurope\n76.423\n3600523\n5937.029526\nLow & Middle Income\n\n\n35\nAlgeria\nAfrica\n72.301\n33333216\n6223.367465\nLow & Middle Income\n\n\n47\nAngola\nAfrica\n42.731\n12420476\n4797.231267\nLow & Middle Income\n\n\n59\nArgentina\nAmericas\n75.320\n40301927\n12779.379640\nLow & Middle Income\n\n\n\n\n\n\n\n\n\n\n\n10.6.3 Percent-Stacked Bar Charts\nTo show proportions instead of counts, we can create percent-stacked bar charts by setting the barnorm parameter to 'percent':\n\n# Create the percent-stacked bar chart\npx.histogram(tips, x=\"day\", color=\"sex\", barnorm=\"percent\")\n\n                                                \n\n\nThis chart normalizes the bar heights to represent percentages, showing the proportion of each gender for each day.\nWe can also add text labels to the bars to show the exact percentages:\n\npx.histogram(tips, x=\"day\", color=\"sex\", barnorm=\"percent\", text_auto=\".1f\")\n\n                                                \n\n\nThe symbol .1f in the text_auto parameter formats the text labels to one decimal place.\n\n\n\n\n\n\nPractice\n\n\n\n10.6.4 Practice Q: Proportion of High and Low Income Countries by Continent\nAgain using the g_2007_income dataset, create a percent-stacked bar chart showing the proportion of high and low income countries in each continent. Add text labels to the bars to show the exact percentages.\nAccording the plot, which continent has the highest proportion of high income countries? Are there any limitations to this plot?\n\n# Your code here\n\n\n\n\n\n10.6.5 Clustered Bar Charts\nFor clustered bar charts, set the barmode parameter to 'group' to place the bars for each category side by side:\n\npx.histogram(tips, x=\"day\", color=\"sex\", barmode=\"group\")\n\n                                                \n\n\nThis layout makes it easier to compare values across categories directly.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#time-series-data",
    "href": "p_data_on_display_multivariate.html#time-series-data",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.7 Time Series Data",
    "text": "10.7 Time Series Data\nTime series data represents observations collected at different points in time. It’s crucial for analyzing trends, patterns, and changes over time. Let’s explore some basic time series visualizations using Nigeria’s population data from the Gapminder dataset.\nFirst, let’s prepare our data:\n\n# Load the Gapminder dataset\ngapminder = px.data.gapminder()\n\n# Subset the data for Nigeria\nnigeria_pop = gapminder.query('country == \"Nigeria\"')[['year', 'pop']]\nnigeria_pop\n\n\n\n\n\n\n\n\nyear\npop\n\n\n\n\n1128\n1952\n33119096\n\n\n1129\n1957\n37173340\n\n\n1130\n1962\n41871351\n\n\n1131\n1967\n47287752\n\n\n1132\n1972\n53740085\n\n\n1133\n1977\n62209173\n\n\n1134\n1982\n73039376\n\n\n1135\n1987\n81551520\n\n\n1136\n1992\n93364244\n\n\n1137\n1997\n106207839\n\n\n1138\n2002\n119901274\n\n\n1139\n2007\n135031164\n\n\n\n\n\n\n\n\n10.7.1 Bar Chart\nA bar chart can be used to plot time series data.\n\n# Bar chart\npx.bar(nigeria_pop, x=\"year\", y=\"pop\")\n\n                                                \n\n\nThis bar chart gives us a clear view of how Nigeria’s population has changed over the years, with each bar representing the population at a specific year.\n\n\n10.7.2 Line Chart\nA line chart is excellent for showing continuous changes over time:\n\n# Line chart\npx.line(nigeria_pop, x=\"year\", y=\"pop\")\n\n                                                \n\n\nThe line chart connects the population values, making it easier to see the overall trend of population growth.\nAdding markers to a line chart can highlight specific data points:\n\n# Line chart with points\npx.line(nigeria_pop, x='year', y='pop', markers=True)\n\n                                                \n\n\nWe can also compare the population growth of multiple countries by adding a color parameter:\n\nnigeria_ghana = gapminder.query('country in [\"Nigeria\", \"Ghana\"]')\npx.line(nigeria_ghana, x=\"year\", y=\"pop\", color=\"country\", markers=True)\n\n                                                \n\n\nThis chart allows us to compare the population trends of Nigeria and Ghana over time.\n\n\n\n\n\n\nPractice\n\n\n\n10.7.3 Practice Q: GDP per Capita Time Series\nUsing the Gapminder dataset, create a time series visualization for the GDP per capita of Iraq.\n\n# Your code here\n\nWhat happened to Iraq in the 1980s that might explain the graph shown?",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#plots-with-three-or-more-variables",
    "href": "p_data_on_display_multivariate.html#plots-with-three-or-more-variables",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.8 Plots with three or more variables",
    "text": "10.8 Plots with three or more variables\nAlthough bivariate visualizations are the most common types of visualizations, plots with three or more variables are also sometimes useful. Let’s explore a few examples.\n\n10.8.1 Bubble Charts\nBubble charts show the relationship between three variables by mapping the size of the points to a third variable. Below, we plot the relationship between gdpPercap and lifeExp with the size of the points representing the population of the country.\n\npx.scatter(g_2007, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\")\n\n                                                \n\n\nWe can easily spot the largest countries by population, such as China, India, and the United States. We can also add a color axis to differentiate between continents:\n\npx.scatter(g_2007, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\", color=\"continent\")\n\n                                                \n\n\nNow we have four different variables being plotted:\n\ngdpPercap on the x-axis\nlifeExp on the y-axis\npop as the size of the points\ncontinent as the color of the points\n\n\n\n\n\n\n\nPractice\n\n\n\n10.8.2 Practice Q: Tips Bubble Chart\nUsing the tips dataset, create a bubble chart showing the relationship between total_bill and tip with the size of the points representing the size of the party, and the color representing the day of the week.\nUse the plot to answer the question:\n\nThe highest two tip amounts were on which days and what was the table size?\n\n\ntips.head()\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n10.8.3 Facet Plots\nFaceting splits a single plot into multiple plots, with each plot showing a different subset of the data. This is useful for comparing distributions across subsets.\nFor example, we can facet the bubble chart by continent:\n\npx.scatter(\n    g_2007,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    facet_col=\"continent\",\n)\n\n                                                \n\n\nWe can change the arrangement of the facets by changing the facet_col_wrap parameter. For example, facet_col_wrap=2 will wrap the facets into two columns:\n\npx.scatter(\n    g_2007,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    facet_col=\"continent\",\n    facet_col_wrap=2,\n)\n\n                                                \n\n\nSimilarly, we can facet the violin plots of tips by day of the week:\n\npx.violin(\n    tips,\n    x=\"sex\",\n    y=\"tip\",\n    color=\"sex\",\n    facet_col=\"day\",\n    facet_col_wrap=2,\n)\n\n                                                \n\n\nFaceting allows us to compare distributions across different days, providing more granular insights.\n\n\n\n\n\n\nPractice\n\n\n\n10.8.4 Practice Q: Tips Facet Plot\nUsing the tips dataset, create a percent-stacked bar chart of the time column, colored by the sex column, and facetted by the day column.\nWhich day-time has the highest proportion of male customers (e.g. Friday Lunch, Saturday Dinner, etc.)?\n\ntips.head()\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#summary",
    "href": "p_data_on_display_multivariate.html#summary",
    "title": "10  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "10.9 Summary",
    "text": "10.9 Summary\nIn this lesson, you learned how to create bivariate and multivariate graphs using Plotly Express. Understanding these visualization techniques will help you explore and communicate relationships in your data more effectively.\nSee you in the next lesson!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html",
    "href": "p_tools_installing_python.html",
    "title": "11  Installing Python",
    "section": "",
    "text": "11.1 Introduction\nSo far in our learning sequence, you’ve been working mostly in Google Colab, a convenient, cloud-based environment for running Python code. Now it’s time to tackle working locally on your own machine. This allows you to work offline and gives you more control over your development environment.\nWe’ll guide you through installing Python 3.12.0 to ensure compatibility with the examples and exercises we’ll cover. Let’s get started!\nBefore we proceed, we’ll split the instructions into two tracks:\nPlease follow the instructions specific to your operating system.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#introduction",
    "href": "p_tools_installing_python.html#introduction",
    "title": "11  Installing Python",
    "section": "",
    "text": "Windows Users\nmacOS Users",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#windows-users",
    "href": "p_tools_installing_python.html#windows-users",
    "title": "11  Installing Python",
    "section": "11.2 Windows Users",
    "text": "11.2 Windows Users\n\n11.2.1 Step 1: Check Your Current Python Version\n\nSearch for “Command Prompt” in your start menu.\nInto the terminal window type python --version and press Enter.\nIf Python is installed, it will display the version number.\nIf you already have version 3.12.0 installed, you can skip the rest of the steps. If you have any other version (higher or lower), proceed to the next step to install 3.12.0.\n\n\n\n11.2.2 Step 2: Download Python 3.12.0\n\nOpen your web browser.\nGo to Google and search for “Python 3.12.0”.\nIn the search results, find the link to python.org that mentions Python 3.12.0.\nScroll down to the bottom of the Python 3.12.0 page to find the “Windows installer (64-bit)”. Then click on the link to download the installer.\n\n\n\n11.2.3 Step 3: Install Python\n\nLocate the downloaded file (usually in your Downloads folder).\nDouble-click the installer to run it.\nImportant: Check the box that says “Add Python 3.12 to PATH” at the bottom of the installer window.\nIf you have the option, also elect to use admin privileges.\nAt the end you may be asked whether to disable path length limit. Click yes to this too.\nWait for the installation to finish.\n\n\n\n11.2.4 Step 4: Verify the Installation\n\nClose your old command prompt window if it is still open.\nIn your start menu, again search for “Command Prompt” and open a new terminal window.\nType python --version and press Enter.\nYou should see Python 3.12.0. (You may see a newer version if you already had Python on your computer. That’s okay. From our IDE, we will be able to select the correct version of Python that we just installed.)\n\n\n\n11.2.5 Step 5: Run Python Locally\n\nIn Command Prompt, type python and press Enter.\nAt the &gt;&gt;&gt; prompt, type 2 + 2 and press Enter.\nYou should see 4.\nType exit() and press Enter.\n\nYay! You’ve successfully installed Python and you ran your first local Python command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#macos-users",
    "href": "p_tools_installing_python.html#macos-users",
    "title": "11  Installing Python",
    "section": "11.3 macOS Users",
    "text": "11.3 macOS Users\n\n11.3.1 Step 1: Check Your Current Python Version\n\nGo to Applications &gt; Utilities &gt; Terminal.\nType python3 --version and press Return.\nIf Python is installed, it will display the version number.\nYou may get a pop-up asking to install command line developer tools if you do not have them. Go ahead and accept and install that.\nIf your computer does not have Python installed, you will get an error.\nOtherwise, you should see a version number.\nIf you already have version 3.12.0 installed, you can skip the rest of the steps. If you have any other version (higher or lower), proceed to the next step to install 3.12.0.\n\n\n\n11.3.2 Step 2: Download Python 3.12.0\n\nOpen your web browser.\nGo to Google and search for “Python 3.12.0”.\nIn the search results, find the link to python.org that mentions Python 3.12.0.\nEnsure the URL is from www.python.org to avoid unofficial sources.\nScroll down to the bottom of the Python 3.12.0 page.\nUnder the “Files” section, find “macOS 64-bit universal2 installer”.\nClick on the link to download the installer.\n\n\n\n11.3.3 Step 3: Install Python 3.12.0\n\nLocate the downloaded file (usually in your Downloads folder).\nDouble-click the installer to run it.\nWait for the installation to finish.\nClick “Close” once the installation is complete.\n\n\n\n11.3.4 Step 4: Verify the Installation\n\nClose your old terminal window if it is still open.\nAgain go to Applications &gt; Utilities &gt; Terminal.\nType python3 --version and press Return.\nYou should see Python 3.12.0.\n\n\n\n11.3.5 Step 5: Run Python Locally\n\nIn Terminal, type python3 and press Return.\nAt the &gt;&gt;&gt; prompt, type 2 + 2 and press Return.\nYou should see 4.\nType exit() and press Return.\n\nYay! You’ve successfully installed Python 3.12.0 on your Mac and you ran your first local Python command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html",
    "href": "p_tools_using_vscode.html",
    "title": "12  Installing and Using VS Code",
    "section": "",
    "text": "12.1 Introduction\nIn this lesson, we will explore Visual Studio Code (VS Code), a powerful and versatile editor for writing, running, and debugging Python code. VS Code is widely used due to its rich feature set and extensive extension library, which makes coding more efficient and enjoyable.\nNote that this lesson is better consumed as a video, since there are many Graphic-user-interface steps that are hard to describe in writing.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#learning-objectives",
    "href": "p_tools_using_vscode.html#learning-objectives",
    "title": "12  Installing and Using VS Code",
    "section": "12.2 Learning Objectives",
    "text": "12.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nOpen VS Code and navigate to the editor, terminal, and explorer tabs.\nCreate a new Python file and run it using the Python extension.\nUse the Command Palette to search for and select a color theme.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#installing-vs-code",
    "href": "p_tools_using_vscode.html#installing-vs-code",
    "title": "12  Installing and Using VS Code",
    "section": "12.3 Installing VS Code",
    "text": "12.3 Installing VS Code\nVS Code is available for Windows, macOS, and Linux. Search for “VS Code” in your favorite search engine and go to the official website, code.visualstudio.com. Download the version for your computer’s operating system.\nFor macOs users, after installing VS Code, you may need to drag the icon to your applications folder.\nFrom your applications folder, open VS Code.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#navigating-the-explorer-tab",
    "href": "p_tools_using_vscode.html#navigating-the-explorer-tab",
    "title": "12  Installing and Using VS Code",
    "section": "12.4 Navigating the Explorer Tab",
    "text": "12.4 Navigating the Explorer Tab\nThe Explorer tab displays the files and folders in your current workspace. When you open VS Code for the first time, it may indicate that you have not yet opened a folder. Most of our work in Python will be organized in folders (also known as workspaces) that contain multiple files.\nLet’s create our first workspace:\n\nOn your computer, navigate to your desktop or another memorable location.\nCreate a new folder called first_python_workspace.\nIn VS Code, click on the “Open Folder” button and locate your newly created folder. Alternatively, you can drag the folder into the VS Code window.\n\nNow that your workspace is open in VS Code, you can create a new file:\n\nIn the Explorer tab, right-click inside the workspace folder.\nSelect “New File”.\nName the file first_script.py. The file will automatically open in the editor.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#writing-and-saving-python-code",
    "href": "p_tools_using_vscode.html#writing-and-saving-python-code",
    "title": "12  Installing and Using VS Code",
    "section": "12.5 Writing and Saving Python Code",
    "text": "12.5 Writing and Saving Python Code\nLet’s write some Python code in your new file:\nprint(2 + 2)\nTo save the file, press Ctrl + S (or Cmd + S on macOS).",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#installing-the-python-extension",
    "href": "p_tools_using_vscode.html#installing-the-python-extension",
    "title": "12  Installing and Using VS Code",
    "section": "12.6 Installing the Python Extension",
    "text": "12.6 Installing the Python Extension\nTo run Python code within VS Code, we need to install the Python extension:\n\nClick on the Extensions tab on the left sidebar (it looks like four squares).\nIn the search bar at the top, type “Python”.\nFind the extension named “Python” published by Microsoft, and click Install.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#running-your-python-script",
    "href": "p_tools_using_vscode.html#running-your-python-script",
    "title": "12  Installing and Using VS Code",
    "section": "12.7 Running Your Python Script",
    "text": "12.7 Running Your Python Script\nWith the Python extension installed, you can now run your script:\n\nOpen first_script.py if it’s not already open.\nClick on the Run icon (a play button) in the top right corner of the editor.\n\nAlternatively, you can right-click inside the editor and select RUn python then “Run Python File in Terminal”.\n\nA terminal window will open at the bottom of the screen, and your code will execute. You should see the output:\n4",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#using-the-command-palette",
    "href": "p_tools_using_vscode.html#using-the-command-palette",
    "title": "12  Installing and Using VS Code",
    "section": "12.8 Using the Command Palette",
    "text": "12.8 Using the Command Palette\nThe Command Palette is a powerful feature in VS Code that allows you to access various commands and settings:\nTo access it, click on the search bar at the top of vscode window, then either press &gt; or select the ‘show and run commands’ option.\nLet’s practice changing the color theme:\n\nType ‘theme’ and select ‘Preferences: Color Theme’.\nUse the up and down arrow keys to cycle through the available color themes.\nPress Enter to select your preferred theme.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#wrap-up",
    "href": "p_tools_using_vscode.html#wrap-up",
    "title": "12  Installing and Using VS Code",
    "section": "12.9 Wrap Up",
    "text": "12.9 Wrap Up\nIn this lesson, we’ve:\n\nInstalled VS Code and set up a workspace.\nCreated and saved a Python script.\nInstalled the Python extension to enable running and debugging code.\nUsed the Command Palette to customize the editor’s appearance.\n\nHappy coding!",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html",
    "href": "p_tools_venv.html",
    "title": "13  Folders and Virtual Environments",
    "section": "",
    "text": "13.1 Introduction\nIn this lesson, we will explore virtual environments in Python using Visual Studio Code (VS Code). We’ll create a new workspace, set up a virtual environment, and install packages specific to our project.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#learning-objectives",
    "href": "p_tools_venv.html#learning-objectives",
    "title": "13  Folders and Virtual Environments",
    "section": "13.2 Learning Objectives",
    "text": "13.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nCreate a new workspace in VS Code\nUnderstand the concept of virtual environments\nCreate and use a virtual environment in VS Code\nInstall and use packages within a virtual environment",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#creating-a-new-workspace",
    "href": "p_tools_venv.html#creating-a-new-workspace",
    "title": "13  Folders and Virtual Environments",
    "section": "13.3 Creating a New Workspace",
    "text": "13.3 Creating a New Workspace\nOn your desktop or in your Documents folder, create a new folder and name it graph_courses_python. This is going to be the main folder for many of the projects for this course so make sure you put it in an easy to reach place.\nOpen VS Code.\nGo to File &gt; Open Folder.\nNavigate to the graph_courses_python folder you just created and select it.\nNow create a new script and call it test_cowsay.py.\nType print(2 + 2) in the file then run it by clicking the run button to make sure everything is working.\nNext, let’s try to import a package we haven’t installed yet. Add the following line to your file:\n\nimport cowsay\n\ncowsay.cow(\"Hello, World!\")\n\nIf you try to run this, you’ll get an error.\nThis will not work because we haven’t installed the cowsay package yet. And to install it properly, we’ll need to use virtual environments.\n\n13.3.1 Vocab: Environments & Interpreter\n\nAn environment is a folder that contains a specific version of Python and any packages you install.\nThe Python Interpreter is the specific",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#creating-a-virtual-environment",
    "href": "p_tools_venv.html#creating-a-virtual-environment",
    "title": "13  Folders and Virtual Environments",
    "section": "13.4 Creating a Virtual Environment",
    "text": "13.4 Creating a Virtual Environment\n\nOpen the Command Palette\nType Python: Create Environment and select it.\nChoose Venv as the environment type.\nSelect the Python interpreter you want to use (e.g., Python 3.12.0).\n\nYou should now see a new folder called .venv. This is the virtual environment. Inside it is a folder called lib, which contains packages.\nNext, tell VS Code to use this virtual environment:\n\nOpen the Command Palette again.\nType Python: Select Interpreter and choose it.\nSelect the interpreter associated with your virtual environment (it should be listed under .venv).\n\nNow we’ve created and selected our virtual environment. We can install packages without affecting other projects.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#installing-packages",
    "href": "p_tools_venv.html#installing-packages",
    "title": "13  Folders and Virtual Environments",
    "section": "13.5 Installing Packages",
    "text": "13.5 Installing Packages\nLet’s install the cowsay package.\n\nOpen a new terminal in VS Code. You can do this with the terminal file menu option then selecting ‘new terminal’.\nEnsure the terminal is using your virtual environment. You can do this by hovering over the terminal icon in terminal window. It should mention that there is an activated environment for ….venv among other things\nRun the following command:\n\n#| eval: false\npip install cowsay\nOccasionally, you might encounter an issue where pip is not recognized in the terminal. This is likely due to a temporary glitch in VS Code’s environment detection. If this happens, try the following steps:\n\nClose and reopen VS Code.\nUse the Command Palette to select the Python interpreter again.\nOpen a new terminal.\n\nThese steps should resolve the issue and restore access to pip. If the problem persists, it may indicate a more complex configuration problem that requires further investigation.\nNow we should be able to use the cowsay package. Open test_cowsay.py and click the run button to execute the script.\nYou should see a cow saying hello!",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#summary-of-key-steps",
    "href": "p_tools_venv.html#summary-of-key-steps",
    "title": "13  Folders and Virtual Environments",
    "section": "13.6 Summary of Key Steps",
    "text": "13.6 Summary of Key Steps\nCongrats! You have now created a virtual environment and installed a package.\nThese are key steps for any new Python project:\n\nFolder: Create a project folder.\nEnvironment: Set up a virtual environment.\nInterpreter: Select the appropriate Python interpreter.\nLibraries: Install the necessary packages.\n\nRemember the acronym FEIL to help you recall these steps. (If you don’t complete these steps you increase your chances of “FEIL”ure 😅)\nIt’s a bit of a pain to have to set up a virtual environment every time you start a new project, but it’s a good habit to get into.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#demonstrating-environment-isolation",
    "href": "p_tools_venv.html#demonstrating-environment-isolation",
    "title": "13  Folders and Virtual Environments",
    "section": "13.7 Demonstrating Environment Isolation",
    "text": "13.7 Demonstrating Environment Isolation\nLet’s demonstrate that the virtual environment is isolated.\n\nOpen your previous workspace my_first_workspace with the File &gt; Open Folder menu option.\nCreate a Python file and try to use the cowsay package:\n::: {#ed072cbb .cell execution_count=2} {.python .cell-code}  import cowsay :::\n\nThis will probably not work because cowsay is not installed in that environment. If it does work, it means you have cowsay installed globally, which is okay.\nNow, let’s return to our main folder/workspace. This is where you’ll conduct most of your analysis for this course.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#installing-course-packages",
    "href": "p_tools_venv.html#installing-course-packages",
    "title": "13  Folders and Virtual Environments",
    "section": "13.8 Installing Course Packages",
    "text": "13.8 Installing Course Packages\nAs a final step, let’s install the packages we’ll need for the course. While we could install each package as we encounter it, it’s more efficient to install them all at once. In the terminal, run the following command. Type these very carefully.\n#| eval: false\npip install plotly pandas jupyter ipykernel kaleido itables\n\npandas: Data manipulation library.\nplotly: Visualization library.\njupyter and ipykernel: Allow us to use Quarto to display our plots.\nkaleido: Library for saving plots in different formats.\nitables: Library for displaying tables in Quarto.\n\nWhen its done installing, your cursor in the terminal should be active again. e.g. you should be able to press enter to start a new command.\nKeep this list of packages handy for future reference, as you’ll likely need them for most projects.\nThis command will install all the required packages in one go. If your installation stops at some point, try rerunning the command. Sometimes network issues may cause the installation to fail. If it freezes for more than 10 minutes, close the terminal and rerun the command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#conclusion",
    "href": "p_tools_venv.html#conclusion",
    "title": "13  Folders and Virtual Environments",
    "section": "13.9 Conclusion",
    "text": "13.9 Conclusion\nYou’ve now learned how to create a workspace, set up a virtual environment, install packages, and use them in your Python projects. Remember that each project should have its own virtual environment to keep dependencies isolated and manageable.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html",
    "href": "p_tools_quarto.html",
    "title": "14  Using Quarto",
    "section": "",
    "text": "14.1 Introduction\nA significant part of your role as a data analyst involves communicating results to others through reports. Quarto is one of the most powerful and versatile tools for producing such reports. It enables you to generate dynamic documents by combining formatted text and results produced by code. With Quarto, you can create documents in various formats such as HTML, PDF, Word, PowerPoint slides, web dashboards, and many others.\nMost of our documents in the GRAPH courses are actually written in Quarto!\nIn this lesson, we will cover the basics of this powerful tool.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#learning-objectives",
    "href": "p_tools_quarto.html#learning-objectives",
    "title": "14  Using Quarto",
    "section": "14.2 Learning Objectives",
    "text": "14.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nCreate and render a Quarto document that includes Python code and narrative text.\nOutput documents in multiple formats, including HTML, PDF, Word, etc.\nUnderstand basic Markdown syntax.\nUse code chunk options to control code execution and output display.\nUse Python packages to display tables and figures in Quarto documents.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#installing-quarto",
    "href": "p_tools_quarto.html#installing-quarto",
    "title": "14  Using Quarto",
    "section": "14.3 Installing Quarto",
    "text": "14.3 Installing Quarto\nTo get started, you first need to install Quarto.\nSearch for “quarto download” in your favorite search engine. Follow the results to the “Quarto.org” website, and then follow the instructions for your operating system. (We are not providing a direct link here since the exact link may change over time).\nAfter installing, you can check that it is installed by running the following command in the command line or terminal:\nquarto --version\nNow that Quarto is installed, use it to install the tinytex package, which we will need to compile our PDFs:\nquarto install tinytex\nTo use all the features of Quarto in VSCode, we need to install the Quarto extension and the Jupyter extension. You can install these in the Extensions tab of VSCode. Be sure to install the official versions of these extensions. Quarto is published by Quarto, and Jupyter is published by Microsoft.\nThat’s a lot of installations, but fear not—you only have to do these steps once on your computer.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#project-setup",
    "href": "p_tools_quarto.html#project-setup",
    "title": "14  Using Quarto",
    "section": "14.4 Project Setup",
    "text": "14.4 Project Setup\nTo begin, open your graph_courses_python project in VSCode.\nIf you did not watch the previous video explaining project setup, please do so now. In that video, we explain how to create a project folder, create a virtual environment, select an interpreter, and install the jupyter, ipykernel, kaleido, itables, plotly, and pandas packages.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#create-a-new-document",
    "href": "p_tools_quarto.html#create-a-new-document",
    "title": "14  Using Quarto",
    "section": "14.5 Create a New Document",
    "text": "14.5 Create a New Document\nA Quarto document is a simple text file with the .qmd extension.\nTo create a new Quarto document, create a new file and save it with a .qmd extension, for example, first_quarto_doc.qmd.\nAdd two sections to your document with the following text:\n# Section 1\n\nHello\n\n\n# Section 2\n\nWorld",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#adding-code-chunks",
    "href": "p_tools_quarto.html#adding-code-chunks",
    "title": "14  Using Quarto",
    "section": "14.6 Adding Code Chunks",
    "text": "14.6 Adding Code Chunks\nYou can add code chunks to your document by using the following syntax with the shortcut Cmd + Shift + I (on Mac) or Ctrl + Shift + I (on Windows). Alternatively, you can click on the “…” button at the top right of the screen.\nLet’s create a code chunk that adds two numbers together and displays the result:\n\n2 + 2\n\n4\n\n\nYou should see a “Run Cell” button in the toolbar. Click this to run the code chunk.\nVSCode may prompt you to install the ipykernel package if you have not yet installed it in your current environment. Go ahead and install it.\nNow practice adding one more code chunk at the end of the document that multiplies 3 by 3.\nAs you add these, you should see the buttons “Run Next Cell” and “Run Above” also appear.\nThere are two shortcuts you should also get used to:\n\nCmd + Enter (Mac) or Ctrl + Enter (Windows/Linux) to run the code chunk.\nOption + Enter (Mac) or Alt + Enter (Windows/Linux) to run the current line or the highlighted section of code.\n\nTo test these, add multiple lines of code to one code chunk, then practice running the whole chunk with the first shortcut, and line by line with the second.\nAs you can see, we can use Quarto as an interactive document similar to a Jupyter notebook or Google Colab. But what makes it really shine is its ability to output to many formats.\nLater in the lesson, we will see how to use this functionality.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#quarto-document-header-yaml",
    "href": "p_tools_quarto.html#quarto-document-header-yaml",
    "title": "14  Using Quarto",
    "section": "14.7 Quarto Document Header (YAML)",
    "text": "14.7 Quarto Document Header (YAML)\nAt the top of the document, let’s add a YAML section. This is where we can specify details about the document, such as its title, author, and format.\n---\ntitle: \"My First Quarto Document\"\nauthor: \"Your Name\"\nformat: html\n---\nFor now, we will just use the html format.\nTo render the document, click on the “Render” button in the top right of the screen.\n(For things to render properly, you need the jupyter package. Watch our video on setting up a virtual environment if you have not done this yet and are not sure how to install packages.)\nYou should see a new tab open in your VSCode with the rendered document.\nIf you go to the explorer, you should see a new file called first_quarto_doc.html.\nSo now we have the main elements of a Quarto document:\n\nThe YAML header\nSection headers\nText\nCode chunks\nThe outputs of those code chunks\n\nThese elements together make Quarto a very powerful tool for reporting.\nYou can also customize the output format with additional options. For example, to embed resources directly into the HTML file, you can modify the format section in the YAML header:\nformat:\n  html:\n    embed-resources: true",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#output-formats",
    "href": "p_tools_quarto.html#output-formats",
    "title": "14  Using Quarto",
    "section": "14.8 Output Formats",
    "text": "14.8 Output Formats\nAs we discussed earlier, a powerful feature of Quarto is its ability to output to many formats.\nYou can change the format value in the YAML header to experiment with other formats.\nTry the following formats:\n\nhtml: Renders the document as an HTML webpage.\npdf: Renders the document as a PDF. You will need to have LaTeX (or tinytex) installed on your computer to use this format.\ndocx: Renders the document as a Microsoft Word document.\npptx: Renders the document as a PowerPoint presentation.\nrevealjs: Renders the document as an HTML slideshow.\ndashboard: Renders the document as an interactive dashboard.\n\nNote that there is a chance some of these may not work on your computer due to different operating systems or versions of software.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#markdown",
    "href": "p_tools_quarto.html#markdown",
    "title": "14  Using Quarto",
    "section": "14.9 Markdown",
    "text": "14.9 Markdown\nThe text inside Quarto documents is written in Markdown.\nMarkdown is a simple set of conventions for adding formatting to plain text. For example, to italicize text, you wrap it in asterisks *text here*, and to start a new header, you use the pound sign #. We will learn these in detail below.\nYou can define titles of different levels by starting a line with one or more #:\n# Level 1 Title\n\n## Level 2 Title\n\n### Level 3 Title\nThe body of the document consists of text that follows the Markdown syntax. A Markdown file is a text file that contains lightweight markup to help set heading levels or format text. For example, the following text:\nThis is text with *italics* and **bold**.\n\nYou can define bulleted lists:\n\n- First element\n- Second element\nWill generate the following formatted text:\n\nThis is text with italics and bold.\nYou can define bulleted lists:\n\nFirst element\nSecond element\n\n\nNote that you need spaces before and after lists, as well as keeping the listed items on separate lines. Otherwise, they will all crunch together rather than making a list.\nWe see that words placed between asterisks are italicized, and lines that begin with a dash are transformed into a bulleted list.\nThe Markdown syntax allows for other formatting, such as the ability to insert links or images. For example, the following code:\n[Example Link](https://example.com)\n… will give the following link:\n\nExample Link\n\nWe can also embed images. In your document, you can type:\n![Alt text](images/picture_name.jpg)\nReplace “Alt text” with a description of the image (it can also be blank), “images” with the name of the image folder in your project, and “picture_name.jpg” with the name of the image you want to use. Alternatively, in some editors, you can drag and drop the image into your document.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#code-chunk-options",
    "href": "p_tools_quarto.html#code-chunk-options",
    "title": "14  Using Quarto",
    "section": "14.10 Code Chunk Options",
    "text": "14.10 Code Chunk Options\nIt is possible to pass options to each code chunk to modify its behavior.\nFor example, a code chunk looks like this:\n\n# Your code here\nx = 2 + 2\nprint(x)\n\n4\n\n\nBut you can add options to control the code chunk’s execution and display:\n\n\n4\n\n\nIn this example, the echo: false option tells Quarto not to display the code in the rendered document, only the output.\n\n14.10.1 Global Options\nYou may want to apply options globally to all code chunks in your document. You can set default code execution options in the YAML header under the execute key.\nFor example:\n---\ntitle: \"Quarto Document\"\nformat: html\nexecute:\n  echo: false\n---\nThis will set echo: false for all code chunks.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#displaying-tables",
    "href": "p_tools_quarto.html#displaying-tables",
    "title": "14  Using Quarto",
    "section": "14.11 Displaying Tables",
    "text": "14.11 Displaying Tables\nBy default, pandas DataFrames display neatly in Quarto. However, for interactive tables, we can use the itables package.\nEnsure that you have the itables package installed. If not, you can install it using the following command:\n\n!pip install itables\n\nThen, you can run code similar to the following:\n\nimport plotly.express as px\nfrom itables import show\n\ntips = px.data.tips()\nshow(tips)\n\n\n\n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.2 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nNote that interactive tables will only work in HTML formats. We will look at tables for other formats later.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#displaying-plots",
    "href": "p_tools_quarto.html#displaying-plots",
    "title": "14  Using Quarto",
    "section": "14.12 Displaying Plots",
    "text": "14.12 Displaying Plots\nFor interactive plots, the plotly package is very useful.\n\ntips = px.data.tips()\ntips_sex = px.violin(tips, x=\"day\", y=\"total_bill\", color=\"sex\")\ntips_sex.show()\n\n                                                \n\n\nThis will display an interactive Plotly plot in the HTML output.\nFor static outputs like PDFs and Word documents, we need to save the plot as an image file and then include it in the document.\nFirst, save the plot as an image:\n\ntips_sex.write_image(\"tips_sex_plot.png\")\n\nThis command will create a static image file in the same folder as your document. We can then include it in the document as follows:\n![Violin plot of total bill by day and sex](tips_sex_plot.png)\nThis will display the image in the output.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#wrapping-up",
    "href": "p_tools_quarto.html#wrapping-up",
    "title": "14  Using Quarto",
    "section": "14.13 Wrapping Up",
    "text": "14.13 Wrapping Up\nIn this lesson, we covered how to create and render Quarto documents, add formatting, and include code chunks. We also learned how to use code chunk options to control the behavior of our documents. We experimented with different output formats and how to customize the display of our documents.\nWith these tools, you can create dynamic and interactive reports that are easily shareable in various formats. Quarto’s flexibility and integration with Python make it an excellent choice for data analysts and researchers alike.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html",
    "href": "p_untangled_subset_columns.html",
    "title": "15  Subsetting columns",
    "section": "",
    "text": "15.1 Introduction\nToday we will begin our exploration of pandas for data manipulation!\nOur first focus will be on selecting and renaming columns. Often your dataset comes with many columns that you do not need, and you would like to narrow it down to just a few. Pandas makes this easy. Let’s see how.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#learning-objectives",
    "href": "p_untangled_subset_columns.html#learning-objectives",
    "title": "15  Subsetting columns",
    "section": "15.2 Learning objectives",
    "text": "15.2 Learning objectives\n\nYou can keep or drop columns from a DataFrame using square brackets [], filter(), and drop().\nYou can select columns based on regex patterns with filter().\nYou can use rename() to change column names.\nYou can use regex to clean column names.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#about-pandas",
    "href": "p_untangled_subset_columns.html#about-pandas",
    "title": "15  Subsetting columns",
    "section": "15.3 About pandas",
    "text": "15.3 About pandas\nPandas is a popular library for data manipulation and analysis. It is designed to make it easy to work with tabular data in Python.\nInstall pandas with the following command in your terminal if it is not already installed:\n\npip install pandas \n\nThen import pandas with the following command in your script:\n\nimport pandas as pd",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_subset_columns.html#the-yaounde-covid-19-dataset",
    "title": "15  Subsetting columns",
    "section": "15.4 The Yaounde COVID-19 dataset",
    "text": "15.4 The Yaounde COVID-19 dataset\nIn this lesson, we analyse results from a COVID-19 survey conducted in Yaounde, Cameroon in late 2020. The survey estimated how many people had been infected with COVID-19 in the region, by testing for antibodies.\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0\nTo download the dataset, visit this link: https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/yaounde_data.zip\nThen unzip the file and place the yaounde_data.csv file in the data folder in the same directory as your notebook.\nLet’s load and examine the dataset:\n\nyao = pd.read_csv(\"data/yaounde_data.csv\")\nyao\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage\nage_category\nage_category_3\nsex\nhighest_education\noccupation\nweight_kg\nheight_cm\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n55\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n31\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n17\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 53 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#selecting-columns-with-square-brackets",
    "href": "p_untangled_subset_columns.html#selecting-columns-with-square-brackets",
    "title": "15  Subsetting columns",
    "section": "15.5 Selecting columns with square brackets []",
    "text": "15.5 Selecting columns with square brackets []\nIn pandas, the most common way to select a column is simply to use square brackets [] and the column name. For example, to select the age and sex columns, we type:\n\nyao[[\"age\", \"sex\"]]\n\n\n\n\n\n\n\n\nage\nsex\n\n\n\n\n0\n45\nFemale\n\n\n1\n55\nMale\n\n\n...\n...\n...\n\n\n969\n31\nFemale\n\n\n970\n17\nFemale\n\n\n\n\n971 rows × 2 columns\n\n\n\nNote the double square brackets [[]]. Without it, you will get an error:\n\nyao[\"age\", \"sex\"]\n\nKeyError: ('age', 'sex')\nIf you want to select a single column, you may omit the double square brackets, but your output will no longer be a DataFrame. Compare the following:\n\nyao[\"age\"] # does not return a DataFrame\n\n0      45\n1      55\n       ..\n969    31\n970    17\nName: age, Length: 971, dtype: int64\n\n\n\nyao[[\"age\"]]  # returns a DataFrame\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\n\n\n\n\n\n\nKey Point\n\n\n\n15.6 Storing data subsets\nNote that these selections are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset with only three columns:\n\nyao_subset = yao[[\"age\", \"sex\", \"igg_result\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\nsex\nigg_result\n\n\n\n\n0\n45\nFemale\nNegative\n\n\n1\n55\nMale\nPositive\n\n\n...\n...\n...\n...\n\n\n969\n31\nFemale\nNegative\n\n\n970\n17\nFemale\nNegative\n\n\n\n\n971 rows × 3 columns\n\n\n\nAnd if we want to overwrite a DataFrame, we can assign the subset back to the original DataFrame. Let’s overwrite the yao_subset DataFrame to have only the age column:\n\nyao_subset = yao_subset[[\"age\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\nThe yao_subset DataFrame has gone from having 3 columns to having 1 column.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.6.1 Practice Q: Select Columns with []\n\nUse the [] operator to select the “weight_kg” and “height_cm” variables in the yao DataFrame. Assign the result to a new DataFrame called yao_weight_height. Then print this new DataFrame.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPro tip\n\n\n\nThere are many ways to select columns in pandas. In your free time, you may choose to explore the .loc[] and .take() methods, which provide additional functionality.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#storing-data-subsets",
    "href": "p_untangled_subset_columns.html#storing-data-subsets",
    "title": "15  Subsetting columns",
    "section": "15.6 Storing data subsets",
    "text": "15.6 Storing data subsets\nNote that these selections are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset with only three columns:\n\nyao_subset = yao[[\"age\", \"sex\", \"igg_result\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\nsex\nigg_result\n\n\n\n\n0\n45\nFemale\nNegative\n\n\n1\n55\nMale\nPositive\n\n\n...\n...\n...\n...\n\n\n969\n31\nFemale\nNegative\n\n\n970\n17\nFemale\nNegative\n\n\n\n\n971 rows × 3 columns\n\n\n\nAnd if we want to overwrite a DataFrame, we can assign the subset back to the original DataFrame. Let’s overwrite the yao_subset DataFrame to have only the age column:\n\nyao_subset = yao_subset[[\"age\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\nThe yao_subset DataFrame has gone from having 3 columns to having 1 column.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#excluding-columns-with-drop",
    "href": "p_untangled_subset_columns.html#excluding-columns-with-drop",
    "title": "15  Subsetting columns",
    "section": "15.7 Excluding columns with drop()",
    "text": "15.7 Excluding columns with drop()\nSometimes it is more useful to drop columns you do not need than to explicitly select the ones that you do need.\nTo drop columns, we can use the drop() method with the columns argument. To drop the age column, we type:\n\nyao.drop(columns=[\"age\"])\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nsex\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\nNon-smoker\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\nEx-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\nNon-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\nNon-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 52 columns\n\n\n\nTo drop several columns:\n\nyao.drop(columns=[\"age\", \"sex\"])\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\nis_pregnant\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nSecondary\nInformal worker\n95\n169\nNon-smoker\nNo\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nUniversity\nSalaried worker\n96\n185\nEx-smoker\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nSecondary\nUnemployed\n66\n169\nNon-smoker\nNo\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nSecondary\nUnemployed\n67\n162\nNon-smoker\nNo response\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 51 columns\n\n\n\nAgain, note that this is not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset age and sex dropped:\n\nyao_subset = yao.drop(columns=[\"age\", \"sex\"])\nyao_subset\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\nis_pregnant\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nSecondary\nInformal worker\n95\n169\nNon-smoker\nNo\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nUniversity\nSalaried worker\n96\n185\nEx-smoker\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nSecondary\nUnemployed\n66\n169\nNon-smoker\nNo\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nSecondary\nUnemployed\n67\n162\nNon-smoker\nNo response\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 51 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.7.1 Practice Q: Drop Columns with drop()\n\nFrom the yao DataFrame, remove the columns highest_education and consultation. Assign the result to a new DataFrame called yao_no_education_consultation. Print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#using-filter-to-select-columns-by-regex",
    "href": "p_untangled_subset_columns.html#using-filter-to-select-columns-by-regex",
    "title": "15  Subsetting columns",
    "section": "15.8 Using filter() to select columns by regex",
    "text": "15.8 Using filter() to select columns by regex\nThe filter() method and its regex argument offer a powerful way to select columns based on patterns in their names. As an example, to select columns containing the string “ig”, we can write:\n\nyao.filter(regex=\"ig\")\n\n\n\n\n\n\n\n\nhighest_education\nweight_kg\nheight_cm\nneighborhood\nigg_result\nigm_result\nsymp_fatigue\n\n\n\n\n0\nSecondary\n95\n169\nBriqueterie\nNegative\nNegative\nNo\n\n\n1\nUniversity\n96\n185\nBriqueterie\nPositive\nNegative\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nSecondary\n66\n169\nTsinga Oliga\nNegative\nNegative\nNo\n\n\n970\nSecondary\n67\n162\nTsinga Oliga\nNegative\nNegative\nNo\n\n\n\n\n971 rows × 7 columns\n\n\n\nThe argument regex specifies the pattern to match. Regex stands for regular expression and refers to a sequence of characters that define a search pattern.\nTo select columns starting with the string “ig”, we write:\n\nyao.filter(regex=\"^ig\")\n\n\n\n\n\n\n\n\nigg_result\nigm_result\n\n\n\n\n0\nNegative\nNegative\n\n\n1\nPositive\nNegative\n\n\n...\n...\n...\n\n\n969\nNegative\nNegative\n\n\n970\nNegative\nNegative\n\n\n\n\n971 rows × 2 columns\n\n\n\nThe symbol ^ is a regex character that matches the beginning of the string.\nTo select columns ending with the string “result”, we can write:\n\nyao.filter(regex=\"result$\")\n\n\n\n\n\n\n\n\nigg_result\nigm_result\n\n\n\n\n0\nNegative\nNegative\n\n\n1\nPositive\nNegative\n\n\n...\n...\n...\n\n\n969\nNegative\nNegative\n\n\n970\nNegative\nNegative\n\n\n\n\n971 rows × 2 columns\n\n\n\nThe character $ is regex that matches the end of the string.\n\n\n\n\n\n\nPro Tip\n\n\n\nRegex is notoriously difficult to remember, but LLMs like ChatGPT are very good at generating the right patterns. Simply ask, for example, “What is the regex for strings starting with ‘ig’”\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.8.1 Practice Q: Select Columns with Regex\n\nSelect all columns in the yao DataFrame that start with “is_”. Assign the result to a new DataFrame called yao_is_columns. Then print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#change-column-names-with-rename",
    "href": "p_untangled_subset_columns.html#change-column-names-with-rename",
    "title": "15  Subsetting columns",
    "section": "15.9 Change column names with rename()",
    "text": "15.9 Change column names with rename()\nWe can use the rename() method to change column names:\n\nyao.rename(columns={\"age\": \"patient_age\", \"sex\": \"patient_sex\"})\n\n\n\n\n\n\n\n\nid\ndate_surveyed\npatient_age\nage_category\nage_category_3\npatient_sex\nhighest_education\noccupation\nweight_kg\nheight_cm\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n55\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n31\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n17\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 53 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.9.1 Practice Q: Rename Columns with rename()\n\nRename the age_category column in the yao DataFrame to age_cat. Assign the result to a new DataFrame called yao_age_cat. Then print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#cleaning-messy-column-names",
    "href": "p_untangled_subset_columns.html#cleaning-messy-column-names",
    "title": "15  Subsetting columns",
    "section": "15.10 Cleaning messy column names",
    "text": "15.10 Cleaning messy column names\nFor cleaning column names, you can use regular expressions with the str.replace() method in pandas.\nHere’s how you can do it on a test DataFrame with messy column names. Messy column names are names with spaces, special characters, or other non-alphanumeric characters.\n\ntest_df = pd.DataFrame(\n    {\"good_name\": range(3), \"bad name\": range(3), \"bad*@name*2\": range(3)}\n)\ntest_df\n\n\n\n\n\n\n\n\ngood_name\nbad name\nbad*@name*2\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2\n\n\n\n\n\n\n\nSuch column names are not ideal because, for example, we cannot select them with the dot operator the way we can for clean names:\n\ntest_df.good_name  # this works\n\n0    0\n1    1\n2    2\nName: good_name, dtype: int64\n\n\nBut this does not work:\n\ntest_df.bad name\n\n      test_df.bad name\n                 ^\nSyntaxError: invalid syntax\nWe can automatically clean such names using the str.replace() method along with regular expressions.\n\nclean_names = test_df.columns.str.replace(r'[^a-zA-Z0-9]', '_', regex=True)\n\nThe regular expression r'[^a-zA-Z0-9]' matches any character that is not a letter (either uppercase or lowercase) or a digit. The str.replace() method replaces these characters with an underscore (‘_’) to make the column names more legible and usable in dot notation.\nNow we can replace the column names in the DataFrame with the cleaned names:\n\ntest_df.columns = clean_names\ntest_df\n\n\n\n\n\n\n\n\ngood_name\nbad_name\nbad__name_2\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.10.1 Practice Q: Clean Column Names with Regex\n\nConsider the data frame defined below with messy column names. Use the str.replace() method to clean the column names.\n\n\ncleaning_practice = pd.DataFrame(\n    {\"Aloha\": range(3), \"Bell Chart\": range(3), \"Animals@the zoo\": range(3)}\n)\ncleaning_practice\n\n\n\n\n\n\n\n\nAloha\nBell Chart\nAnimals@the zoo\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#wrap-up",
    "href": "p_untangled_subset_columns.html#wrap-up",
    "title": "15  Subsetting columns",
    "section": "15.11 Wrap up",
    "text": "15.11 Wrap up\nHopefully this lesson has shown you how intuitive and useful pandas is for data manipulation!\nThis is the first of a series of basic data wrangling techniques: see you in the next lesson to learn more.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html",
    "href": "p_untangled_query_rows.html",
    "title": "16  Querying rows",
    "section": "",
    "text": "16.1 Intro\nQuerying rows is one of the most frequently used operations in data analysis. It allows you to filter your dataset to focus on specific subsets of interest, enabling more targeted and efficient analysis.\nIn this lesson, we’ll explore various techniques to subset rows in pandas.\nLet’s get started!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#learning-objectives",
    "href": "p_untangled_query_rows.html#learning-objectives",
    "title": "16  Querying rows",
    "section": "16.2 Learning objectives",
    "text": "16.2 Learning objectives\n\nYou can use the query() method to keep or drop rows from a DataFrame.\nYou can specify conditions using relational operators like greater than (&gt;), less than (&lt;), equal to (==), not equal to (!=), and is an element of (isin()).\nYou can combine conditions with & and |.\nYou can negate conditions with ~.\nYou can use the isna() and notna() methods.\nYou can query based on string patterns using str.contains().",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_query_rows.html#the-yaounde-covid-19-dataset",
    "title": "16  Querying rows",
    "section": "16.3 The Yaounde COVID-19 dataset",
    "text": "16.3 The Yaounde COVID-19 dataset\nIn this lesson, we will again use the data from the COVID-19 serological survey conducted in Yaounde, Cameroon.\nYou can download the dataset from this link: yaounde_data.csv\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0\nLet’s load the data into a pandas DataFrame.\n\nimport pandas as pd\n\nyaounde = pd.read_csv(\"data/yaounde_data.csv\")\n# a smaller subset of variables\nyao = yaounde[\n    [\n        \"age\",\n        \"sex\",\n        \"weight_kg\",\n        \"neighborhood\",\n        \"occupation\",\n        \"symptoms\",\n        \"is_smoker\",\n        \"is_pregnant\",\n        \"igg_result\",\n        \"igm_result\",\n    ]\n]\nyao.head()\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n4\n55\nFemale\n67\nBriqueterie\nTrader--Farmer\nNo symptoms\nNon-smoker\nNo\nPositive\nNegative",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#introducing-query",
    "href": "p_untangled_query_rows.html#introducing-query",
    "title": "16  Querying rows",
    "section": "16.4 Introducing query()",
    "text": "16.4 Introducing query()\nWe can use the query() method to keep rows that satisfy a set of conditions. Let’s take a look at a simple example. If we want to keep just the male records, we run:\n\nyao.query('sex == \"Male\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 10 columns\n\n\n\nAs you can see, the query() syntax is quite simple. (It may be a bit surprising to have to put code in quotes, but it is quite readable.)\nNote the use of double equals (==) instead of single equals (=) there. The == sign tests for equality, while the single equals sign assigns a value. This is a common source of errors when you are a beginner, so watch out for it.\nWe can chain query() with shape[0] to count the number of male respondents.\n\nyao.query('sex == \"Male\"').shape[0]\n\n422\n\n\n\n\n\n\n\n\nReminder\n\n\n\nThe shape property returns the number of rows and columns in a DataFrame. The first element, shape[0], is the number of rows, and the second element, shape[1], is the number of columns.\nFor example:\n\nyao.shape\n\n(971, 10)\n\n\n\nyao.shape[0] # rows\n\n971\n\n\n\nyao.shape[1]  # columns\n\n10\n\n\n\n\n\n\n\n\n\n\nKey Point\n\n\n\nNote that these subsets are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset of male respondents:\n\nyao_male = yao.query('sex == \"Male\"')\nyao_male\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 10 columns\n\n\n\nBut for ease of explanation, in the examples below, we are simply printing the result, without storing it in a variable.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.4.1 Practice Q: Subset for Pregnant Respondents\nSubset the yao data frame to respondents who were pregnant during the survey (The is_pregnant column contains “Yes”, “No” or NaN). Assign the result to a new DataFrame called yao_pregnant. Then print this new DataFrame. There should be 24 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#relational-operators",
    "href": "p_untangled_query_rows.html#relational-operators",
    "title": "16  Querying rows",
    "section": "16.5 Relational operators",
    "text": "16.5 Relational operators\nThe == operator introduced above is an example of a “relational” operator, as it tests the relation between two values. Here is a list of some more of these operators. You will use these often when you are querying rows in your data.\n\n\n\nOperator\nis True if\n\n\nA == B\nA is equal to B\n\n\nA != B\nA is not equal to B\n\n\nA &lt; B\nA is less than B\n\n\nA &lt;= B\nA is less than or equal to B\n\n\nA &gt; B\nA is greater than B\n\n\nA &gt;= B\nA is greater than or equal to B\n\n\nA.isin([B])\nA is an element of B\n\n\n\nLet’s see how to use these with query():\n\nyao.query('sex == \"Female\"')  # keep rows where `sex` is female\nyao.query('sex != \"Male\"')  # keep rows where `sex` is not \"Male\"\nyao.query(\"age &lt; 6\")  # keep respondents under 6\nyao.query(\"age &gt;= 70\")  # keep respondents aged at least 70\n\n# keep respondents whose neighbourhood is \"Tsinga\" or \"Messa\"\nyao.query('neighborhood.isin([\"Tsinga\", \"Messa\"])')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n605\n55\nMale\n70\nMessa\nInformal worker\nNo symptoms\nNon-smoker\nNaN\nNegative\nNegative\n\n\n606\n59\nFemale\n59\nMessa\nTrader\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n902\n25\nFemale\n41\nTsinga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n903\n28\nMale\n69\nTsinga\nTrader\nSneezing--Headache\nEx-smoker\nNaN\nNegative\nNegative\n\n\n\n\n129 rows × 10 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.5.1 Practice Q: Subset for Children\n\nFrom yao, keep only respondents who were children (under 18). Assign the result to a new DataFrame called yao_children. There should be 291 rows.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.5.2 Practice Q: Subset for Tsinga and Messa\n\nWith isin(), keep only respondents who live in the “Carriere” or “Ekoudou” neighborhoods. Assign the result to a new DataFrame called yao_carriere_ekoudou. There should be 426 rows.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#accessing-external-variables-in-query",
    "href": "p_untangled_query_rows.html#accessing-external-variables-in-query",
    "title": "16  Querying rows",
    "section": "16.6 Accessing external variables in query()",
    "text": "16.6 Accessing external variables in query()\nThe query() method allows you to access variables outside the DataFrame using the @ symbol. This is useful when you want to use dynamic values in your query conditions.\nFor example, say you have the variable min_age that you want to use in your query. You can do this as follows:\n\nmin_age = 25\n\n# Query using external variables\nyao.query('age &gt;= @min_age')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n524 rows × 10 columns\n\n\n\nThis feature is helpful when you need to filter data based on values that may change or are determined at runtime.\n\n\n\n\n\n\nPractice\n\n\n\n16.6.1 Practice Q: Subset for Young Respondents\n\nFrom yao, keep respondents who are less than or equal to the variable max_age, defined below. Assign the result to a new DataFrame called yao_young. There should be 590 rows.\n\n\nmax_age = 30\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#combining-conditions-with-and",
    "href": "p_untangled_query_rows.html#combining-conditions-with-and",
    "title": "16  Querying rows",
    "section": "16.7 Combining conditions with & and |",
    "text": "16.7 Combining conditions with & and |\nWe can pass multiple conditions to query() using & (the “ampersand” symbol) for AND and | (the “vertical bar” or “pipe” symbol) for OR.\nFor example, to keep respondents who are either younger than 18 OR older than 65, we can write:\n\nyao.query(\"age &lt; 18 | age &gt; 65\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n5\n17\nFemale\n65\nBriqueterie\nStudent\nFever--Cough--Rhinitis--Nausea or vomiting--Di...\nNon-smoker\nNo\nNegative\nNegative\n\n\n6\n13\nFemale\n65\nBriqueterie\nStudent\nSneezing\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n962\n15\nMale\n44\nTsinga Oliga\nStudent\nFever--Cough--Rhinitis\nNon-smoker\nNaN\nPositive\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n331 rows × 10 columns\n\n\n\nTo keep respondents who are pregnant and are ex-smokers, we write:\n\nyao.query('is_pregnant == \"Yes\" & is_smoker == \"Ex-smoker\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n273\n25\nFemale\n90\nCarriere\nHome-maker\nCough--Rhinitis--Sneezing\nEx-smoker\nYes\nPositive\nNegative\n\n\n\n\n\n\n\nTo keep all respondents who are pregnant or ex-smokers, we write:\n\nyao.query('is_pregnant == \"Yes\" | is_smoker == \"Ex-smoker\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n14\n42\nMale\n71\nBriqueterie\nTrader\nNo symptoms\nEx-smoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n953\n31\nFemale\n90\nTsinga Oliga\nSalaried worker\nFever--Cough--Sore throat--Headache\nEx-smoker\nNo\nPositive\nNegative\n\n\n967\n23\nFemale\n76\nTsinga Oliga\nInformal worker--Trader\nHeadache\nNon-smoker\nYes\nNegative\nNegative\n\n\n\n\n94 rows × 10 columns\n\n\n\n\n\n\n\n\n\nSide note\n\n\n\nTo get the unique values in a column, you can use the value_counts() method.\n\nyao.is_smoker.value_counts()\n\nis_smoker\nNon-smoker    859\nEx-smoker      71\nSmoker         39\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.7.1 Practice Q: Subset for IgG Positive Men\nSubset yao to only keep men who tested IgG positive. Assign the result to a new DataFrame called yao_igg_positive_men. There should be 148 rows after your query. Think carefully about whether to use & or |.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#negating-conditions-with-the-operator",
    "href": "p_untangled_query_rows.html#negating-conditions-with-the-operator",
    "title": "16  Querying rows",
    "section": "16.8 Negating conditions with the ~ operator",
    "text": "16.8 Negating conditions with the ~ operator\nTo negate conditions in query(), we use the ~ operator (pronounced “tilde”).\nLet’s use this to drop respondents who are students:\n\nyao.query('~ (occupation == \"Student\")')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n588 rows × 10 columns\n\n\n\nNotice that we have to enclose the condition in parentheses.\nWe can also enclose multiple conditions in parentheses.\nImagine we want to give out a drug, but since it is a strong drug, we don’t want children or lightweight (under 30kg) respondents to take it. First, we can write a query to select the children and these light respondents:\n\nyao.query(\"age &lt; 18 | weight_kg &lt; 30\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n5\n17\nFemale\n65\nBriqueterie\nStudent\nFever--Cough--Rhinitis--Nausea or vomiting--Di...\nNon-smoker\nNo\nNegative\nNegative\n\n\n6\n13\nFemale\n65\nBriqueterie\nStudent\nSneezing\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n962\n15\nMale\n44\nTsinga Oliga\nStudent\nFever--Cough--Rhinitis\nNon-smoker\nNaN\nPositive\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n291 rows × 10 columns\n\n\n\nNow to drop these individuals, we can negate the condition with ~:\n\nyao.query(\"~ (age &lt; 18 | weight_kg &lt; 30)\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n680 rows × 10 columns\n\n\n\nThis could also be written as:\n\nyao.query(\"age &gt;= 18 & weight_kg &gt;= 30\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n680 rows × 10 columns\n\n\n\nBut sometimes negated conditions are easier to read.\n\n\n\n\n\n\nPractice\n\n\n\n16.8.1 Practice Q: Drop Smokers and drop those over 50\nWe want to avoid giving a drug to older individuals and smokers. From yao, drop respondents that are either above 50 or who are smokers. Use ~ to negate the conditions. Assign the result to a new DataFrame called yao_dropped. Your output should have 810 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#nan-values",
    "href": "p_untangled_query_rows.html#nan-values",
    "title": "16  Querying rows",
    "section": "16.9 NaN values",
    "text": "16.9 NaN values\nThe relational operators introduced so far do not work with null values like NaN.\nFor example, the is_pregnant column contains (NA) values for men. To keep the rows with missing is_pregnant values, we could try writing:\n\nyao.query(\"is_pregnant == NaN\")  # does not work\n\nBut this will not work. This is because NaN is a non-existent value. So the system cannot evaluate whether it is “equal to” or “not equal to” anything.\nInstead, we can use the isna() method to select rows with missing values:\n\nyao.query(\"is_pregnant.isna()\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 10 columns\n\n\n\nOr we can select rows that are not missing with notna():\n\nyao.query(\"is_pregnant.notna()\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n549 rows × 10 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.9.1 Practice Q: Keep Missing Smoking Status\nFrom the yao dataset, keep all the respondents who had NA records for the report of their smoking status.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#querying-based-on-string-patterns",
    "href": "p_untangled_query_rows.html#querying-based-on-string-patterns",
    "title": "16  Querying rows",
    "section": "16.10 Querying Based on String Patterns",
    "text": "16.10 Querying Based on String Patterns\nSometimes, we need to filter our data based on whether a string column contains a certain substring. This is particularly useful when dealing with multi-answer type variables, where responses may contain multiple values separated by delimiters. Let’s explore this using the occupation column in our dataset.\nFirst, let’s take a look at the unique values in the occupation column:\n\nyao.occupation.value_counts().to_dict()\n\n{'Student': 383,\n 'Informal worker': 189,\n 'Trader': 111,\n 'Unemployed': 68,\n 'Home-maker': 65,\n 'Salaried worker': 54,\n 'Retired': 27,\n 'Student--Informal worker': 13,\n 'Other': 13,\n 'No response': 9,\n 'Farmer': 5,\n 'Informal worker--Trader': 4,\n 'Student--Trader': 4,\n 'Trader--Farmer': 4,\n 'Home-maker--Informal worker': 3,\n 'Home-maker--Trader': 3,\n 'Retired--Informal worker': 3,\n 'Informal worker--Other': 2,\n 'Home-maker--Farmer': 2,\n 'Student--Other': 1,\n 'Farmer--Other': 1,\n 'Trader--Unemployed': 1,\n 'Retired--Other': 1,\n 'Informal worker--Unemployed': 1,\n 'Retired--Trader': 1,\n 'Home-maker--Informal worker--Farmer': 1,\n 'Student--Informal worker--Other': 1,\n 'Informal worker--Trader--Farmer--Other': 1}\n\n\nAs we can see, some respondents have multiple occupations, separated by “–”. To query based on string containment, we can use the str.contains() method within our query().\n\n16.10.1 Basic String Containment\nTo find all respondents who are students (either solely or in combination with other occupations), we can use:\n\nyao.query(\"occupation.str.contains('Student')\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n963\n26\nFemale\n63\nTsinga Oliga\nStudent\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n964\n28\nMale\n76\nTsinga Oliga\nStudent--Informal worker\nNo symptoms\nNon-smoker\nNaN\nNegative\nNegative\n\n\n\n\n402 rows × 10 columns\n\n\n\nThis query will return all rows where the occupation column contains the word “Student”, regardless of whether it’s the only occupation or part of a multiple-occupation entry.\n\n\n16.10.2 Negating String Containment\nTo find respondents who are not students (i.e., their occupation does not contain “Student”), you can use the ~ operator:\n\nyao.query(\"~occupation.str.contains('Student')\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n569 rows × 10 columns\n\n\n\n\n\n16.10.3 Using | with string containment\nTo find respondents who are students or farmers, we can use:\n\nyao.query(\"occupation.str.contains('Student|Farmer')\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n963\n26\nFemale\n63\nTsinga Oliga\nStudent\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n964\n28\nMale\n76\nTsinga Oliga\nStudent--Informal worker\nNo symptoms\nNon-smoker\nNaN\nNegative\nNegative\n\n\n\n\n416 rows × 10 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.10.4 Practice Q: Symptoms\nThe symptoms column contains a list of symptoms that respondents reported.\nQuery yao to find respondents who reported “Cough” or “Fever” as symptoms. Your answer should have 219 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#wrap-up",
    "href": "p_untangled_query_rows.html#wrap-up",
    "title": "16  Querying rows",
    "section": "16.11 Wrap up",
    "text": "16.11 Wrap up\nGreat job! You’ve learned how to select specific columns and filter rows based on various conditions.\nThese skills allow you to focus on relevant data and create targeted subsets for analysis.\nNext, we’ll explore how to modify and transform your data, further expanding your data wrangling toolkit. See you in the next lesson!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html",
    "href": "p_untangled_transform_variables.html",
    "title": "17  Transforming Variables in pandas",
    "section": "",
    "text": "17.1 Introduction\nIn data analysis, one of the most common tasks is transforming variables within your dataset. The pandas library provides straightforward and efficient ways to accomplish this.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#learning-objectives",
    "href": "p_untangled_transform_variables.html#learning-objectives",
    "title": "17  Transforming Variables in pandas",
    "section": "17.2 Learning Objectives",
    "text": "17.2 Learning Objectives\n\nUnderstand how to create new variables in a DataFrame.\nLearn how to modify existing variables.\nHandle potential issues with modifying variables on views.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#imports",
    "href": "p_untangled_transform_variables.html#imports",
    "title": "17  Transforming Variables in pandas",
    "section": "17.3 Imports",
    "text": "17.3 Imports\nFirst, let’s import the pandas package:\n\nimport pandas as pd\n\nNow we’ll set an important option that will help us avoid some warnings down the line. Later in the lesson, we’ll discuss this in more detail.\n\npd.options.mode.copy_on_write = True",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#dataset",
    "href": "p_untangled_transform_variables.html#dataset",
    "title": "17  Transforming Variables in pandas",
    "section": "17.4 Dataset",
    "text": "17.4 Dataset\nIn this lesson, we’ll use a dataset of United States counties with demographic and economic data. You can download the dataset from this link:https://github.com/the-graph-courses/idap_book/raw/refs/heads/main/data/us_counties_data.zip.\nOnce you’ve downloaded the file, unzip it and place the us_counties_data.csv file in the data folder for your project.\n\ncounties = pd.read_csv(\"data/us_counties_data.csv\")\ncounties\n\n\n\n\n\n\n\n\nstate\ncounty\npop_20\narea_sq_miles\nhh_inc_21\necon_type\nunemp_20\nforeign_born_num\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAL\nAutauga, AL\n58877.0\n594.456107\n66444.0\nNonspecialized\n5.4\n1241.0\n7.758700\n9.0\n\n\n1\nAL\nBaldwin, AL\n233140.0\n1589.836014\n65658.0\nRecreation\n6.2\n7938.0\n27.159356\n28.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nPR\nYabucoa, PR\n30364.0\n55.214614\nNaN\nNaN\nNaN\nNaN\n-19.807069\n0.1\n\n\n3225\nPR\nYauco, PR\n34062.0\n67.711484\nNaN\nNaN\nNaN\nNaN\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 10 columns\n\n\n\nThe variables in the dataset are:\n\nstate: US state\ncounty: US county\npop_20: Population estimate for 2020\narea_sq_miles: Area in square miles\nhh_inc_21: Median household income for 2021\necon_type: Economic type of the county\npop_change_2010_2020: Population change between 2010 and 2020 (%)\nunemp_20: Unemployment rate for 2020 (%)\npct_emp_change_2010_2021: Percentage change in employment between 2010 and 2021 (%)\nforeign_born_num: Number of foreign-born residents\n\nLet’s create a small subset of the dataset with just the area and population columns for illustration.\n\n# Small subset for illustration\narea_df = counties[[\"county\", \"area_sq_miles\", \"pop_20\"]]\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n\n\n\n\n3226 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#creating-new-variables",
    "href": "p_untangled_transform_variables.html#creating-new-variables",
    "title": "17  Transforming Variables in pandas",
    "section": "17.5 Creating New Variables",
    "text": "17.5 Creating New Variables\nSuppose we want to convert the area from square miles to square kilometers. Since 1 square mile is approximately 2.59 square kilometers, we can create a new variable area_sq_km by multiplying the area_sq_miles column by 2.59.\n\narea_df[\"area_sq_km\"] = area_df[\"area_sq_miles\"] * 2.59\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n\n\n\n\n3226 rows × 4 columns\n\n\n\nThe syntax is very easy to understand, although a bit hard to type.\nWith area_df[\"area_sq_km\"], we’re indicating that we want to create a new column called area_sq_km, then area_df[\"area_sq_miles\"] * 2.59 is the expression that computes the values for this new column.\nLet’s add another variable, this time in hectares. The conversion factor is 1 square mile = 259 hectares.\n\n# Convert area to hectares as well\narea_df[\"area_hectares\"] = area_df[\"area_sq_miles\"] * 259\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.5.1 Practice Q: Area in Acres\nUsing the area_df dataset, create a new column called area_acres by multiplying the area_sq_miles variable by 640. Store the result back into area_df and display the DataFrame.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#modifying-existing-variables",
    "href": "p_untangled_transform_variables.html#modifying-existing-variables",
    "title": "17  Transforming Variables in pandas",
    "section": "17.6 Modifying Existing Variables",
    "text": "17.6 Modifying Existing Variables\nSuppose we want to round the area_sq_km variable to one decimal place. We can call the round method on the area_sq_km column.\n\narea_df[\"area_sq_km\"] = area_df[\"area_sq_km\"].round(1)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.6.1 Practice Q: Rounding area_acres\nUsing the area_df dataset, round the area_acres variable to one decimal place. Update the DataFrame area_df and display it.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#calculations-with-multiple-variables",
    "href": "p_untangled_transform_variables.html#calculations-with-multiple-variables",
    "title": "17  Transforming Variables in pandas",
    "section": "17.7 Calculations with Multiple Variables",
    "text": "17.7 Calculations with Multiple Variables\nWe can create new variables based on multiple existing variables.\nFor example, let’s calculate the population density per square kilometer.\n\narea_df[\"pop_per_sq_km\"] = area_df[\"pop_20\"] / area_df[\"area_sq_km\"]\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n38.241751\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n56.618986\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n212.335664\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n194.196123\n\n\n\n\n3226 rows × 6 columns\n\n\n\nWe could tag on the round method to this output to round the result to one decimal place.\n\narea_df[\"pop_per_sq_km\"] = (area_df[\"pop_20\"] / area_df[\"area_sq_km\"]).round(1)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n38.2\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n212.3\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n194.2\n\n\n\n\n3226 rows × 6 columns\n\n\n\nOr, if you prefer, you can do this in two steps:\n\narea_df[\"pop_per_sq_km\"] = area_df[\"pop_20\"] / area_df[\"area_sq_km\"]\narea_df[\"pop_per_sq_km\"] = area_df[\"pop_per_sq_km\"].round(1)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n38.2\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n212.3\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n194.2\n\n\n\n\n3226 rows × 6 columns\n\n\n\nAfter calculating the population density, we might want to sort the DataFrame based on this new variable. Let’s sort in descending order.\n\n# Sort by population density in descending order\narea_df = area_df.sort_values(\"pop_per_sq_km\", ascending=False)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n1863\nNew York, NY\n22.656266\n1687834.0\n58.7\n5867.972888\n28753.6\n\n\n1856\nKings, NY\n69.376570\n2727393.0\n179.7\n17968.531752\n15177.5\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n98\nWrangell-Petersburg, AK\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2921\nBedford, VA\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3226 rows × 6 columns\n\n\n\nWe see that New York County has the highest population density in the dataset.\n\n\n\n\n\n\nPractice\n\n\n\n17.7.1 Practice Q: Calculate Foreign-Born Percentage\nUse the counties dataset to calculate the percentage of foreign-born residents in each county. The variable foreign_born_num shows the number of foreign-born residents and pop_20 shows the total population. Sort the DataFrame in descending order of the percentage of foreign-born residents. Which two counties have the highest percentage of foreign-born residents?\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#creating-boolean-variables",
    "href": "p_untangled_transform_variables.html#creating-boolean-variables",
    "title": "17  Transforming Variables in pandas",
    "section": "17.8 Creating Boolean Variables",
    "text": "17.8 Creating Boolean Variables\nIt is sometimes useful to create Boolean variables to categorize or flag data based on conditions. Boolean variables are variables that take on only two values: True or False.\nConsider the pop_change_2010_2020 variable in the counties dataset, which shows the percentage change in population between 2010 and 2020.\n\nchanges_df = counties[[\"county\", \"pop_change_2010_2020\", \"pct_emp_change_2010_2021\"]]\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\n\n\n1\nBaldwin, AL\n27.159356\n28.2\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 3 columns\n\n\n\nWe might want to create a Boolean variable to flag whether the population increased. For this, let’s set the pop_increase variable to True if the population increased and False otherwise.\nRunning the expression changes_df[\"pop_change_2010_2020\"] &gt; 0 returns a Series of Boolean values:\n\nchanges_df[\"pop_change_2010_2020\"] &gt; 0\n\n0        True\n1        True\n        ...  \n3224    False\n3225    False\nName: pop_change_2010_2020, Length: 3226, dtype: bool\n\n\nWe can assign this Series of Boolean values to the pop_increase variable.\n\nchanges_df[\"pop_increase\"] = changes_df[\"pop_change_2010_2020\"] &gt; 0\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\nTrue\n\n\n1\nBaldwin, AL\n27.159356\n28.2\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\nFalse\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\nFalse\n\n\n\n\n3226 rows × 4 columns\n\n\n\nSimilarly, we can create a Boolean variable emp_increase for employment change.\n\nchanges_df[\"emp_increase\"] = changes_df[\"pct_emp_change_2010_2021\"] &gt; 0\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\nemp_increase\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\nTrue\nTrue\n\n\n1\nBaldwin, AL\n27.159356\n28.2\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\nFalse\nTrue\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\nFalse\nFalse\n\n\n\n\n3226 rows × 5 columns\n\n\n\nWe can now filter the DataFrame to find counties where the population increased but employment decreased.\n\n# Counties where population increased but employment decreased\npop_up_emp_down = changes_df.query(\"pop_increase == True & emp_increase == False\")\npop_up_emp_down\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\nemp_increase\n\n\n\n\n71\nBethel, AK\n9.716099\n-0.7\nTrue\nFalse\n\n\n75\nDillingham, AK\n0.206313\n-16.1\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n3127\nCampbell, WY\n1.935708\n-14.8\nTrue\nFalse\n\n\n3137\nNatrona, WY\n5.970842\n-0.2\nTrue\nFalse\n\n\n\n\n242 rows × 5 columns\n\n\n\nYou could also write this in shorthand like so:\n\n# Counties where population increased but employment decreased\npop_up_emp_down = changes_df.query(\"pop_increase & ~(emp_increase)\")\npop_up_emp_down\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\nemp_increase\n\n\n\n\n71\nBethel, AK\n9.716099\n-0.7\nTrue\nFalse\n\n\n75\nDillingham, AK\n0.206313\n-16.1\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n3127\nCampbell, WY\n1.935708\n-14.8\nTrue\nFalse\n\n\n3137\nNatrona, WY\n5.970842\n-0.2\nTrue\nFalse\n\n\n\n\n242 rows × 5 columns\n\n\n\nThere are several such counties, which might be of interest for further analysis.\n\n\n\n\n\n\nPractice\n\n\n\n17.8.1 Practice Q: Categorize Counties by Foreign-Born Population\nIn a previous practice question, we calculated the percentage of foreign-born residents in each county. Now, create a Boolean variable foreign_born_pct_gt_30 that is True if the percentage is greater than 30%.\nWhen you’re done, query the DataFrame to show only counties where foreign_born_pct_gt_30 is True. You should get 24 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#the-copy-on-write-warning",
    "href": "p_untangled_transform_variables.html#the-copy-on-write-warning",
    "title": "17  Transforming Variables in pandas",
    "section": "17.9 The Copy-on-Write Warning",
    "text": "17.9 The Copy-on-Write Warning\nEarlier in this lesson, we enabled “copy-on-write” mode. Let’s see what happens when this feature is disabled.\n\npd.set_option(\"mode.copy_on_write\", False)\n\n# Create a small subset of our data\nsubset = counties.query(\"state == 'AL'\")\n\nsubset\n\n\n\n\n\n\n\n\nstate\ncounty\npop_20\narea_sq_miles\nhh_inc_21\necon_type\nunemp_20\nforeign_born_num\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAL\nAutauga, AL\n58877.0\n594.456107\n66444.0\nNonspecialized\n5.4\n1241.0\n7.758700\n9.0\n\n\n1\nAL\nBaldwin, AL\n233140.0\n1589.836014\n65658.0\nRecreation\n6.2\n7938.0\n27.159356\n28.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n65\nAL\nWilcox, AL\n10518.0\n887.857611\n30071.0\nManufacturing\n16.2\n36.0\n-9.168809\n3.8\n\n\n66\nAL\nWinston, AL\n23491.0\n612.998002\n47176.0\nManufacturing\n5.2\n408.0\n-3.855579\n18.1\n\n\n\n\n67 rows × 10 columns\n\n\n\nWhen we attempt to modify the subset, we receive a warning:\n\n# Modify the subset\nsubset['unemp_20'] = subset['unemp_20'].round(0)\n\n/var/folders/vr/shb6ffvj2rl61kh7qqczhrgh0000gp/T/ipykernel_51189/317403666.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nWhile we won’t delve deeply into the technical details of this warning (as it involves complex pandas internals), it’s worth noting that the warning includes a link to the pandas documentation. This documentation contains the setting we used at the beginning of our lesson.\nIf you ever need to reference this setting again, you can simply click the link in the warning message to access the documentation. The documentation page also provides more detailed information about this particular issue.\nAlso note that from Pandas 3.0 (probably to be released in 2025) this warning will be removed, as the default behavior will be to copy on write.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#wrap-up",
    "href": "p_untangled_transform_variables.html#wrap-up",
    "title": "17  Transforming Variables in pandas",
    "section": "17.10 Wrap-Up",
    "text": "17.10 Wrap-Up\nTransforming data is a fundamental step in any data analysis workflow. pandas makes it straightforward to create and modify variables within your DataFrames using simple and intuitive syntax.\nIn this lesson, you’ve learned how to:\n\nCreate new variables by assigning to new columns.\nModify existing variables.\nPerform calculations involving multiple variables.\nCreate Boolean variables based on conditions.\n\nCongratulations on completing this lesson!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html",
    "href": "p_untangled_conditional_transforms.html",
    "title": "18  Conditional Transformations of Variables",
    "section": "",
    "text": "18.1 Introduction\nIn the previous lesson, you learned the basics of data transformation in pandas.\nIn this lesson, we will explore how to conditionally transform variables in pandas using methods like replace() and custom functions.\nConditional transformations are essential when you need to recode variables or create new ones based on specific criteria.\nLet’s dive in!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#learning-objectives",
    "href": "p_untangled_conditional_transforms.html#learning-objectives",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.2 Learning Objectives",
    "text": "18.2 Learning Objectives\nBy the end of this lesson, you will:\n\nBe able to transform or create new variables based on conditions using replace() and dictionaries.\nKnow how to handle NaN values in replace() transformations.\nBe able to define and apply custom functions to recode variables.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#packages",
    "href": "p_untangled_conditional_transforms.html#packages",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.3 Packages",
    "text": "18.3 Packages\nThis lesson will require pandas, numpy, plotly.express, and vega_datasets:\n\nimport pandas as pd\nimport numpy as np\nimport vega_datasets as vd\nimport plotly.express as px",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#introduction-to-replace",
    "href": "p_untangled_conditional_transforms.html#introduction-to-replace",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.4 Introduction to replace()",
    "text": "18.4 Introduction to replace()\nOne common task in data wrangling is to replace values in a column based on certain conditions. The replace() method in pandas is a versatile tool for this purpose.\nIn the tips dataset, the day column contains abbreviated day names:\n\ntips = px.data.tips()\ntips['day'].unique()\n\narray(['Sun', 'Sat', 'Thur', 'Fri'], dtype=object)\n\n\nOur goal is to replace these abbreviations with the full day names.\nWe can create a dictionary that maps the abbreviated names to the full names:\n\nday_mapping = {\n    \"Sun\": \"Sunday\",\n    \"Sat\": \"Saturday\",\n    \"Fri\": \"Friday\",\n    \"Thur\": \"Thursday\"\n}\n\nNow, we use the replace() method with the dictionary:\n\ntips['day_full'] = tips['day'].replace(day_mapping)\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\nday_full\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\nSunday\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\nSunday\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\nSunday\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\nSaturday\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\nSaturday\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\nThursday\n\n\n\n\n244 rows × 8 columns\n\n\n\nAlternatively, we can perform the replacement directly within the replace() method without explicitly defining a dictionary:\n\ntips['day_full'] = tips['day'].replace({\n    \"Sun\": \"Sunday\",\n    \"Sat\": \"Saturday\",\n    \"Fri\": \"Friday\",\n    \"Thur\": \"Thursday\"\n})\ntips[['day', 'day_full']].head()\n\n\n\n\n\n\n\n\nday\nday_full\n\n\n\n\n0\nSun\nSunday\n\n\n1\nSun\nSunday\n\n\n2\nSun\nSunday\n\n\n3\nSun\nSunday\n\n\n4\nSun\nSunday\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.5 Practice Q: Abbreviate Sex\nUsing the tips dataset, replace the values in the sex column to abbreviate gender:\n\nReplace \"Female\" with \"F\".\nReplace \"Male\" with \"M\".\n\nAssign the result to a new column called sex_abbr and display the first few rows.\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#practice-q-abbreviate-sex",
    "href": "p_untangled_conditional_transforms.html#practice-q-abbreviate-sex",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.5 Practice Q: Abbreviate Sex",
    "text": "18.5 Practice Q: Abbreviate Sex\nUsing the tips dataset, replace the values in the sex column to abbreviate gender:\n\nReplace \"Female\" with \"F\".\nReplace \"Male\" with \"M\".\n\nAssign the result to a new column called sex_abbr and display the first few rows.\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#handling-missing-values-with-replace",
    "href": "p_untangled_conditional_transforms.html#handling-missing-values-with-replace",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.6 Handling Missing Values with replace()",
    "text": "18.6 Handling Missing Values with replace()\nSometimes, your dataset may contain missing values (NaN or None) that you want to replace with a placeholder or specific value. The replace() method can handle this.\nLet’s examine the Creative_Type column in the movies dataset from vega_datasets:\n\nmovies = vd.data.movies()\nmovies['Creative_Type'].value_counts(dropna=False)\n\nCreative_Type\nContemporary Fiction       1453\nNone                        446\nHistorical Fiction          350\n                           ... \nFactual                      49\nSuper Hero                   49\nMultiple Creative Types       1\nName: count, Length: 10, dtype: int64\n\n\nNotice that there are some None values in the Creative_Type column.\nLet’s replace None with \"Unknown/Unclear\":\n\nmovies['Creative_Type'] = movies['Creative_Type'].replace({\n    None: \"Unknown/Unclear\", # 👈 On this line, None is the key\n})\n\nNow, let’s verify the replacement:\n\nmovies['Creative_Type'].value_counts(dropna=False)\n\nCreative_Type\nContemporary Fiction       1453\nUnknown/Unclear             446\nHistorical Fiction          350\n                           ... \nFactual                      49\nSuper Hero                   49\nMultiple Creative Types       1\nName: count, Length: 10, dtype: int64\n\n\nWhile None is typically used to represent missing strings, NaN is used for missing numbers. Consider the US_DVD_Sales column:\n\nmovies.query(\"US_DVD_Sales.isna()\").shape # Check the number of missing values\n\n(2637, 16)\n\n\n\nmovies['US_DVD_Sales'].tail(10) # View the last 10 values. Some are missing.\n\n3191     3273039.0\n3192    22025352.0\n3193           NaN\n           ...    \n3198     6679409.0\n3199           NaN\n3200           NaN\nName: US_DVD_Sales, Length: 10, dtype: float64\n\n\nWe can use replace() to replace NaN with 0:\n\nmovies['US_DVD_Sales'] = movies['US_DVD_Sales'].replace({\n    np.nan: 0 # 👈 `NaN` is represented by `np.nan` in pandas\n})\n\nLet’s verify the replacement:\n\nmovies['US_DVD_Sales'].tail(10)\n\n3191     3273039.0\n3192    22025352.0\n3193           0.0\n           ...    \n3198     6679409.0\n3199           0.0\n3200           0.0\nName: US_DVD_Sales, Length: 10, dtype: float64\n\n\n\nmovies.query(\"US_DVD_Sales.isna()\").shape\n\n(0, 16)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#practice-q-standardize-mpaa-ratings",
    "href": "p_untangled_conditional_transforms.html#practice-q-standardize-mpaa-ratings",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.7 Practice Q: Standardize MPAA Ratings",
    "text": "18.7 Practice Q: Standardize MPAA Ratings\nIn the movies dataset, the MPAA_Rating column contains movie ratings. Some entries are None or \"Not Rated\". Replace both None and \"Not Rated\" with \"Unrated\".\nThen, use value_counts() to see how many movies are unrated. There should be 699 movies in this category.\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#categorizing-numeric-data-with-custom-functions",
    "href": "p_untangled_conditional_transforms.html#categorizing-numeric-data-with-custom-functions",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.8 Categorizing Numeric Data with Custom Functions",
    "text": "18.8 Categorizing Numeric Data with Custom Functions\nRecall from our previous lesson that we can use custom functions with conditional logic to transform variables. For example, we can categorize the US_Gross column into three categories based on the following criteria:\n\nIf the value is less than 10 million, the category is \"Low\".\nIf the value is between 10 million and 50 million, the category is \"Medium\".\nIf the value is greater than 50 million, the category is \"High\".\n\n\ndef categ_gross(gross):\n    if gross &lt; 10000000:\n        return \"Low\"\n    elif gross &gt;= 10000000 and gross &lt;= 50000000:\n        return \"Medium\"\n    elif gross &gt; 50000000:\n        return \"High\"\n    else:\n        return None \n\n\ncateg_gross_vec = np.vectorize(categ_gross)\n\n\n\n\n\n\n\nSide Note\n\n\n\nThe np.vectorize function in the above case will return None as a string. To enforce the None type, you can use the otypes parameter:\n\ncateg_gross_vec = np.vectorize(categ_gross, otypes=[object])\n\n\n\nNow we can apply it to the entire column:\n\nmovies['Gross_Category'] = categ_gross_vec(movies['US_Gross'])\nmovies['Gross_Category'].value_counts(dropna=False)\n\nGross_Category\nMedium    1241\nLow       1046\nHigh       907\nNone         7\nName: count, dtype: int64\n\n\nThis can also be achieved with pd.cut(), np.where() and np.select(). But the custom function approach is the most flexible. Below we’ll see how to extend this to more complex conditions.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#complex-transformations-with-custom-functions",
    "href": "p_untangled_conditional_transforms.html#complex-transformations-with-custom-functions",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.9 Complex Transformations with Custom Functions",
    "text": "18.9 Complex Transformations with Custom Functions\nThe flexibility of custom functions can be extended easily to more complex conditional transformations.\nFor example, suppose we want to flag superhero movies as “US action movie” or “Global action movie” based on their US and worldwide gross earnings.\n\nFor Super Hero movies, if the US gross and worldwide gross are the same (indicating sales were only in the US), the movie is flagged as a US action movie.\nFor Super Hero movies, if the worldwide gross is greater than the US gross, the movie is flagged as a global action movie.\nFor all other movies, we leave the flag blank\n\nWe can define a funcion that takes in three arguments and returns the appropriate flag:\n\n# Define the function to flag movies based on the conditions\ndef flag_movie(movie_type, us, worldwide):\n    if movie_type == 'Super Hero' and us == worldwide:\n        return 'US action movie'\n    elif movie_type == 'Super Hero' and worldwide &gt; us:\n        return 'Global action movie'\n    else:\n        return None\n\nLet’s test it out with a few sets of values:\n\nprint(flag_movie(movie_type='Super Hero', us=100, worldwide=100))\nprint(flag_movie(movie_type='Super Hero', us=100, worldwide=200))\nprint(flag_movie(movie_type='Comedy', us=100, worldwide=100))\n\nUS action movie\nGlobal action movie\nNone\n\n\nNow, let’s vectorize it:\n\nflag_movie_vec = np.vectorize(flag_movie)\n\nWe can now apply it to the columns:\n\nmovies['Action_Flag'] = flag_movie_vec(movies['Creative_Type'], movies['US_Gross'], movies['Worldwide_Gross'])\nmovies\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\nGross_Category\nAction_Flag\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n0.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nUnknown/Unclear\nNone\nNaN\n6.1\n1071.0\nLow\nNone\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n0.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nUnknown/Unclear\nNone\nNaN\n6.9\n207.0\nLow\nNone\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n0.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nUnknown/Unclear\nNone\nNaN\n6.8\n865.0\nLow\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3198\nZoom\n11989328.0\n12506188.0\n6679409.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\nMedium\nGlobal action movie\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n0.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\nMedium\nNone\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n0.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\nHigh\nNone\n\n\n\n\n3201 rows × 18 columns\n\n\n\nTo see the distribution of movie categories based on our flag, we can use value_counts():\n\nmovies['Action_Flag'].value_counts(dropna=False)\n\nAction_Flag\nNone                   3152\nGlobal action movie      42\nUS action movie           7\nName: count, dtype: int64\n\n\n\n18.9.1 Practice: Flag Movies Based on Ratings\nIn the movies dataset, flag movies as Critic-friendly or Commercial based on their Rotten Tomatoes and IMDB ratings.\n\nIf the Rotten Tomatoes rating is above 70% and the IMDB rating is below 5, the movie is flagged as Critic-friendly.\nIf the Rotten Tomatoes rating is below 50% and the IMDB rating is above 7, the movie is flagged as Commercial.\nOtherwise, the movie is categorized as Other.\nCount how many movies are Critic-friendly and Commercial. There should be 13 Critic-friendly movies and 33 Commercial movies. Do you recognize any of them?\n\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_conditional_transforms.html#wrap-up",
    "href": "p_untangled_conditional_transforms.html#wrap-up",
    "title": "18  Conditional Transformations of Variables",
    "section": "18.10 Wrap-Up",
    "text": "18.10 Wrap-Up\nIn this lesson, you learned how to conditionally transform variables in pandas using:\n\nThe replace() method with dictionaries to map and replace specific values.\nHandling missing values (NaN or None) during replacements.\nDefining custom functions and applying them to handle complex conditions.\n\nThese techniques are powerful tools for data cleaning and preprocessing, allowing you to reshape your data to meet your analysis needs.\nSee you next time!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html",
    "href": "p_untangled_groupby_agg.html",
    "title": "19  Grouping and summarizing data",
    "section": "",
    "text": "19.1 Introduction\nIn this lesson, we’ll explore two powerful pandas methods: agg() and groupby(). These tools will enable you to extract summary statistics and perform operations on grouped data effortlessly.\nA summary statistic is a single value (such as a mean or median) that describes a sequence of values (typically a column in your dataset).\nLet’s see how to use these!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#learning-objectives",
    "href": "p_untangled_groupby_agg.html#learning-objectives",
    "title": "19  Grouping and summarizing data",
    "section": "19.2 Learning objectives",
    "text": "19.2 Learning objectives\n\nYou can use pandas.DataFrame.agg() to extract summary statistics from datasets.\nYou can use pandas.DataFrame.groupby() to group data by one or more variables before performing operations on them.\nYou can pass custom functions to agg() to compute summary statistics.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#libraries",
    "href": "p_untangled_groupby_agg.html#libraries",
    "title": "19  Grouping and summarizing data",
    "section": "19.3 Libraries",
    "text": "19.3 Libraries\nRun the following lines to import the necessary libraries:\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_groupby_agg.html#the-yaounde-covid-19-dataset",
    "title": "19  Grouping and summarizing data",
    "section": "19.4 The Yaounde COVID-19 dataset",
    "text": "19.4 The Yaounde COVID-19 dataset\nIn this lesson, we will again use a subset of data from the COVID-19 serological survey conducted in Yaounde, Cameroon.\nYou can download the dataset from this link: yaounde_mini.csv\n\nyao = pd.read_csv(\"data/yaounde_mini.csv\")\nyao\n\n\n\n\n\n\n\n\nage\nage_category_3\nsex\nweight_kg\nheight_cm\nneighborhood\nis_smoker\nis_pregnant\noccupation\ntreatment_combinations\nsymptoms\nn_days_miss_work\nn_bedridden_days\nhighest_education\nigg_result\n\n\n\n\n0\n45\nAdult\nFemale\n95\n169\nBriqueterie\nNon-smoker\nNo\nInformal worker\nParacetamol\nMuscle pain\n0.0\n0.0\nSecondary\nNegative\n\n\n1\n55\nAdult\nMale\n96\n185\nBriqueterie\nEx-smoker\nNaN\nSalaried worker\nNaN\nNo symptoms\nNaN\nNaN\nUniversity\nPositive\n\n\n2\n23\nAdult\nMale\n74\n180\nBriqueterie\nSmoker\nNaN\nStudent\nNaN\nNo symptoms\nNaN\nNaN\nUniversity\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nAdult\nMale\n77\n168\nTsinga Oliga\nSmoker\nNaN\nInformal worker\nParacetamol\nHeadache\n0.0\n0.0\nUniversity\nPositive\n\n\n969\n31\nAdult\nFemale\n66\n169\nTsinga Oliga\nNon-smoker\nNo\nUnemployed\nNaN\nNo symptoms\nNaN\nNaN\nSecondary\nNegative\n\n\n970\n17\nChild\nFemale\n67\n162\nTsinga Oliga\nNon-smoker\nNo response\nUnemployed\nNaN\nNo symptoms\nNaN\nNaN\nSecondary\nNegative\n\n\n\n\n971 rows × 15 columns\n\n\n\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#introducing-pandas.dataframe.agg",
    "href": "p_untangled_groupby_agg.html#introducing-pandas.dataframe.agg",
    "title": "19  Grouping and summarizing data",
    "section": "19.5 Introducing pandas.DataFrame.agg()",
    "text": "19.5 Introducing pandas.DataFrame.agg()\nTo get started, let’s consider how to get simple summary statistics without using agg(), then we will consider why you should actually use agg().\nImagine you were asked to find the mean age of respondents in the yao data frame. You can do this by calling the mean() method on the age column of the yao data frame:\n\nyao[[\"age\"]].mean()\n\nage    29.017508\ndtype: float64\n\n\nNow, let’s see how to do this with agg().\n\nyao.agg(mean_age=('age', 'mean'))\n\n\n\n\n\n\n\n\nage\n\n\n\n\nmean_age\n29.017508\n\n\n\n\n\n\n\nThe anatomy of this syntax is:\n\ndataframe.agg(summary_name=(\"COLUMN_TO_SUMMARIZE\", \"SUMMARY_FUNCTION\"))\n\nThis part (\"COLUMN_TO_SUMMARIZE\", \"SUMMARY_FUNCTION\") is called a tuple. The first element of the tuple is the name of the column to summarize, and the second element is the summary function to apply to that column.\nThe syntax is more complex, but as you will see later, much more powerful, as it allows you to compute multiple summary statistics, and to compute statistics per group.\n\nLet’s see how to compute multiple summary statistics in a single agg() statement. If you wanted both the mean and the median age, you could run:\n\nyao.agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n\n\n\n\n\n\n\n\nage\n\n\n\n\nmean_age\n29.017508\n\n\nmedian_age\n26.000000\n\n\n\n\n\n\n\nGreat, now try your hand at the practice question below.\n\n\n\n\n\n\nPractice\n\n\n\n19.6 Practice Q: Mean and median weight\nUse agg() and the relevant summary functions to obtain the mean and median of respondent weights from the weight_kg variable of the yao data frame.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#practice-q-mean-and-median-weight",
    "href": "p_untangled_groupby_agg.html#practice-q-mean-and-median-weight",
    "title": "19  Grouping and summarizing data",
    "section": "19.6 Practice Q: Mean and median weight",
    "text": "19.6 Practice Q: Mean and median weight\nUse agg() and the relevant summary functions to obtain the mean and median of respondent weights from the weight_kg variable of the yao data frame.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#grouped-summaries-with-pandas.dataframe.groupby",
    "href": "p_untangled_groupby_agg.html#grouped-summaries-with-pandas.dataframe.groupby",
    "title": "19  Grouping and summarizing data",
    "section": "19.7 Grouped summaries with pandas.DataFrame.groupby()",
    "text": "19.7 Grouped summaries with pandas.DataFrame.groupby()\nNow let’s see how to use groupby() to obtain grouped summaries, the primary reason for using agg() in the first place.\nAs its name suggests, pandas.DataFrame.groupby() lets you group a data frame by the values in a variable (e.g. male vs female sex). You can then perform operations that are split according to these groups.\nLet’s try to group the yao data frame by sex and observe the effect:\n\nyao.groupby(\"sex\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x150c4a2d0&gt;\n\n\nHmm. Apparently nothing happened. We just get a GroupBy object.\nBut watch what happens when we chain the groupby() with the agg() call we used in the previous section:\n\nyao.groupby(\"sex\").agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n\n\n\n\n\n\n\n\nmean_age\nmedian_age\n\n\nsex\n\n\n\n\n\n\nFemale\n29.495446\n26.0\n\n\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\nNow we get a different statistic for each group! The mean age for female respondents is about 29.5, while for male respondents it’s about 28.4.\nAs was mentioned earlier, this kind of grouped summary is the primary reason the agg() function is so useful.\nYou may notice that there are two header rows. This is because the output has a hierarchical index (called a MultiIndex in pandas). While this can be useful in some cases, it often makes further data manipulation more difficult. We can reset the index to convert the group labels back to a regular column with the reset_index() method.\n\nyao.groupby(\"sex\").agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\")).reset_index()\n\n\n\n\n\n\n\n\nsex\nmean_age\nmedian_age\n\n\n\n\n0\nFemale\n29.495446\n26.0\n\n\n1\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\nYou may notice that the line of code is getting quite long. We can move each new method call to a new line to make the code more readable, but we need to wrap the entire chain in parentheses.\n\n(\n    yao.groupby(\"sex\")\n    .agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\nmean_age\nmedian_age\n\n\n\n\n0\nFemale\n29.495446\n26.0\n\n\n1\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\n\nLet’s see one more example.\nSuppose you were asked to obtain the maximum and minimum weights for individuals in different neighborhoods, and also present the number of individuals in each neighborhood. We can write:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        max_weight=(\"weight_kg\", \"max\"),\n        min_weight=(\"weight_kg\", \"min\"),\n        count=(\"weight_kg\", \"size\"),  # the size function counts rows per group\n    )\n    .reset_index()\n) \n\n\n\n\n\n\n\n\nneighborhood\nmax_weight\nmin_weight\ncount\n\n\n\n\n0\nBriqueterie\n128\n20\n106\n\n\n1\nCarriere\n129\n14\n236\n\n\n2\nCité Verte\n118\n16\n72\n\n\n...\n...\n...\n...\n...\n\n\n6\nNkomkana\n161\n15\n75\n\n\n7\nTsinga\n105\n15\n81\n\n\n8\nTsinga Oliga\n100\n17\n67\n\n\n\n\n9 rows × 4 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n19.8 Practice Q: Min and max height per sex\nUse groupby(), agg(), and the relevant summary functions to obtain the minimum and maximum heights for each sex in the yao data frame, as well as the number of individuals in each sex group.\nYour output should be a DataFrame that looks like this:\n\n\n\nsex\nmin_height_cm\nmax_height_cm\ncount\n\n\n\n\nFemale\n\n\n\n\n\nMale\n\n\n\n\n\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#practice-q-min-and-max-height-per-sex",
    "href": "p_untangled_groupby_agg.html#practice-q-min-and-max-height-per-sex",
    "title": "19  Grouping and summarizing data",
    "section": "19.8 Practice Q: Min and max height per sex",
    "text": "19.8 Practice Q: Min and max height per sex\nUse groupby(), agg(), and the relevant summary functions to obtain the minimum and maximum heights for each sex in the yao data frame, as well as the number of individuals in each sex group.\nYour output should be a DataFrame that looks like this:\n\n\n\nsex\nmin_height_cm\nmax_height_cm\ncount\n\n\n\n\nFemale\n\n\n\n\n\nMale\n\n\n\n\n\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#grouping-by-multiple-variables-nested-grouping",
    "href": "p_untangled_groupby_agg.html#grouping-by-multiple-variables-nested-grouping",
    "title": "19  Grouping and summarizing data",
    "section": "19.9 Grouping by multiple variables (nested grouping)",
    "text": "19.9 Grouping by multiple variables (nested grouping)\nIt is possible to group a data frame by more than one variable. This is sometimes called “nested” grouping.\nSuppose you want to know the mean age of men and women in each neighbourhood, you could put both sex and neighborhood in the groupby() statement:\n\n(\n    yao\n    .groupby(['sex', 'neighborhood'])\n    .agg(mean_age=('age', 'mean'))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\nneighborhood\nmean_age\n\n\n\n\n0\nFemale\nBriqueterie\n31.622951\n\n\n1\nFemale\nCarriere\n28.164286\n\n\n2\nFemale\nCité Verte\n31.750000\n\n\n...\n...\n...\n...\n\n\n15\nMale\nNkomkana\n29.812500\n\n\n16\nMale\nTsinga\n28.820513\n\n\n17\nMale\nTsinga Oliga\n24.297297\n\n\n\n\n18 rows × 3 columns\n\n\n\nFrom this output data frame you can tell that, for example, women from Briqueterie have a mean age of 31.6 years.\n\n\n\n\n\n\nPractice\n\n\n\n19.10 Practice Q: Min and max height per age sex group\nUse groupby(), agg(), and min() and max() to get the minimum and maximum heights for each age-sex group in the yao data frame. The variables needed are age_category_3 and sex.\nYour output should be a DataFrame that looks like this:\n\n\n\nage_category_3\nsex\nmin_height\nmax_height\n\n\n\n\nAdult\nFemale\n78\n185\n\n\nAdult\nMale\n147\n196\n\n\nChild\nFemale\n54\n183\n\n\nChild\nMale\n96\n190\n\n\nSenior\nFemale\n143\n174\n\n\nSenior\nMale\n160\n195\n\n\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#practice-q-min-and-max-height-per-age-sex-group",
    "href": "p_untangled_groupby_agg.html#practice-q-min-and-max-height-per-age-sex-group",
    "title": "19  Grouping and summarizing data",
    "section": "19.10 Practice Q: Min and max height per age sex group",
    "text": "19.10 Practice Q: Min and max height per age sex group\nUse groupby(), agg(), and min() and max() to get the minimum and maximum heights for each age-sex group in the yao data frame. The variables needed are age_category_3 and sex.\nYour output should be a DataFrame that looks like this:\n\n\n\nage_category_3\nsex\nmin_height\nmax_height\n\n\n\n\nAdult\nFemale\n78\n185\n\n\nAdult\nMale\n147\n196\n\n\nChild\nFemale\n54\n183\n\n\nChild\nMale\n96\n190\n\n\nSenior\nFemale\n143\n174\n\n\nSenior\nMale\n160\n195\n\n\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#nan-values-in-agg",
    "href": "p_untangled_groupby_agg.html#nan-values-in-agg",
    "title": "19  Grouping and summarizing data",
    "section": "19.11 NaN values in agg()",
    "text": "19.11 NaN values in agg()\nWhen using agg() to compute grouped summary statistics, pay attention to whether your group of interest contains NaN values.\nFor example, to get mean weight by smoking status, we can write:\n\n(\n    yao.groupby(\"is_smoker\")\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n\n\n\n\n\nBut this actually excludes some rows with NaN smoking status from the summary table.\nWe can include these individuals in the summary table by setting dropna=False with the groupby() function.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n3\nNaN\n73.000000\n\n\n\n\n\n\n\nAlso recall that you can see how many individuals are in each smoking status group by using the size() function. It is often useful to include this information in your summary table, so that you know how many individuals are behind each summary statistic.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"), \n         count=(\"weight_kg\", \"size\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\ncount\n\n\n\n\n0\nEx-smoker\n76.366197\n71\n\n\n1\nNon-smoker\n63.033760\n859\n\n\n2\nSmoker\n72.410256\n39\n\n\n3\nNaN\n73.000000\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n19.12 Practice Q: Mean weight by pregnancy status\nUse groupby(), agg(), and the mean() function to obtain the mean weight (kg) by pregnancy status in the yao data frame. Include individuals with NaN pregnancy status in the summary table.\nThe output data frame should look something like this:\n\n\n\nis_pregnant\nweight_mean\n\n\n\n\nNo\n\n\n\nNo response\n\n\n\nYes\n\n\n\nNaN\n\n\n\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#practice-q-mean-weight-by-pregnancy-status",
    "href": "p_untangled_groupby_agg.html#practice-q-mean-weight-by-pregnancy-status",
    "title": "19  Grouping and summarizing data",
    "section": "19.12 Practice Q: Mean weight by pregnancy status",
    "text": "19.12 Practice Q: Mean weight by pregnancy status\nUse groupby(), agg(), and the mean() function to obtain the mean weight (kg) by pregnancy status in the yao data frame. Include individuals with NaN pregnancy status in the summary table.\nThe output data frame should look something like this:\n\n\n\nis_pregnant\nweight_mean\n\n\n\n\nNo\n\n\n\nNo response\n\n\n\nYes\n\n\n\nNaN\n\n\n\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#custom-summary-statistics-with-lambda-functions",
    "href": "p_untangled_groupby_agg.html#custom-summary-statistics-with-lambda-functions",
    "title": "19  Grouping and summarizing data",
    "section": "19.13 Custom summary statistics with lambda functions",
    "text": "19.13 Custom summary statistics with lambda functions\nBefore we dive into custom summary statistics, let’s briefly introduce lambda functions. Lambda functions in Python are small, anonymous functions defined with the lambda keyword.\nFor example, consider a function that calculates the range (difference between the maximum and minimum) of a list. You could define it using a regular function like this:\n\ndef range_func(x):\n    return max(x) - min(x)\n\nprint(range_func([1, 2, 3, 4]))  # Output: 3\n\n3\n\n\nAlternatively, you can achieve the same result using a lambda function:\n\nrange_func = lambda x: max(x) - min(x)\nprint(range_func([1, 2, 3, 4]))  # Output: 3\n\n3\n\n\nNow, let’s see how we can use lambda functions to apply custom summary statistics in our data analysis.\nFor example, let’s say we want to calculate the range of weights in each neighborhood. We could do this with our range_func function:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(weight_range=(\"weight_kg\", range_func))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nweight_range\n\n\n\n\n0\nBriqueterie\n108\n\n\n1\nCarriere\n115\n\n\n2\nCité Verte\n102\n\n\n...\n...\n...\n\n\n6\nNkomkana\n146\n\n\n7\nTsinga\n90\n\n\n8\nTsinga Oliga\n83\n\n\n\n\n9 rows × 2 columns\n\n\n\nNotice that we did not wrap range_func in quotes. Only built-in functions are wrapped in quotes.\nNow, instead of calling range_func we can use a lambda function directly in the agg() call:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(weight_range=(\"weight_kg\", lambda x: max(x) - min(x)))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nweight_range\n\n\n\n\n0\nBriqueterie\n108\n\n\n1\nCarriere\n115\n\n\n2\nCité Verte\n102\n\n\n...\n...\n...\n\n\n6\nNkomkana\n146\n\n\n7\nTsinga\n90\n\n\n8\nTsinga Oliga\n83\n\n\n\n\n9 rows × 2 columns\n\n\n\nNotice that we still provide a tuple to the agg() function, ('weight_kg', lambda x: max(x) - min(x)), but the second element of the tuple is a lambda function.\nThis lambda function operates on the column provided in the tuple, weight_kg.\nLet’s see another example: calculating the coefficient of variation (CV) of weight within each neighborhood. The CV is the standard deviation divided by the mean, and is a unitless measure of the relative variability of a distribution.\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(weight_cv=(\"weight_kg\", lambda x: (np.std(x) / np.mean(x)) * 100))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nweight_cv\n\n\n\n\n0\nBriqueterie\n33.531748\n\n\n1\nCarriere\n32.027533\n\n\n2\nCité Verte\n33.255829\n\n\n...\n...\n...\n\n\n6\nNkomkana\n33.187496\n\n\n7\nTsinga\n33.937145\n\n\n8\nTsinga Oliga\n35.894453\n\n\n\n\n9 rows × 2 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n19.14 Practice Q: IQR of age by neighborhood\nFind the interquartile range (IQR) of the age variable for each neighborhood. The IQR is the difference between the 75th and 25th percentiles. Your lambda will look like this: lambda x: x.quantile(0.75) - x.quantile(0.25)\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#practice-q-iqr-of-age-by-neighborhood",
    "href": "p_untangled_groupby_agg.html#practice-q-iqr-of-age-by-neighborhood",
    "title": "19  Grouping and summarizing data",
    "section": "19.14 Practice Q: IQR of age by neighborhood",
    "text": "19.14 Practice Q: IQR of age by neighborhood\nFind the interquartile range (IQR) of the age variable for each neighborhood. The IQR is the difference between the 75th and 25th percentiles. Your lambda will look like this: lambda x: x.quantile(0.75) - x.quantile(0.25)\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#wrap-up",
    "href": "p_untangled_groupby_agg.html#wrap-up",
    "title": "19  Grouping and summarizing data",
    "section": "19.15 Wrap up",
    "text": "19.15 Wrap up\nIn this lesson, you’ve learned how to obtain quick summary statistics from your data using agg(), group your data using groupby(), and combine groupby() with agg() for powerful data summarization.\nThese skills are essential for both exploratory data analysis and preparing data for presentation or plotting. The combination of groupby() and agg() is one of the most common and useful data manipulation techniques in pandas.\nIn our next lesson, we’ll explore ways to combine groupby() with other pandas methods.\nSee you there!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html",
    "href": "p_untangled_other_grouped_operations.html",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "",
    "text": "20.1 Introduction\nIn our previous lessons, you’ve learned how to extract summary statistics from groups using groupby() and agg(). Now, we’ll take a step further by exploring some additional useful grouped data transformations. Let’s get started.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#learning-objectives",
    "href": "p_untangled_other_grouped_operations.html#learning-objectives",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.2 Learning Objectives",
    "text": "20.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nAdd group-level summary statistics as new columns using transform().\nCount values within groups using value_counts().\nCompute cumulative sums within groups.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#imports",
    "href": "p_untangled_other_grouped_operations.html#imports",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.3 Imports",
    "text": "20.3 Imports\nRun the following cell to import the necessary libraries:\n\nimport pandas as pd\nimport vega_datasets as vd\nimport plotly.express as px\nimport warnings\nimport calendar",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#data",
    "href": "p_untangled_other_grouped_operations.html#data",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.4 Data",
    "text": "20.4 Data\nWe’ll use the weather dataset for our examples.\n\nweather_raw = vd.data.seattle_weather()\n\n# Select just 2012 data using query and add a month column\nweather = weather_raw.query(\"date.dt.year == 2012\")\nweather[\"month\"] = pd.Categorical(\n    weather[\"date\"].dt.strftime(\"%B\"),\n    categories=list(calendar.month_name[1:]),\n    ordered=True,\n)\nweather\n\n/var/folders/vr/shb6ffvj2rl61kh7qqczhrgh0000gp/T/ipykernel_51212/2556095375.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\nmonth\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\nJanuary\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\nJanuary\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\nJanuary\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\nJanuary\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\nJanuary\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\n4.1\n7.8\n3.3\n3.2\nrain\nDecember\n\n\n362\n2012-12-28\n0.0\n8.3\n3.9\n1.7\nrain\nDecember\n\n\n363\n2012-12-29\n1.5\n5.0\n3.3\n1.7\nrain\nDecember\n\n\n364\n2012-12-30\n0.0\n4.4\n0.0\n1.8\ndrizzle\nDecember\n\n\n365\n2012-12-31\n0.0\n3.3\n-1.1\n2.0\ndrizzle\nDecember\n\n\n\n\n366 rows × 7 columns\n\n\n\nNow let’s set the display options for the rest of the lesson:\n\npd.options.display.max_rows = 20\n\nAnd let’s ignore the warnings that come up when working with categorical data with the current version of pandas:\n\nwarnings.filterwarnings(\n    \"ignore\"\n)  ## There is a class of warnings that come up when working with categorical data with the current version of pandas that we can ignore",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#adding-summary-statistics-using-transform",
    "href": "p_untangled_other_grouped_operations.html#adding-summary-statistics-using-transform",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.5 Adding Summary Statistics Using transform()",
    "text": "20.5 Adding Summary Statistics Using transform()\nIn the previous lesson, you learned how to calculate summary statistics like mean, median, or standard deviation using agg().\nFor example, to compute the mean precipitation (rain + snow) for each month, you could use:\n\nweather.groupby('month').agg(mean_precip = ('precipitation', 'mean'))\n\n\n\n\n\n\n\n\nmean_precip\n\n\nmonth\n\n\n\n\n\nJanuary\n5.590323\n\n\nFebruary\n3.182759\n\n\nMarch\n5.903226\n\n\nApril\n2.270000\n\n\nMay\n1.683871\n\n\n...\n...\n\n\nAugust\n0.000000\n\n\nSeptember\n0.030000\n\n\nOctober\n5.493548\n\n\nNovember\n7.016667\n\n\nDecember\n5.612903\n\n\n\n\n12 rows × 1 columns\n\n\n\nSometimes, we want to add these group-level statistics as new columns to our original DataFrame. We can’t do this directly with the agg() output:\n\n# Does not work\nweather['mean_precip'] = weather.groupby('month').agg(mean_precip = ('precipitation', 'mean'))\nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\nmonth\nmean_precip\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\nJanuary\nNaN\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\nJanuary\nNaN\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\nJanuary\nNaN\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\nJanuary\nNaN\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\nJanuary\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\n4.1\n7.8\n3.3\n3.2\nrain\nDecember\nNaN\n\n\n362\n2012-12-28\n0.0\n8.3\n3.9\n1.7\nrain\nDecember\nNaN\n\n\n363\n2012-12-29\n1.5\n5.0\n3.3\n1.7\nrain\nDecember\nNaN\n\n\n364\n2012-12-30\n0.0\n4.4\n0.0\n1.8\ndrizzle\nDecember\nNaN\n\n\n365\n2012-12-31\n0.0\n3.3\n-1.1\n2.0\ndrizzle\nDecember\nNaN\n\n\n\n\n366 rows × 8 columns\n\n\n\nBut we can do this using transform(). transform() reshapes the output to match the original DataFrame’s shape, allowing us to add the group-level statistics as new columns.\n\nweather['mean_precip_month'] = weather.groupby('month')['precipitation'].transform('mean')\nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\nmonth\nmean_precip\nmean_precip_month\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\nJanuary\nNaN\n5.590323\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\nJanuary\nNaN\n5.590323\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\nJanuary\nNaN\n5.590323\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\nJanuary\nNaN\n5.590323\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\nJanuary\nNaN\n5.590323\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\n4.1\n7.8\n3.3\n3.2\nrain\nDecember\nNaN\n5.612903\n\n\n362\n2012-12-28\n0.0\n8.3\n3.9\n1.7\nrain\nDecember\nNaN\n5.612903\n\n\n363\n2012-12-29\n1.5\n5.0\n3.3\n1.7\nrain\nDecember\nNaN\n5.612903\n\n\n364\n2012-12-30\n0.0\n4.4\n0.0\n1.8\ndrizzle\nDecember\nNaN\n5.612903\n\n\n365\n2012-12-31\n0.0\n3.3\n-1.1\n2.0\ndrizzle\nDecember\nNaN\n5.612903\n\n\n\n\n366 rows × 9 columns\n\n\n\nYou can compute other statistics similarly. For example, to compute the median precipitation for each month, you could use:\n\nweather['prep_median_month'] = weather.groupby('month')['precipitation'].transform('median')    \nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\nmonth\nmean_precip\nmean_precip_month\nprep_median_month\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\nJanuary\nNaN\n5.590323\n3.0\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\nJanuary\nNaN\n5.590323\n3.0\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\nJanuary\nNaN\n5.590323\n3.0\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\nJanuary\nNaN\n5.590323\n3.0\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\nJanuary\nNaN\n5.590323\n3.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\n4.1\n7.8\n3.3\n3.2\nrain\nDecember\nNaN\n5.612903\n3.3\n\n\n362\n2012-12-28\n0.0\n8.3\n3.9\n1.7\nrain\nDecember\nNaN\n5.612903\n3.3\n\n\n363\n2012-12-29\n1.5\n5.0\n3.3\n1.7\nrain\nDecember\nNaN\n5.612903\n3.3\n\n\n364\n2012-12-30\n0.0\n4.4\n0.0\n1.8\ndrizzle\nDecember\nNaN\n5.612903\n3.3\n\n\n365\n2012-12-31\n0.0\n3.3\n-1.1\n2.0\ndrizzle\nDecember\nNaN\n5.612903\n3.3\n\n\n\n\n366 rows × 10 columns\n\n\n\nOr to get the sum of precipitation for each month:\n\nweather['precip_sum_month'] = weather.groupby('month')['precipitation'].transform('sum')\nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\nmonth\nmean_precip\nmean_precip_month\nprep_median_month\nprecip_sum_month\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\nJanuary\nNaN\n5.590323\n3.0\n173.3\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\n4.1\n7.8\n3.3\n3.2\nrain\nDecember\nNaN\n5.612903\n3.3\n174.0\n\n\n362\n2012-12-28\n0.0\n8.3\n3.9\n1.7\nrain\nDecember\nNaN\n5.612903\n3.3\n174.0\n\n\n363\n2012-12-29\n1.5\n5.0\n3.3\n1.7\nrain\nDecember\nNaN\n5.612903\n3.3\n174.0\n\n\n364\n2012-12-30\n0.0\n4.4\n0.0\n1.8\ndrizzle\nDecember\nNaN\n5.612903\n3.3\n174.0\n\n\n365\n2012-12-31\n0.0\n3.3\n-1.1\n2.0\ndrizzle\nDecember\nNaN\n5.612903\n3.3\n174.0\n\n\n\n\n366 rows × 11 columns\n\n\n\nWith the sum in hand, we can easily calculate the proportion of that month’s precipitation that fell on each day:\n\nweather[\"precip_month_prop\"] = weather[\"precipitation\"] / weather[\"precip_sum_month\"]\nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\nmonth\nmean_precip\nmean_precip_month\nprep_median_month\nprecip_sum_month\nprecip_month_prop\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\nJanuary\nNaN\n5.590323\n3.0\n173.3\n0.000000\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n0.062897\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n0.004616\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n0.117138\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\nJanuary\nNaN\n5.590323\n3.0\n173.3\n0.007501\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\n4.1\n7.8\n3.3\n3.2\nrain\nDecember\nNaN\n5.612903\n3.3\n174.0\n0.023563\n\n\n362\n2012-12-28\n0.0\n8.3\n3.9\n1.7\nrain\nDecember\nNaN\n5.612903\n3.3\n174.0\n0.000000\n\n\n363\n2012-12-29\n1.5\n5.0\n3.3\n1.7\nrain\nDecember\nNaN\n5.612903\n3.3\n174.0\n0.008621\n\n\n364\n2012-12-30\n0.0\n4.4\n0.0\n1.8\ndrizzle\nDecember\nNaN\n5.612903\n3.3\n174.0\n0.000000\n\n\n365\n2012-12-31\n0.0\n3.3\n-1.1\n2.0\ndrizzle\nDecember\nNaN\n5.612903\n3.3\n174.0\n0.000000\n\n\n\n\n366 rows × 12 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n20.6 Practice Q: Daily Tip Proportions\nUsing the tips dataset, calculate: 1. A new column daily_total_tips containing the total tips for each day 2. A new column tip_proportion showing what proportion of that day’s total tips came from each customer\n\n# Your code here:\ntips = px.data.tips()\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nThe first few rows of your output data should look something like this:\ntotal_bill    tip     sex      smoker    day    time      size    daily_total_tips    tip_proportion\n16.99         1.01    Female   No        Sun    Dinner    2       247.39              0.004083\n10.34         1.66    Male     No        Sun    Dinner    3       247.39              0.006710\n21.01         3.50    Male     No        Sun    Dinner    3       247.39              0.014148\n23.68         3.31    Male     No        Sun    Dinner    2       247.39              0.013380\n\n\nLet’s reinitialize the weather DataFrame to a smaller set of columns for the rest of the lesson:\n\nweather = weather[['date', 'month', 'precipitation', 'wind', 'weather']]\nweather\n\n\n\n\n\n\n\n\ndate\nmonth\nprecipitation\nwind\nweather\n\n\n\n\n0\n2012-01-01\nJanuary\n0.0\n4.7\ndrizzle\n\n\n1\n2012-01-02\nJanuary\n10.9\n4.5\nrain\n\n\n2\n2012-01-03\nJanuary\n0.8\n2.3\nrain\n\n\n3\n2012-01-04\nJanuary\n20.3\n4.7\nrain\n\n\n4\n2012-01-05\nJanuary\n1.3\n6.1\nrain\n\n\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\nDecember\n4.1\n3.2\nrain\n\n\n362\n2012-12-28\nDecember\n0.0\n1.7\nrain\n\n\n363\n2012-12-29\nDecember\n1.5\n1.7\nrain\n\n\n364\n2012-12-30\nDecember\n0.0\n1.8\ndrizzle\n\n\n365\n2012-12-31\nDecember\n0.0\n2.0\ndrizzle\n\n\n\n\n366 rows × 5 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#practice-q-daily-tip-proportions",
    "href": "p_untangled_other_grouped_operations.html#practice-q-daily-tip-proportions",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.6 Practice Q: Daily Tip Proportions",
    "text": "20.6 Practice Q: Daily Tip Proportions\nUsing the tips dataset, calculate: 1. A new column daily_total_tips containing the total tips for each day 2. A new column tip_proportion showing what proportion of that day’s total tips came from each customer\n\n# Your code here:\ntips = px.data.tips()\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nThe first few rows of your output data should look something like this:\ntotal_bill    tip     sex      smoker    day    time      size    daily_total_tips    tip_proportion\n16.99         1.01    Female   No        Sun    Dinner    2       247.39              0.004083\n10.34         1.66    Male     No        Sun    Dinner    3       247.39              0.006710\n21.01         3.50    Male     No        Sun    Dinner    3       247.39              0.014148\n23.68         3.31    Male     No        Sun    Dinner    2       247.39              0.013380",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#counting-values-within-groups-using-value_counts",
    "href": "p_untangled_other_grouped_operations.html#counting-values-within-groups-using-value_counts",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.7 Counting Values Within Groups Using value_counts()",
    "text": "20.7 Counting Values Within Groups Using value_counts()\nCounting occurrences of categorical variables within groups can reveal interesting patterns, and you often need to do this after using groupby().\nFirst, let’s recall how value_counts() works on the entire DataFrame.\n\n# Count of weather types\nweather[\"weather\"].value_counts()\n\nweather\nrain       191\nsun        118\ndrizzle     31\nsnow        21\nfog          5\nName: count, dtype: int64\n\n\nWe can add normalize=True to get proportions:\n\nweather['weather'].value_counts(normalize=True)\n\nweather\nrain       0.521858\nsun        0.322404\ndrizzle    0.084699\nsnow       0.057377\nfog        0.013661\nName: proportion, dtype: float64\n\n\nNow, to count weather types within each month, we first group by month, then subset the weather column and apply value_counts() to it.\n\n# Counts of weather types per month\nweather.groupby('month')['weather'].value_counts()\n\nmonth     weather\nJanuary   rain       18\n          snow        7\n          sun         4\n          drizzle     2\n          fog         0\n                     ..\nDecember  rain       23\n          snow        5\n          drizzle     2\n          sun         1\n          fog         0\nName: count, Length: 60, dtype: int64\n\n\nThis returns a Series with a MultiIndex, which can be converted to a regular DataFrame with reset_index():\n\nweather.groupby('month')['weather'].value_counts().reset_index()\n\n\n\n\n\n\n\n\nmonth\nweather\ncount\n\n\n\n\n0\nJanuary\nrain\n18\n\n\n1\nJanuary\nsnow\n7\n\n\n2\nJanuary\nsun\n4\n\n\n3\nJanuary\ndrizzle\n2\n\n\n4\nJanuary\nfog\n0\n\n\n...\n...\n...\n...\n\n\n55\nDecember\nrain\n23\n\n\n56\nDecember\nsnow\n5\n\n\n57\nDecember\ndrizzle\n2\n\n\n58\nDecember\nsun\n1\n\n\n59\nDecember\nfog\n0\n\n\n\n\n60 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n20.8 Practice Q: Count Smokers and Non-Smokers by Day\nUsing the tips dataset, count the number of smokers and non-smokers for each day.\n\ntips = px.data.tips()\ntips\n\n# Your code here:\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nThe first few rows of your result should look something like this:\nday smoker  count\nFri Yes 15\nFri No  4\nSat No  45",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#practice-q-count-smokers-and-non-smokers-by-day",
    "href": "p_untangled_other_grouped_operations.html#practice-q-count-smokers-and-non-smokers-by-day",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.8 Practice Q: Count Smokers and Non-Smokers by Day",
    "text": "20.8 Practice Q: Count Smokers and Non-Smokers by Day\nUsing the tips dataset, count the number of smokers and non-smokers for each day.\n\ntips = px.data.tips()\ntips\n\n# Your code here:\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nThe first few rows of your result should look something like this:\nday smoker  count\nFri Yes 15\nFri No  4\nSat No  45",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#computing-cumulative-sums-within-groups",
    "href": "p_untangled_other_grouped_operations.html#computing-cumulative-sums-within-groups",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.9 Computing Cumulative Sums Within Groups",
    "text": "20.9 Computing Cumulative Sums Within Groups\nCumulative sums help track running totals within groups. This is an often-useful operation. Let’s see how we can do this to grouped data.\nAs a recall, here’s how we can compute the cumulative sum of precipitation for the entire DataFrame:\n\n# Cumulative sum of precipitation\nweather[\"precip_cumul\"] = weather[\"precipitation\"].cumsum()\nweather\n\n\n\n\n\n\n\n\ndate\nmonth\nprecipitation\nwind\nweather\nprecip_cumul\n\n\n\n\n0\n2012-01-01\nJanuary\n0.0\n4.7\ndrizzle\n0.0\n\n\n1\n2012-01-02\nJanuary\n10.9\n4.5\nrain\n10.9\n\n\n2\n2012-01-03\nJanuary\n0.8\n2.3\nrain\n11.7\n\n\n3\n2012-01-04\nJanuary\n20.3\n4.7\nrain\n32.0\n\n\n4\n2012-01-05\nJanuary\n1.3\n6.1\nrain\n33.3\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\nDecember\n4.1\n3.2\nrain\n1224.5\n\n\n362\n2012-12-28\nDecember\n0.0\n1.7\nrain\n1224.5\n\n\n363\n2012-12-29\nDecember\n1.5\n1.7\nrain\n1226.0\n\n\n364\n2012-12-30\nDecember\n0.0\n1.8\ndrizzle\n1226.0\n\n\n365\n2012-12-31\nDecember\n0.0\n2.0\ndrizzle\n1226.0\n\n\n\n\n366 rows × 6 columns\n\n\n\nTo compute cumulative precipitation within each month, we can use groupby() and cumsum():\n\n# Cumulative precipitation per month\nweather[\"precip_cumul\"] = weather.groupby(\"month\")[\"precipitation\"].cumsum()\nweather\n\n\n\n\n\n\n\n\ndate\nmonth\nprecipitation\nwind\nweather\nprecip_cumul\n\n\n\n\n0\n2012-01-01\nJanuary\n0.0\n4.7\ndrizzle\n0.0\n\n\n1\n2012-01-02\nJanuary\n10.9\n4.5\nrain\n10.9\n\n\n2\n2012-01-03\nJanuary\n0.8\n2.3\nrain\n11.7\n\n\n3\n2012-01-04\nJanuary\n20.3\n4.7\nrain\n32.0\n\n\n4\n2012-01-05\nJanuary\n1.3\n6.1\nrain\n33.3\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n361\n2012-12-27\nDecember\n4.1\n3.2\nrain\n172.5\n\n\n362\n2012-12-28\nDecember\n0.0\n1.7\nrain\n172.5\n\n\n363\n2012-12-29\nDecember\n1.5\n1.7\nrain\n174.0\n\n\n364\n2012-12-30\nDecember\n0.0\n1.8\ndrizzle\n174.0\n\n\n365\n2012-12-31\nDecember\n0.0\n2.0\ndrizzle\n174.0\n\n\n\n\n366 rows × 6 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#practice-q-cumulative-tip-amount-by-day",
    "href": "p_untangled_other_grouped_operations.html#practice-q-cumulative-tip-amount-by-day",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.10 Practice Q: Cumulative Tip Amount by Day",
    "text": "20.10 Practice Q: Cumulative Tip Amount by Day\nUsing the tips dataset, compute the cumulative sum of total_bill for each day, adding a new column cumul_total_bill_day. Then add another column cumul_tip_day that contains the cumulative sum of tip for each day.\n\ntips = px.data.tips()\ntips = tips.sort_values('day')\ntips\n# Your code here:\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n96\n27.28\n4.00\nMale\nYes\nFri\nDinner\n2\n\n\n101\n15.38\n3.00\nFemale\nYes\nFri\nDinner\n2\n\n\n98\n21.01\n3.00\nMale\nYes\nFri\nDinner\n2\n\n\n97\n12.03\n1.50\nMale\nYes\nFri\nDinner\n2\n\n\n95\n40.17\n4.73\nMale\nYes\nFri\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n132\n11.17\n1.50\nFemale\nNo\nThur\nLunch\n2\n\n\n131\n20.27\n2.83\nFemale\nNo\nThur\nLunch\n2\n\n\n130\n19.08\n1.50\nMale\nNo\nThur\nLunch\n2\n\n\n128\n11.38\n2.00\nFemale\nNo\nThur\nLunch\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#wrap-up",
    "href": "p_untangled_other_grouped_operations.html#wrap-up",
    "title": "20  Other Grouped Operations in Pandas",
    "section": "20.11 Wrap-Up",
    "text": "20.11 Wrap-Up\nIn this lesson, you’ve learned several powerful group-level data transformations in pandas:\n\nAdding Summary Statistics: Using transform() to add group-level calculations as new columns\nCounting Within Groups: Using value_counts() to count occurrences in groups\nComputing Cumulative Sums: Tracking running totals within groups\n\nThese techniques allow you to analyze patterns and statistics within specific subsets of your data. Keep practicing with different datasets to build your data manipulation skills!\nSee you next time!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Other Grouped Operations in Pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html",
    "href": "p_untangled_joining_1.html",
    "title": "21  Introduction to Joining Datasets",
    "section": "",
    "text": "21.1 Data & Packages\nPlease run the code below to load the packages and datasets we’ll be using throughout this lesson.\nimport pandas as pd\n\n\n# TB incidence in Africa\ntb_2019_africa = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/tb_incidence_2019.csv\"\n) \n\n# Health expenditure data\nhealth_exp_2019 = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/health_expend_per_cap_2019.csv\"\n)\n\n# Highest expenditure countries\nhighest_exp = health_exp_2019.sort_values(\"expend_usd\", ascending=False).head(70)\n\n# TB cases in children\ntb_cases_children = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/tb_cases_children_2012.csv\"\n).dropna()\n\n# Country continents data\ncountry_continents = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/country_continents.csv\"\n)\n\n# people data\npeople = pd.DataFrame({\"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"age\": [25, 32, 45]})\n\n# Test information\ntest_info = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"test_date\": [\"2023-06-05\", \"2023-08-10\", \"2023-07-15\"],\n        \"result\": [\"Negative\", \"Positive\", \"Negative\"],\n    }\n)\n\n# Disordered test information\ntest_info_disordered = pd.DataFrame(\n    {\n        \"name\": [\"Bob\", \"Alice\", \"Charlie\"],  # Bob in first row\n        \"test_date\": [\"2023-08-10\", \"2023-06-05\", \"2023-07-15\"],\n        \"result\": [\"Positive\", \"Negative\", \"Negative\"],\n    }\n)\n\n# Multiple test information\ntest_info_multiple = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Alice\", \"Bob\", \"Charlie\"],\n        \"test_date\": [\"2023-06-05\", \"2023-06-06\", \"2023-08-10\", \"2023-07-15\"],\n        \"result\": [\"Negative\", \"Negative\", \"Positive\", \"Negative\"],\n    }\n)\n\n# Test information with different name\ntest_info_different_name = pd.DataFrame(\n    {\n        \"first_name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"test_date\": [\"2023-06-05\", \"2023-08-10\", \"2023-07-15\"],\n        \"result\": [\"Negative\", \"Positive\", \"Negative\"],\n    }\n)\n\n# Test information including Xavier\ntest_info_xavier = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Xavier\"],\n        \"test_date\": [\"2023-06-05\", \"2023-08-10\", \"2023-05-02\"],\n        \"result\": [\"Negative\", \"Positive\", \"Negative\"],\n    }\n)\n\n# Students data\nstudents = pd.DataFrame(\n    {\"student_id\": [1, 2, 3], \"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"age\": [20, 22, 21]}\n)\n\n# Exam dates data\nexam_dates = pd.DataFrame(\n    {\"student_id\": [1, 3], \"exam_date\": [\"2023-05-20\", \"2023-05-22\"]}\n)\n\n# Employee details\nemployee_details = pd.DataFrame(\n    {\n        \"id_number\": [\"E001\", \"E002\", \"E003\"],\n        \"full_name\": [\"Emily\", \"Frank\", \"Grace\"],\n        \"department\": [\"HR\", \"IT\", \"Marketing\"],\n    }\n)\n\n# Performance reviews\nperformance_reviews = pd.DataFrame(\n    {\n        \"employee_code\": [\"E001\", \"E002\", \"E003\"],\n        \"review_type\": [\"Annual\", \"Mid-year\", \"Annual\"],\n        \"review_date\": [\"2022-05-10\", \"2023-09-01\", \"2021-12-15\"],\n    }\n)\n\n# Sales data\nsales_data = pd.DataFrame(\n    {\n        \"salesperson_id\": [1, 4, 8],\n        \"product\": [\"Laptop\", \"Smartphone\", \"Tablet\"],\n        \"date_of_sale\": [\"2023-01-15\", \"2023-03-05\", \"2023-02-20\"],\n    }\n)\n\n# Salesperson peoples\nsalesperson_peoples = pd.DataFrame(\n    {\n        \"salesperson_id\": [1, 2, 3, 5, 8],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n        \"age\": [28, 45, 32, 55, 40],\n        \"gender\": [\"Female\", \"Male\", \"Male\", \"Female\", \"Female\"],\n    }\n)\n\n# Total sales data\ntotal_sales = pd.DataFrame(\n    {\n        \"product\": [\n            \"Laptop\",\n            \"Desktop\",\n            \"Tablet\",\n            \"Smartphone\",\n            \"Smartwatch\",\n            \"Headphones\",\n            \"Monitor\",\n            \"Keyboard\",\n            \"Mouse\",\n            \"Printer\",\n        ],\n        \"total_units_sold\": [9751, 136, 8285, 2478, 3642, 5231, 1892, 4267, 3891, 982],\n    }\n)\n\n# Product feedback data\nproduct_feedback = pd.DataFrame(\n    {\n        \"product\": [\n            \"Laptop\",\n            \"Desktop\",\n            \"Tablet\",\n            \"Smartphone\",\n            \"Smartwatch\",\n            \"Headphones\",\n            \"Monitor\",\n            \"Gaming Console\",\n            \"Camera\",\n            \"Speaker\",\n        ],\n        \"n_positive_reviews\": [1938, 128, 842, 1567, 723, 956, 445, 582, 234, 678],\n        \"n_negative_reviews\": [42, 30, 56, 89, 34, 28, 15, 11, 8, 25],\n    }\n)\n\n# Sales incidence data\nsales = pd.DataFrame(\n    {\n        \"year\": [2010, 2011, 2014, 2016, 2017],\n        \"sales_count\": [69890, 66507, 59831, 58704, 59151],\n    }\n)\n\n# Customer complaints data\ncustomer_complaints = pd.DataFrame(\n    {\n        \"year\": [2011, 2013, 2015, 2016, 2019],\n        \"complaints_count\": [1292, 1100, 1011, 940, 895],\n    }\n)\n\n\nemployees = pd.DataFrame(\n    {\"employee_id\": [1, 2, 3], \"name\": [\"John\", \"Joy\", \"Khan\"], \"age\": [32, 28, 40]}\n)\n\ntraining_sessions = pd.DataFrame(\n    {\n        \"employee_id\": [1, 2, 3],\n        \"training_date\": [\"2023-01-20\", \"2023-02-20\", \"2023-05-15\"],\n    }\n)\n\ncustomer_details = pd.DataFrame(\n    {\n        \"id_number\": [\"A001\", \"B002\", \"C003\"],\n        \"full_name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"address\": [\"123 Elm St\", \"456 Maple Dr\", \"789 Oak Blvd\"],\n    }\n)\n\n# Order Records\norder_records = pd.DataFrame(\n    {\n        \"customer_code\": [\"A001\", \"B002\", \"C003\"],\n        \"product_type\": [\"Electronics\", \"Books\", \"Clothing\"],\n        \"order_date\": [\"2022-05-10\", \"2023-09-01\", \"2021-12-15\"],\n    }\n)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#intro",
    "href": "p_untangled_joining_1.html#intro",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.2 Intro",
    "text": "21.2 Intro\nJoining is a key skill when working with data as it allows you to combine information about the same entities from multiple sources, leading to more comprehensive and insightful analyses. In this lesson, you’ll learn how to use different joining techniques using Python’s pandas library. Let’s get started!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#learning-objectives",
    "href": "p_untangled_joining_1.html#learning-objectives",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.3 Learning Objectives",
    "text": "21.3 Learning Objectives\n\nYou understand how each of the different joins work: left, right, inner, and outer.\nYou can join simple datasets together using the pd.merge() function.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#why-do-we-need-joins",
    "href": "p_untangled_joining_1.html#why-do-we-need-joins",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.4 Why Do We Need Joins?",
    "text": "21.4 Why Do We Need Joins?\nTo illustrate the utility of joins, let’s start with a toy example. Consider the following two datasets. The first, people, contains names and ages of three individuals:\n\npeople\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n32\n\n\n2\nCharlie\n45\n\n\n\n\n\n\n\nThe second, test_info, contains test dates and results for those individuals:\n\ntest_info\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nWe’d like to analyze these data together, and so we need a way to combine them.\nOne option we might consider is concatenating the dataframes horizontally using pd.concat():\n\npd.concat([people, test_info], axis=1)\n\n\n\n\n\n\n\n\nname\nage\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n32\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nThis successfully merges the datasets, but it doesn’t do so very intelligently. The function essentially “pastes” or “staples” the two tables together. So, as you can notice, the “name” column appears twice. This is not ideal and will be problematic for analysis.\nAnother problem occurs if the rows in the two datasets are not already aligned. In this case, the data will be combined incorrectly with pd.concat(). Consider the test_info_disordered dataset, which now has Bob in the first row:\n\ntest_info_disordered\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nBob\n2023-08-10\nPositive\n\n\n1\nAlice\n2023-06-05\nNegative\n\n\n2\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nWhat happens if we concatenate this with the original people dataset, where Bob was in the second row?\n\npd.concat([people, test_info_disordered], axis=1)\n\n\n\n\n\n\n\n\nname\nage\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\nBob\n2023-08-10\nPositive\n\n\n1\nBob\n32\nAlice\n2023-06-05\nNegative\n\n\n2\nCharlie\n45\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nAlice’s people details are now mistakenly aligned with Bob’s test info!\nA third issue arises when an entity appears more than once in one dataset. Perhaps Alice had multiple tests:\n\ntest_info_multiple\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nAlice\n2023-06-06\nNegative\n\n\n2\nBob\n2023-08-10\nPositive\n\n\n3\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nIf we try to concatenate this with the people dataset, we’ll get mismatched data due to differing row counts:\n\npd.concat([people, test_info_multiple], axis=1)\n\n\n\n\n\n\n\n\nname\nage\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n25.0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n32.0\nAlice\n2023-06-06\nNegative\n\n\n2\nCharlie\n45.0\nBob\n2023-08-10\nPositive\n\n\n3\nNaN\nNaN\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nThis results in NaN values and misaligned data.\n\n\n\n\n\n\nSide Note\n\n\n\nWhat we have here is called a one-to-many relationship—one Alice in the people data, but multiple Alice rows in the test data since she had multiple tests. Joining in such cases will be covered in detail in the second joining lesson.\n\n\n\nClearly, we need a smarter way to combine datasets than concatenation; we’ll need to venture into the world of joining. In pandas, the function that performs joins is pd.merge().\nIt works for the simple case, and it does not duplicate the name column:\n\npd.merge(people, test_info)\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\n2023-07-15\nNegative\n\n\n\n\n\n\n\nIt works where the datasets are not ordered identically:\n\npd.merge(people, test_info_disordered)\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\n2023-07-15\nNegative\n\n\n\n\n\n\n\nAs you can see, Alice’s details are now correctly aligned with her test results.\nAnd it works when there are multiple test rows per individual:\n\npd.merge(people, test_info_multiple)\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nAlice\n25\n2023-06-06\nNegative\n\n\n2\nBob\n32\n2023-08-10\nPositive\n\n\n3\nCharlie\n45\n2023-07-15\nNegative\n\n\n\n\n\n\n\nIn this case, the pd.merge() function correctly repeats Alice’s details for each of her tests.\nSimple and beautiful!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#pd.merge-syntax",
    "href": "p_untangled_joining_1.html#pd.merge-syntax",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.5 pd.merge() syntax",
    "text": "21.5 pd.merge() syntax\nNow that we understand why we need joins, let’s look at their basic syntax.\nJoins take two dataframes as the first two arguments: left (the left dataframe) and right (the right dataframe). In pandas, you can provide these as positional or keyword arguments:\n\n# left and right\npd.merge(left=people, right=test_info)  # keyword arguments\npd.merge(people, test_info)  # positional arguments\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\n2023-07-15\nNegative\n\n\n\n\n\n\n\nAnother critical argument is on, which indicates the column or key used to connect the tables. We don’t always need to supply this argument; it can be inferred from the datasets. For example, in our original examples, “name” is the only column common to people and test_info. So the merge function assumes on='name':\n\n# on argument is optional if the column key is the same in both dataframes\npd.merge(people, test_info)\npd.merge(people, test_info, on=\"name\")\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\n2023-07-15\nNegative\n\n\n\n\n\n\n\n\n\n\n\n\n\nVocab\n\n\n\nThe column used to connect rows across the tables is known as a key. In the pandas merge() function, the key is specified in the on argument, as seen in pd.merge(people, test_info, on='name').\n\n\nWhat happens if the keys are named differently in the two datasets? Consider the test_info_different_name dataset, where the “name” column has been changed to “first_name”:\n\ntest_info_different_name\n\n\n\n\n\n\n\n\nfirst_name\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nIf we try to join test_info_different_name with our original people dataset, we will encounter an error:\n\npd.merge(people, test_info_different_name)\n\nMergeError: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False\nThe error indicates that there are no common variables, so the join is not possible.\nIn situations like this, you have two choices: you can rename the column in the second dataframe to match the first, or more simply, specify which columns to join on using left_on and right_on.\nHere’s how to do this:\n\npd.merge(people, test_info_different_name, left_on='name', right_on='first_name')\n\n\n\n\n\n\n\n\nname\nage\nfirst_name\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n32\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\nCharlie\n2023-07-15\nNegative\n\n\n\n\n\n\n\nThis syntax essentially says, “Connect name from the left dataframe with first_name from the right dataframe because they represent the same data.”\n\n\n\n\n\n\n\nVocab\n\n\n\nKey: The column or set of columns used to match rows between two dataframes in a join operation.\nLeft Join: A type of join that keeps all rows from the left dataframe and adds matching rows from the right dataframe. If there is no match, the result is NaN on the right side.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n21.6 Practice Q: Join Employees and Training Sessions\nConsider the two datasets below, one with employee details and the other with training session dates for these employees.\n\nemployees\n\n\n\n\n\n\n\n\nemployee_id\nname\nage\n\n\n\n\n0\n1\nJohn\n32\n\n\n1\n2\nJoy\n28\n\n\n2\n3\nKhan\n40\n\n\n\n\n\n\n\n\ntraining_sessions\n\n\n\n\n\n\n\n\nemployee_id\ntraining_date\n\n\n\n\n0\n1\n2023-01-20\n\n\n1\n2\n2023-02-20\n\n\n2\n3\n2023-05-15\n\n\n\n\n\n\n\nHow many rows and columns would you expect to have after joining these two datasets?\nNow join the two datasets and check your answer.\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n21.7 Practice Q: Join with on Argument\nTwo datasets are shown below, one with customer details and the other with order records for those customers.\n\ncustomer_details\n\n\n\n\n\n\n\n\nid_number\nfull_name\naddress\n\n\n\n\n0\nA001\nAlice\n123 Elm St\n\n\n1\nB002\nBob\n456 Maple Dr\n\n\n2\nC003\nCharlie\n789 Oak Blvd\n\n\n\n\n\n\n\n\norder_records\n\n\n\n\n\n\n\n\ncustomer_code\nproduct_type\norder_date\n\n\n\n\n0\nA001\nElectronics\n2022-05-10\n\n\n1\nB002\nBooks\n2023-09-01\n\n\n2\nC003\nClothing\n2021-12-15\n\n\n\n\n\n\n\nJoin the customer_details and order_records datasets. You will need to use the left_on and right_on arguments because the customer identifier columns have different names.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#practice-q-join-employees-and-training-sessions",
    "href": "p_untangled_joining_1.html#practice-q-join-employees-and-training-sessions",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.6 Practice Q: Join Employees and Training Sessions",
    "text": "21.6 Practice Q: Join Employees and Training Sessions\nConsider the two datasets below, one with employee details and the other with training session dates for these employees.\n\nemployees\n\n\n\n\n\n\n\n\nemployee_id\nname\nage\n\n\n\n\n0\n1\nJohn\n32\n\n\n1\n2\nJoy\n28\n\n\n2\n3\nKhan\n40\n\n\n\n\n\n\n\n\ntraining_sessions\n\n\n\n\n\n\n\n\nemployee_id\ntraining_date\n\n\n\n\n0\n1\n2023-01-20\n\n\n1\n2\n2023-02-20\n\n\n2\n3\n2023-05-15\n\n\n\n\n\n\n\nHow many rows and columns would you expect to have after joining these two datasets?\nNow join the two datasets and check your answer.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#practice-q-join-with-on-argument",
    "href": "p_untangled_joining_1.html#practice-q-join-with-on-argument",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.7 Practice Q: Join with on Argument",
    "text": "21.7 Practice Q: Join with on Argument\nTwo datasets are shown below, one with customer details and the other with order records for those customers.\n\ncustomer_details\n\n\n\n\n\n\n\n\nid_number\nfull_name\naddress\n\n\n\n\n0\nA001\nAlice\n123 Elm St\n\n\n1\nB002\nBob\n456 Maple Dr\n\n\n2\nC003\nCharlie\n789 Oak Blvd\n\n\n\n\n\n\n\n\norder_records\n\n\n\n\n\n\n\n\ncustomer_code\nproduct_type\norder_date\n\n\n\n\n0\nA001\nElectronics\n2022-05-10\n\n\n1\nB002\nBooks\n2023-09-01\n\n\n2\nC003\nClothing\n2021-12-15\n\n\n\n\n\n\n\nJoin the customer_details and order_records datasets. You will need to use the left_on and right_on arguments because the customer identifier columns have different names.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#types-of-joins",
    "href": "p_untangled_joining_1.html#types-of-joins",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.8 Types of joins",
    "text": "21.8 Types of joins\nThe toy examples so far have involved datasets that could be matched perfectly—every row in one dataset had a corresponding row in the other dataset.\nReal-world data is usually messier. Often, there will be entries in the first table that do not have corresponding entries in the second table, and vice versa.\nTo handle these cases of imperfect matching, there are different join types with specific behaviors: left, right, inner, and outer. In the upcoming sections, we’ll look at examples of how each join type operates on datasets with imperfect matches.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#left-join",
    "href": "p_untangled_joining_1.html#left-join",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.9 left join",
    "text": "21.9 left join\nLet’s start with the left join. To see how it handles unmatched rows, we will try to join our original people dataset with a modified version of the test_info dataset.\nAs a reminder, here is the people dataset, with Alice, Bob, and Charlie:\n\npeople\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n32\n\n\n2\nCharlie\n45\n\n\n\n\n\n\n\nFor test information, we’ll remove Charlie and we’ll add a new individual, Xavier, and his test data:\n\ntest_info_xavier\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nXavier\n2023-05-02\nNegative\n\n\n\n\n\n\n\nWe can specify the join type using the how argument:\n\npd.merge(people, test_info_xavier, how='left')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\nNaN\nNaN\n\n\n\n\n\n\n\nAs you can see, with the left join, all records from the left dataframe (people) are retained. So, even though Charlie doesn’t have a match in the test_info_xavier dataset, he’s still included in the output. (But of course, since his test information is not available in test_info_xavier, those values were left as NaN.)\nXavier, on the other hand, who was only present in the right dataset, gets dropped.\nThe graphic below shows how this join worked:\n\n\n\nLeft Join\n\n\nNow what if we flip the dataframes? Let’s see the outcome when test_info_xavier is the left dataframe and people is the right one:\n\npd.merge(test_info_xavier, people, on='name', how='left')\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\nage\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n25.0\n\n\n1\nBob\n2023-08-10\nPositive\n32.0\n\n\n2\nXavier\n2023-05-02\nNegative\nNaN\n\n\n\n\n\n\n\nOnce again, the left join retains all rows from the left dataframe (now test_info_xavier). This means Xavier’s data is included this time. Charlie, on the other hand, is excluded.\n\n\n\n\n\n\nKey Point\n\n\n\nPrimary Dataset: In the context of joins, the primary dataset refers to the main or prioritized dataset in an operation. In a left join, the left dataframe is considered the primary dataset because all of its rows are retained in the output, regardless of whether they have a matching row in the other dataframe.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n21.10 Practice Q: Left Join Students and Exam Dates\nConsider the two datasets below, one with student details and the other with exam dates for some of these students.\n\nstudents\n\n\n\n\n\n\n\n\nstudent_id\nname\nage\n\n\n\n\n0\n1\nAlice\n20\n\n\n1\n2\nBob\n22\n\n\n2\n3\nCharlie\n21\n\n\n\n\n\n\n\n\nexam_dates\n\n\n\n\n\n\n\n\nstudent_id\nexam_date\n\n\n\n\n0\n1\n2023-05-20\n\n\n1\n3\n2023-05-22\n\n\n\n\n\n\n\nJoin the students dataset with the exam_dates dataset using a left join.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#practice-q-left-join-students-and-exam-dates",
    "href": "p_untangled_joining_1.html#practice-q-left-join-students-and-exam-dates",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.10 Practice Q: Left Join Students and Exam Dates",
    "text": "21.10 Practice Q: Left Join Students and Exam Dates\nConsider the two datasets below, one with student details and the other with exam dates for some of these students.\n\nstudents\n\n\n\n\n\n\n\n\nstudent_id\nname\nage\n\n\n\n\n0\n1\nAlice\n20\n\n\n1\n2\nBob\n22\n\n\n2\n3\nCharlie\n21\n\n\n\n\n\n\n\n\nexam_dates\n\n\n\n\n\n\n\n\nstudent_id\nexam_date\n\n\n\n\n0\n1\n2023-05-20\n\n\n1\n3\n2023-05-22\n\n\n\n\n\n\n\nJoin the students dataset with the exam_dates dataset using a left join.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#analysing-african-tb-incidence-and-health-expenditure",
    "href": "p_untangled_joining_1.html#analysing-african-tb-incidence-and-health-expenditure",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.11 Analysing African TB Incidence and Health Expenditure",
    "text": "21.11 Analysing African TB Incidence and Health Expenditure\nLet’s try another example, this time with a more realistic set of data.\nFirst, we have data on the TB incidence rate per 100,000 people for some African countries, from the WHO:\n\ntb_2019_africa\n\n\n\n\n\n\n\n\ncountry\ncases\nconf_int_95\n\n\n\n\n0\nBurundi\n107\n[69 – 153]\n\n\n1\nSao Tome and Principe\n114\n[45 – 214]\n\n\n2\nSenegal\n117\n[83 – 156]\n\n\n3\nMauritius\n12\n[9 – 15]\n\n\n4\nCôte d’Ivoire\n137\n[88 – 197]\n\n\n5\nEthiopia\n140\n[98 – 188]\n\n\n6\nChad\n142\n[92 – 202]\n\n\n7\nGhana\n144\n[70 – 244]\n\n\n8\nMalawi\n146\n[78 – 235]\n\n\n9\nSeychelles\n15\n[13 – 18]\n\n\n10\nGambia\n158\n[117 – 204]\n\n\n11\nGuinea\n176\n[114 – 251]\n\n\n12\nCameroon\n179\n[116 – 255]\n\n\n13\nZimbabwe\n199\n[147 – 258]\n\n\n14\nUganda\n200\n[117 – 303]\n\n\n15\nNigeria\n219\n[143 – 311]\n\n\n16\nSouth Sudan\n227\n[147 – 324]\n\n\n17\nMadagascar\n233\n[151 – 333]\n\n\n18\nUnited Republic of Tanzania\n237\n[112 – 408]\n\n\n19\nBotswana\n253\n[195 – 317]\n\n\n20\nKenya\n267\n[163 – 396]\n\n\n21\nEquatorial Guinea\n286\n[185 – 408]\n\n\n22\nSierra Leone\n295\n[190 – 422]\n\n\n23\nLiberia\n308\n[199 – 440]\n\n\n24\nDemocratic Republic of the Congo\n320\n[207 – 457]\n\n\n25\nZambia\n333\n[216 – 474]\n\n\n26\nComoros\n35\n[23 – 50]\n\n\n27\nAngola\n351\n[227 – 501]\n\n\n28\nMozambique\n361\n[223 – 532]\n\n\n29\nGuinea-Bissau\n361\n[234 – 516]\n\n\n30\nEswatini\n363\n[228 – 527]\n\n\n31\nTogo\n37\n[30 – 45]\n\n\n32\nCongo\n373\n[237 – 541]\n\n\n33\nCabo Verde\n46\n[35 – 58]\n\n\n34\nBurkina Faso\n47\n[30 – 67]\n\n\n35\nNamibia\n486\n[348 – 647]\n\n\n36\nMali\n52\n[34 – 74]\n\n\n37\nGabon\n521\n[337 – 744]\n\n\n38\nCentral African Republic\n540\n[349 – 771]\n\n\n39\nBenin\n55\n[36 – 79]\n\n\n40\nRwanda\n57\n[44 – 72]\n\n\n41\nAlgeria\n61\n[46 – 77]\n\n\n42\nSouth Africa\n615\n[427 – 835]\n\n\n43\nLesotho\n654\n[406 – 959]\n\n\n44\nNiger\n84\n[54 – 120]\n\n\n45\nEritrea\n86\n[40 – 151]\n\n\n46\nMauritania\n89\n[58 – 127]\n\n\n\n\n\n\n\nWe want to analyze how TB incidence in African countries varies with government health expenditure per capita. For this, we have data on health expenditure per capita in USD, also from the WHO, for countries from all continents:\n\nhealth_exp_2019\n\n\n\n\n\n\n\n\ncountry\nexpend_usd\n\n\n\n\n0\nNigeria\n10.97\n\n\n1\nBahamas\n1002.00\n\n\n2\nUnited Arab Emirates\n1015.00\n\n\n3\nNauru\n1038.00\n\n\n4\nSlovakia\n1058.00\n\n\n...\n...\n...\n\n\n180\nMyanmar\n9.64\n\n\n181\nMalawi\n9.78\n\n\n182\nCuba\n901.80\n\n\n183\nTunisia\n97.75\n\n\n184\nNicaragua\n99.73\n\n\n\n\n185 rows × 2 columns\n\n\n\nWhich dataset should we use as the left dataframe for the join?\nSince our goal is to analyze African countries, we should use tb_2019_africa as the left dataframe. This will ensure we keep all the African countries in the final joined dataset.\nLet’s join them:\n\ntb_health_exp_joined = pd.merge(tb_2019_africa, health_exp_2019, on='country', how='left')\ntb_health_exp_joined\n\n\n\n\n\n\n\n\ncountry\ncases\nconf_int_95\nexpend_usd\n\n\n\n\n0\nBurundi\n107\n[69 – 153]\n6.07\n\n\n1\nSao Tome and Principe\n114\n[45 – 214]\n47.64\n\n\n2\nSenegal\n117\n[83 – 156]\n15.47\n\n\n3\nMauritius\n12\n[9 – 15]\nNaN\n\n\n4\nCôte d’Ivoire\n137\n[88 – 197]\n22.25\n\n\n5\nEthiopia\n140\n[98 – 188]\n5.93\n\n\n6\nChad\n142\n[92 – 202]\n4.76\n\n\n7\nGhana\n144\n[70 – 244]\n30.01\n\n\n8\nMalawi\n146\n[78 – 235]\n9.78\n\n\n9\nSeychelles\n15\n[13 – 18]\n572.00\n\n\n10\nGambia\n158\n[117 – 204]\n9.40\n\n\n11\nGuinea\n176\n[114 – 251]\n9.61\n\n\n12\nCameroon\n179\n[116 – 255]\n6.26\n\n\n13\nZimbabwe\n199\n[147 – 258]\n7.82\n\n\n14\nUganda\n200\n[117 – 303]\n5.05\n\n\n15\nNigeria\n219\n[143 – 311]\n10.97\n\n\n16\nSouth Sudan\n227\n[147 – 324]\nNaN\n\n\n17\nMadagascar\n233\n[151 – 333]\n6.26\n\n\n18\nUnited Republic of Tanzania\n237\n[112 – 408]\n16.02\n\n\n19\nBotswana\n253\n[195 – 317]\n292.10\n\n\n20\nKenya\n267\n[163 – 396]\n39.57\n\n\n21\nEquatorial Guinea\n286\n[185 – 408]\n47.30\n\n\n22\nSierra Leone\n295\n[190 – 422]\n6.28\n\n\n23\nLiberia\n308\n[199 – 440]\n8.38\n\n\n24\nDemocratic Republic of the Congo\n320\n[207 – 457]\n3.14\n\n\n25\nZambia\n333\n[216 – 474]\n27.09\n\n\n26\nComoros\n35\n[23 – 50]\nNaN\n\n\n27\nAngola\n351\n[227 – 501]\n28.59\n\n\n28\nMozambique\n361\n[223 – 532]\n9.35\n\n\n29\nGuinea-Bissau\n361\n[234 – 516]\n3.90\n\n\n30\nEswatini\n363\n[228 – 527]\n131.50\n\n\n31\nTogo\n37\n[30 – 45]\n7.56\n\n\n32\nCongo\n373\n[237 – 541]\n25.82\n\n\n33\nCabo Verde\n46\n[35 – 58]\n111.50\n\n\n34\nBurkina Faso\n47\n[30 – 67]\n17.17\n\n\n35\nNamibia\n486\n[348 – 647]\n204.30\n\n\n36\nMali\n52\n[34 – 74]\n11.03\n\n\n37\nGabon\n521\n[337 – 744]\n125.60\n\n\n38\nCentral African Republic\n540\n[349 – 771]\n3.58\n\n\n39\nBenin\n55\n[36 – 79]\n6.33\n\n\n40\nRwanda\n57\n[44 – 72]\n20.20\n\n\n41\nAlgeria\n61\n[46 – 77]\n163.00\n\n\n42\nSouth Africa\n615\n[427 – 835]\n321.70\n\n\n43\nLesotho\n654\n[406 – 959]\n53.02\n\n\n44\nNiger\n84\n[54 – 120]\n11.14\n\n\n45\nEritrea\n86\n[40 – 151]\n4.45\n\n\n46\nMauritania\n89\n[58 – 127]\n22.40\n\n\n\n\n\n\n\nNow in the joined dataset, we have just the African countries, which is exactly what we wanted.\nAll rows from the left dataframe tb_2019_africa were kept, while non-African countries from health_exp_2019 were discarded.\nWe can check if any rows in tb_2019_africa did not have a match in health_exp_2019 by filtering for NaN values:\n\ntb_health_exp_joined.query(\"expend_usd.isna()\")\n\n\n\n\n\n\n\n\ncountry\ncases\nconf_int_95\nexpend_usd\n\n\n\n\n3\nMauritius\n12\n[9 – 15]\nNaN\n\n\n16\nSouth Sudan\n227\n[147 – 324]\nNaN\n\n\n26\nComoros\n35\n[23 – 50]\nNaN\n\n\n\n\n\n\n\nThis shows that 3 countries—Mauritius, South Sudan, and Comoros—did not have expenditure data in health_exp_2019. But because they were present in tb_2019_africa, and that was the left dataframe, they were still included in the joined data.\n\n\n\n\n\n\nPractice\n\n\n\n21.12 Practice Q: Left Join TB Cases and Continents\nThe first, tb_cases_children, contains the number of TB cases in under 15s in 2012, by country:\n\ntb_cases_children\n\n\n\n\n\n\n\n\ncountry\ntb_cases_smear_0_14\n\n\n\n\n0\nAfghanistan\n588.0\n\n\n1\nAlbania\n0.0\n\n\n2\nAlgeria\n89.0\n\n\n4\nAndorra\n0.0\n\n\n5\nAngola\n982.0\n\n\n...\n...\n...\n\n\n211\nViet Nam\n142.0\n\n\n213\nWest Bank and Gaza Strip\n0.0\n\n\n214\nYemen\n105.0\n\n\n215\nZambia\n321.0\n\n\n216\nZimbabwe\n293.0\n\n\n\n\n200 rows × 2 columns\n\n\n\nAnd country_continents, lists all countries and their corresponding region and continent:\n\ncountry_continents\n\n\n\n\n\n\n\n\ncountry.name.en\ncontinent\nregion\n\n\n\n\n0\nAfghanistan\nAsia\nSouth Asia\n\n\n1\nAlbania\nEurope\nEurope & Central Asia\n\n\n2\nAlgeria\nAfrica\nMiddle East & North Africa\n\n\n3\nAmerican Samoa\nOceania\nEast Asia & Pacific\n\n\n4\nAndorra\nEurope\nEurope & Central Asia\n\n\n...\n...\n...\n...\n\n\n286\nYugoslavia\nNaN\nEurope & Central Asia\n\n\n287\nZambia\nAfrica\nSub-Saharan Africa\n\n\n288\nZanzibar\nNaN\nSub-Saharan Africa\n\n\n289\nZimbabwe\nAfrica\nSub-Saharan Africa\n\n\n290\nÅland Islands\nEurope\nEurope & Central Asia\n\n\n\n\n291 rows × 3 columns\n\n\n\nYour goal is to add the continent and region data to the TB cases dataset.\nWhich dataframe should be the left one? And which should be the right one? Once you’ve decided, join the datasets appropriately using a left join.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#practice-q-left-join-tb-cases-and-continents",
    "href": "p_untangled_joining_1.html#practice-q-left-join-tb-cases-and-continents",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.12 Practice Q: Left Join TB Cases and Continents",
    "text": "21.12 Practice Q: Left Join TB Cases and Continents\nThe first, tb_cases_children, contains the number of TB cases in under 15s in 2012, by country:\n\ntb_cases_children\n\n\n\n\n\n\n\n\ncountry\ntb_cases_smear_0_14\n\n\n\n\n0\nAfghanistan\n588.0\n\n\n1\nAlbania\n0.0\n\n\n2\nAlgeria\n89.0\n\n\n4\nAndorra\n0.0\n\n\n5\nAngola\n982.0\n\n\n...\n...\n...\n\n\n211\nViet Nam\n142.0\n\n\n213\nWest Bank and Gaza Strip\n0.0\n\n\n214\nYemen\n105.0\n\n\n215\nZambia\n321.0\n\n\n216\nZimbabwe\n293.0\n\n\n\n\n200 rows × 2 columns\n\n\n\nAnd country_continents, lists all countries and their corresponding region and continent:\n\ncountry_continents\n\n\n\n\n\n\n\n\ncountry.name.en\ncontinent\nregion\n\n\n\n\n0\nAfghanistan\nAsia\nSouth Asia\n\n\n1\nAlbania\nEurope\nEurope & Central Asia\n\n\n2\nAlgeria\nAfrica\nMiddle East & North Africa\n\n\n3\nAmerican Samoa\nOceania\nEast Asia & Pacific\n\n\n4\nAndorra\nEurope\nEurope & Central Asia\n\n\n...\n...\n...\n...\n\n\n286\nYugoslavia\nNaN\nEurope & Central Asia\n\n\n287\nZambia\nAfrica\nSub-Saharan Africa\n\n\n288\nZanzibar\nNaN\nSub-Saharan Africa\n\n\n289\nZimbabwe\nAfrica\nSub-Saharan Africa\n\n\n290\nÅland Islands\nEurope\nEurope & Central Asia\n\n\n\n\n291 rows × 3 columns\n\n\n\nYour goal is to add the continent and region data to the TB cases dataset.\nWhich dataframe should be the left one? And which should be the right one? Once you’ve decided, join the datasets appropriately using a left join.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#right-join",
    "href": "p_untangled_joining_1.html#right-join",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.13 right join",
    "text": "21.13 right join\nA right join can be thought of as a mirror image of a left join. The mechanics are the same, but now all rows from the right dataframe are retained, while only those rows from the left dataframe that find a match in the right are kept.\nLet’s look at an example to understand this. We’ll use our original people and modified test_info_xavier datasets:\n\npeople\ntest_info_xavier\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nXavier\n2023-05-02\nNegative\n\n\n\n\n\n\n\nNow let’s try a right join, with people as the right dataframe:\n\npd.merge(test_info_xavier, people, on='name', how='right')\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\nage\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n25\n\n\n1\nBob\n2023-08-10\nPositive\n32\n\n\n2\nCharlie\nNaN\nNaN\n45\n\n\n\n\n\n\n\nHopefully you’re getting the hang of this and could predict that output! Since people was the right dataframe, and we are using a right join, all the rows from people are kept—Alice, Bob, and Charlie—but only matching records from test_info_xavier.\nThe graphic below illustrates this process:\n\n\n\nRight Join\n\n\nAn important point—the same final dataframe can be created with either a left join or a right join; it just depends on what order you provide the dataframes to these functions:\n\n# Here, right join prioritizes the right dataframe, people\npd.merge(test_info_xavier, people, on='name', how='right')\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\nage\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n25\n\n\n1\nBob\n2023-08-10\nPositive\n32\n\n\n2\nCharlie\nNaN\nNaN\n45\n\n\n\n\n\n\n\n\n# Here, left join prioritizes the left dataframe, again people\npd.merge(people, test_info_xavier, on='name', how='left')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\nNaN\nNaN\n\n\n\n\n\n\n\nAs we previously mentioned, data scientists typically favor left joins over right joins. It makes more sense to specify your primary dataset first, in the left position. Opting for a left join is a common best practice due to its clearer logic, making it less error-prone.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#inner-join",
    "href": "p_untangled_joining_1.html#inner-join",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.14 inner join",
    "text": "21.14 inner join\nWhat makes an inner join distinct is that rows are only kept if the joining values are present in both dataframes. Let’s return to our example of individuals and their test results. As a reminder, here are our datasets:\n\npeople\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n32\n\n\n2\nCharlie\n45\n\n\n\n\n\n\n\n\ntest_info_xavier\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nXavier\n2023-05-02\nNegative\n\n\n\n\n\n\n\nNow that we have a better understanding of how joins work, we can already picture what the final dataframe would look like if we used an inner join on our two dataframes above. If only rows with joining values that are in both dataframes are kept, and the only individuals that are in both people and test_info_xavier are Alice and Bob, then they should be the only individuals in our final dataset! Let’s try it out.\n\npd.merge(people, test_info_xavier, on='name', how='inner')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n\n\n\n\n\nPerfect, that’s exactly what we expected! Here, Charlie was only in the people dataset, and Xavier was only in the test_info_xavier dataset, so both of them were removed. The graphic below shows how this join works:\n\n\n\nInner Join\n\n\nNote that the default join type is inner. So if you don’t specify how='inner', you’re actually performing an inner join! Try it out:\n\npd.merge(people, test_info_xavier)\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n21.15 Practice Q: Inner Join Products\nThe following data is on product sales and customer feedback in 2019.\n\ntotal_sales\n\n\n\n\n\n\n\n\nproduct\ntotal_units_sold\n\n\n\n\n0\nLaptop\n9751\n\n\n1\nDesktop\n136\n\n\n2\nTablet\n8285\n\n\n3\nSmartphone\n2478\n\n\n4\nSmartwatch\n3642\n\n\n5\nHeadphones\n5231\n\n\n6\nMonitor\n1892\n\n\n7\nKeyboard\n4267\n\n\n8\nMouse\n3891\n\n\n9\nPrinter\n982\n\n\n\n\n\n\n\n\nproduct_feedback\n\n\n\n\n\n\n\n\nproduct\nn_positive_reviews\nn_negative_reviews\n\n\n\n\n0\nLaptop\n1938\n42\n\n\n1\nDesktop\n128\n30\n\n\n2\nTablet\n842\n56\n\n\n3\nSmartphone\n1567\n89\n\n\n4\nSmartwatch\n723\n34\n\n\n5\nHeadphones\n956\n28\n\n\n6\nMonitor\n445\n15\n\n\n7\nGaming Console\n582\n11\n\n\n8\nCamera\n234\n8\n\n\n9\nSpeaker\n678\n25\n\n\n\n\n\n\n\nUse an inner join to combine the datasets.\nHow many products are there in common between the two datasets.\nWhich product has the highest ratio of positive reviews to units sold? (Should be desktops)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#practice-q-inner-join-products",
    "href": "p_untangled_joining_1.html#practice-q-inner-join-products",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.15 Practice Q: Inner Join Products",
    "text": "21.15 Practice Q: Inner Join Products\nThe following data is on product sales and customer feedback in 2019.\n\ntotal_sales\n\n\n\n\n\n\n\n\nproduct\ntotal_units_sold\n\n\n\n\n0\nLaptop\n9751\n\n\n1\nDesktop\n136\n\n\n2\nTablet\n8285\n\n\n3\nSmartphone\n2478\n\n\n4\nSmartwatch\n3642\n\n\n5\nHeadphones\n5231\n\n\n6\nMonitor\n1892\n\n\n7\nKeyboard\n4267\n\n\n8\nMouse\n3891\n\n\n9\nPrinter\n982\n\n\n\n\n\n\n\n\nproduct_feedback\n\n\n\n\n\n\n\n\nproduct\nn_positive_reviews\nn_negative_reviews\n\n\n\n\n0\nLaptop\n1938\n42\n\n\n1\nDesktop\n128\n30\n\n\n2\nTablet\n842\n56\n\n\n3\nSmartphone\n1567\n89\n\n\n4\nSmartwatch\n723\n34\n\n\n5\nHeadphones\n956\n28\n\n\n6\nMonitor\n445\n15\n\n\n7\nGaming Console\n582\n11\n\n\n8\nCamera\n234\n8\n\n\n9\nSpeaker\n678\n25\n\n\n\n\n\n\n\nUse an inner join to combine the datasets.\nHow many products are there in common between the two datasets.\nWhich product has the highest ratio of positive reviews to units sold? (Should be desktops)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#outer-join",
    "href": "p_untangled_joining_1.html#outer-join",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.16 outer join",
    "text": "21.16 outer join\nThe peculiarity of the outer join is that it retains all records, regardless of whether or not there is a match between the two datasets. Where there is missing information in our final dataset, cells are set to NaN just as we have seen in the left and right joins. Let’s take a look at our people and test_info_xavier datasets to illustrate this.\nHere is a reminder of our datasets:\n\npeople\ntest_info_xavier\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nXavier\n2023-05-02\nNegative\n\n\n\n\n\n\n\nNow let’s perform an outer join:\n\npd.merge(people, test_info_xavier, on='name', how='outer')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25.0\n2023-06-05\nNegative\n\n\n1\nBob\n32.0\n2023-08-10\nPositive\n\n\n2\nCharlie\n45.0\nNaN\nNaN\n\n\n3\nXavier\nNaN\n2023-05-02\nNegative\n\n\n\n\n\n\n\nAs we can see, all rows were kept so there was no loss in information! The graphic below illustrates this process:\n\n\n\nOuter Join\n\n\nJust as we saw above, all of the data from both of the original dataframes are still there, with any missing information set to NaN.\n\n\n\n\n\n\nPractice\n\n\n\n21.17 Practice Q: Join Sales Data\nThe following dataframes contain global sales and global customer complaints from various years.\n\nsales\n\n\n\n\n\n\n\n\nyear\nsales_count\n\n\n\n\n0\n2010\n69890\n\n\n1\n2011\n66507\n\n\n2\n2014\n59831\n\n\n3\n2016\n58704\n\n\n4\n2017\n59151\n\n\n\n\n\n\n\n\ncustomer_complaints\n\n\n\n\n\n\n\n\nyear\ncomplaints_count\n\n\n\n\n0\n2011\n1292\n\n\n1\n2013\n1100\n\n\n2\n2015\n1011\n\n\n3\n2016\n940\n\n\n4\n2019\n895\n\n\n\n\n\n\n\nJoin the above tables using the appropriate join to retain all information from the two datasets.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#practice-q-join-sales-data",
    "href": "p_untangled_joining_1.html#practice-q-join-sales-data",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.17 Practice Q: Join Sales Data",
    "text": "21.17 Practice Q: Join Sales Data\nThe following dataframes contain global sales and global customer complaints from various years.\n\nsales\n\n\n\n\n\n\n\n\nyear\nsales_count\n\n\n\n\n0\n2010\n69890\n\n\n1\n2011\n66507\n\n\n2\n2014\n59831\n\n\n3\n2016\n58704\n\n\n4\n2017\n59151\n\n\n\n\n\n\n\n\ncustomer_complaints\n\n\n\n\n\n\n\n\nyear\ncomplaints_count\n\n\n\n\n0\n2011\n1292\n\n\n1\n2013\n1100\n\n\n2\n2015\n1011\n\n\n3\n2016\n940\n\n\n4\n2019\n895\n\n\n\n\n\n\n\nJoin the above tables using the appropriate join to retain all information from the two datasets.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_1.html#wrap-up",
    "href": "p_untangled_joining_1.html#wrap-up",
    "title": "21  Introduction to Joining Datasets",
    "section": "21.18 Wrap Up!",
    "text": "21.18 Wrap Up!\nWay to go, you now understand the basics of joining! The Venn diagram below gives a helpful summary of the different joins and the information that each one retains. It may be helpful to save this image for future reference!\n\n\n\nJoin Types",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Introduction to Joining Datasets</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html",
    "href": "p_untangled_joining_2.html",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "",
    "text": "22.1 Packages\nimport pandas as pd\nimport country_converter as cc",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#data",
    "href": "p_untangled_joining_2.html#data",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.2 Data",
    "text": "22.2 Data\nRun the code below to load and define the datasets to be used in this lesson.\n\n# Load datasets\noil_consumption = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/oil_consumption.csv\"\n)\ntidyr_population = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/tidyr_population.csv\"\n)\ncountry_regions = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/country_continent_data.csv\"\n)\n\n\noil_2012 = (\n    oil_consumption[oil_consumption[\"year\"] == 2012].copy().drop(columns=[\"year\"])\n)\n\n# people data\npeople = pd.DataFrame({\"name\": [\"Alice\", \"Bob\", \"Charlie\"], \"age\": [25, 32, 45]})\n\ntest_info_many = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Alice\", \"Bob\", \"Bob\", \"Charlie\", \"Charlie\"],\n        \"test_date\": [\n            \"2023-06-05\",\n            \"2023-06-10\",\n            \"2023-08-10\",\n            \"2023-05-02\",\n            \"2023-05-12\",\n            \"2023-05-15\",\n        ],\n        \"result\": [\n            \"Negative\",\n            \"Positive\",\n            \"Positive\",\n            \"Negative\",\n            \"Negative\",\n            \"Negative\",\n        ],\n    }\n)\n\nfarm_info = pd.DataFrame(\n    {\n        \"farm_id\": [1, 2, 3],\n        \"farm_name\": [\"Green Acres\", \"Harvest Hill\", \"Golden Fields\"],\n        \"location\": [\"County A\", \"County B\", \"County A\"],\n    }\n)\n\ncrop_yields = pd.DataFrame(\n    {\n        \"farm_id\": [1, 1, 2, 3, 3],\n        \"crop\": [\"Wheat\", \"Corn\", \"Soybeans\", \"Wheat\", \"Barley\"],\n        \"yield_tons\": [50, 60, 45, 55, 30],\n    }\n)\n\ntraffic_flow = pd.DataFrame(\n    {\n        \"street_name\": [\n            \"Main St\",\n            \"Main St\",\n            \"Broadway\",\n            \"Broadway\",\n            \"Elm St\",\n            \"Elm St\",\n        ],\n        \"time_of_day\": [\"9am\", \"2pm\", \"9am\", \"2pm\", \"9am\", \"2pm\"],\n        \"vehicle_count\": [1200, 900, 1500, 1100, 700, 600],\n    }\n)\n\npollution_levels = pd.DataFrame(\n    {\n        \"street_name\": [\n            \"Main St\",\n            \"Main St\",\n            \"Broadway\",\n            \"Broadway\",\n            \"Elm St\",\n            \"Elm St\",\n        ],\n        \"time_of_day\": [\"9am\", \"2pm\", \"9am\", \"2pm\", \"9am\", \"2pm\"],\n        \"pm_2_5_level\": [35.5, 42.1, 40.3, 48.2, 25.7, 30.9],\n    }\n)\n\ntest_info_diff = pd.DataFrame(\n    {\n        \"name\": [\"alice\", \"Bob\", \"Charlie \"],\n        \"test_date\": [\"2023-06-05\", \"2023-08-10\", \"2023-05-02\"],\n        \"result\": [\"Negative\", \"Positive\", \"Negative\"],\n    }\n)\n\nasia_countries = pd.DataFrame(\n    {\n        \"Country\": [\"India\", \"Indonesia\", \"Philippines\"],\n        \"Capital\": [\"New Delhi\", \"Jakarta\", \"Manila\"],\n    }\n)\n\nasia_population = pd.DataFrame(\n    {\n        \"Country\": [\"India\", \"indonesia\", \"Philipines\"],\n        \"Population\": [1393000000, 273500000, 113000000],\n        \"Life_Expectancy\": [69.7, 71.7, 72.7],\n    }\n)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#introduction",
    "href": "p_untangled_joining_2.html#introduction",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.3 Introduction",
    "text": "22.3 Introduction\nNow that we have a solid grasp on the different types of joins and how they work, we can look at how to manage more complex joins and messier data.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#learning-objectives",
    "href": "p_untangled_joining_2.html#learning-objectives",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.4 Learning Objectives",
    "text": "22.4 Learning Objectives\n\nYou understand the concept of a one-to-many join\nYou know how to join on multiple key columns\nYou know how to check for mismatched values between dataframes",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#one-to-many-joins",
    "href": "p_untangled_joining_2.html#one-to-many-joins",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.5 One-to-many joins",
    "text": "22.5 One-to-many joins\nSo far, we have primarily looked at one-to-one joins, where an observation in one dataframe corresponded to only one observation in the other dataframe. In a one-to-many join, an observation in one dataframe corresponds to multiple observations in the other dataframe.\nTo illustrate a one-to-many join, let’s return to our patients and their COVID test data. Let’s imagine that in our dataset, Alice and Xavier got tested multiple times for COVID. We can add two more rows to our test_info dataframe with their new test information:\n\npeople\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n32\n\n\n2\nCharlie\n45\n\n\n\n\n\n\n\n\ntest_info_many\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nAlice\n2023-06-10\nPositive\n\n\n2\nBob\n2023-08-10\nPositive\n\n\n3\nBob\n2023-05-02\nNegative\n\n\n4\nCharlie\n2023-05-12\nNegative\n\n\n5\nCharlie\n2023-05-15\nNegative\n\n\n\n\n\n\n\nNext, let’s take a look at what happens when we use a merge() with people as the left dataframe:\n\npd.merge(people, test_info_many, on=\"name\", how=\"left\")\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nAlice\n25\n2023-06-10\nPositive\n\n\n2\nBob\n32\n2023-08-10\nPositive\n\n\n3\nBob\n32\n2023-05-02\nNegative\n\n\n4\nCharlie\n45\n2023-05-12\nNegative\n\n\n5\nCharlie\n45\n2023-05-15\nNegative\n\n\n\n\n\n\n\nWhat’s happened above? Basically, when you perform a one-to-many join, the data from the “one” side are duplicated for each matching row of the “many” side.\n\n\n\n\n\n\nPractice\n\n\n\n22.6 Practice Q: Merging One-to-Many Crop Yields\nRun the code below to print the two small dataframes:\n\nfarm_info\n\n\n\n\n\n\n\n\nfarm_id\nfarm_name\nlocation\n\n\n\n\n0\n1\nGreen Acres\nCounty A\n\n\n1\n2\nHarvest Hill\nCounty B\n\n\n2\n3\nGolden Fields\nCounty A\n\n\n\n\n\n\n\n\ncrop_yields\n\n\n\n\n\n\n\n\nfarm_id\ncrop\nyield_tons\n\n\n\n\n0\n1\nWheat\n50\n\n\n1\n1\nCorn\n60\n\n\n2\n2\nSoybeans\n45\n\n\n3\n3\nWheat\n55\n\n\n4\n3\nBarley\n30\n\n\n\n\n\n\n\nIf you use a merge() to join these datasets, how many rows will be in the final dataframe? Try to figure it out and then perform the join to see if you were right.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#practice-q-merging-one-to-many-crop-yields",
    "href": "p_untangled_joining_2.html#practice-q-merging-one-to-many-crop-yields",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.6 Practice Q: Merging One-to-Many Crop Yields",
    "text": "22.6 Practice Q: Merging One-to-Many Crop Yields\nRun the code below to print the two small dataframes:\n\nfarm_info\n\n\n\n\n\n\n\n\nfarm_id\nfarm_name\nlocation\n\n\n\n\n0\n1\nGreen Acres\nCounty A\n\n\n1\n2\nHarvest Hill\nCounty B\n\n\n2\n3\nGolden Fields\nCounty A\n\n\n\n\n\n\n\n\ncrop_yields\n\n\n\n\n\n\n\n\nfarm_id\ncrop\nyield_tons\n\n\n\n\n0\n1\nWheat\n50\n\n\n1\n1\nCorn\n60\n\n\n2\n2\nSoybeans\n45\n\n\n3\n3\nWheat\n55\n\n\n4\n3\nBarley\n30\n\n\n\n\n\n\n\nIf you use a merge() to join these datasets, how many rows will be in the final dataframe? Try to figure it out and then perform the join to see if you were right.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#multiple-key-columns",
    "href": "p_untangled_joining_2.html#multiple-key-columns",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.7 Multiple Key Columns",
    "text": "22.7 Multiple Key Columns\nSometimes we have more than one column that uniquely identifies the observations that we want to match on. For example, let’s imagine we have traffic flow data for three streets at two different times of day: 9am and 2pm.\n\ntraffic_flow\n\n\n\n\n\n\n\n\nstreet_name\ntime_of_day\nvehicle_count\n\n\n\n\n0\nMain St\n9am\n1200\n\n\n1\nMain St\n2pm\n900\n\n\n2\nBroadway\n9am\n1500\n\n\n3\nBroadway\n2pm\n1100\n\n\n4\nElm St\n9am\n700\n\n\n5\nElm St\n2pm\n600\n\n\n\n\n\n\n\nNow, let’s imagine we have another dataset for the same three streets, recording air pollution levels (measured in particulate matter, PM2.5) during the same times of day.\n\npollution_levels\n\n\n\n\n\n\n\n\nstreet_name\ntime_of_day\npm_2_5_level\n\n\n\n\n0\nMain St\n9am\n35.5\n\n\n1\nMain St\n2pm\n42.1\n\n\n2\nBroadway\n9am\n40.3\n\n\n3\nBroadway\n2pm\n48.2\n\n\n4\nElm St\n9am\n25.7\n\n\n5\nElm St\n2pm\n30.9\n\n\n\n\n\n\n\nWe want to join the two datasets so that each street has two rows: one for the 9am time point and one for the 2pm time point. To do this, our first instinct may be to join the datasets only on street_name. Let’s try it out and see what happens:\n\npd.merge(traffic_flow, pollution_levels, on=\"street_name\", how=\"left\")\n\n\n\n\n\n\n\n\nstreet_name\ntime_of_day_x\nvehicle_count\ntime_of_day_y\npm_2_5_level\n\n\n\n\n0\nMain St\n9am\n1200\n9am\n35.5\n\n\n1\nMain St\n9am\n1200\n2pm\n42.1\n\n\n2\nMain St\n2pm\n900\n9am\n35.5\n\n\n3\nMain St\n2pm\n900\n2pm\n42.1\n\n\n4\nBroadway\n9am\n1500\n9am\n40.3\n\n\n5\nBroadway\n9am\n1500\n2pm\n48.2\n\n\n6\nBroadway\n2pm\n1100\n9am\n40.3\n\n\n7\nBroadway\n2pm\n1100\n2pm\n48.2\n\n\n8\nElm St\n9am\n700\n9am\n25.7\n\n\n9\nElm St\n9am\n700\n2pm\n30.9\n\n\n10\nElm St\n2pm\n600\n9am\n25.7\n\n\n11\nElm St\n2pm\n600\n2pm\n30.9\n\n\n\n\n\n\n\nAs we can see, this isn’t what we wanted at all! We end up with duplicated rows—now we have four rows for each street.\nWhat we want to do is match on BOTH street_name AND time_of_day. To do this, we need to tell Python to match on two columns by specifying both column names in a list.\n\npd.merge(traffic_flow, pollution_levels, on=[\"street_name\", \"time_of_day\"])\n\n\n\n\n\n\n\n\nstreet_name\ntime_of_day\nvehicle_count\npm_2_5_level\n\n\n\n\n0\nMain St\n9am\n1200\n35.5\n\n\n1\nMain St\n2pm\n900\n42.1\n\n\n2\nBroadway\n9am\n1500\n40.3\n\n\n3\nBroadway\n2pm\n1100\n48.2\n\n\n4\nElm St\n9am\n700\n25.7\n\n\n5\nElm St\n2pm\n600\n30.9\n\n\n\n\n\n\n\nNow we have the correct number of rows! We can directly see the vehicle count and PM2.5 level for each street at each time of day.\n\n\n\n\n\n\nPractice\n\n\n\n22.8 Practice Q: Calculate Oil Consumption per Capita\nWe have two datasets containing information about countries:\n\noil_consumption: Contains yearly oil consumption in tonnes\ntidyr_population: Contains yearly population data\n\n\n# View the datasets\noil_consumption.sort_values(by=[\"country\", \"year\"])\n\n\n\n\n\n\n\n\ncountry\nyear\noil_consump\n\n\n\n\n19\nAlgeria\n1995\n8430000\n\n\n98\nAlgeria\n1996\n8060000\n\n\n177\nAlgeria\n1997\n7990000\n\n\n256\nAlgeria\n1998\n8220000\n\n\n335\nAlgeria\n1999\n8110000\n\n\n...\n...\n...\n...\n\n\n1183\nVietnam\n2009\n14200000\n\n\n1262\nVietnam\n2010\n15300000\n\n\n1341\nVietnam\n2011\n16700000\n\n\n1420\nVietnam\n2012\n17000000\n\n\n1499\nVietnam\n2013\n18200000\n\n\n\n\n1501 rows × 3 columns\n\n\n\n\ntidyr_population.sort_values(by=[\"country\", \"year\"])\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4040\nZimbabwe\n2009\n12888918\n\n\n4041\nZimbabwe\n2010\n13076978\n\n\n4042\nZimbabwe\n2011\n13358738\n\n\n4043\nZimbabwe\n2012\n13724317\n\n\n4044\nZimbabwe\n2013\n14149648\n\n\n\n\n4045 rows × 3 columns\n\n\n\n\nJoin these datasets using merge() with a left join. Since we want to match both country AND year, you’ll need to join on multiple columns. (You may notice that not all rows are matched. You can ignore this for now.)\nAfter joining, create a new column called consumption_per_capita that calculates the yearly oil consumption per person (in tonnes).\nWhich country had the highest per capita oil consumption in 1995?",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#practice-q-calculate-oil-consumption-per-capita",
    "href": "p_untangled_joining_2.html#practice-q-calculate-oil-consumption-per-capita",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.8 Practice Q: Calculate Oil Consumption per Capita",
    "text": "22.8 Practice Q: Calculate Oil Consumption per Capita\nWe have two datasets containing information about countries:\n\noil_consumption: Contains yearly oil consumption in tonnes\ntidyr_population: Contains yearly population data\n\n\n# View the datasets\noil_consumption.sort_values(by=[\"country\", \"year\"])\n\n\n\n\n\n\n\n\ncountry\nyear\noil_consump\n\n\n\n\n19\nAlgeria\n1995\n8430000\n\n\n98\nAlgeria\n1996\n8060000\n\n\n177\nAlgeria\n1997\n7990000\n\n\n256\nAlgeria\n1998\n8220000\n\n\n335\nAlgeria\n1999\n8110000\n\n\n...\n...\n...\n...\n\n\n1183\nVietnam\n2009\n14200000\n\n\n1262\nVietnam\n2010\n15300000\n\n\n1341\nVietnam\n2011\n16700000\n\n\n1420\nVietnam\n2012\n17000000\n\n\n1499\nVietnam\n2013\n18200000\n\n\n\n\n1501 rows × 3 columns\n\n\n\n\ntidyr_population.sort_values(by=[\"country\", \"year\"])\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4040\nZimbabwe\n2009\n12888918\n\n\n4041\nZimbabwe\n2010\n13076978\n\n\n4042\nZimbabwe\n2011\n13358738\n\n\n4043\nZimbabwe\n2012\n13724317\n\n\n4044\nZimbabwe\n2013\n14149648\n\n\n\n\n4045 rows × 3 columns\n\n\n\n\nJoin these datasets using merge() with a left join. Since we want to match both country AND year, you’ll need to join on multiple columns. (You may notice that not all rows are matched. You can ignore this for now.)\nAfter joining, create a new column called consumption_per_capita that calculates the yearly oil consumption per person (in tonnes).\nWhich country had the highest per capita oil consumption in 1995?",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#key-mismatches",
    "href": "p_untangled_joining_2.html#key-mismatches",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.9 Key Mismatches",
    "text": "22.9 Key Mismatches\nOften you will need to pre-clean your data when you draw it from different sources before you’re able to join it. This is because there can be inconsistencies in ways that values are recorded.\nTo illustrate this, let’s return to our mock patient data from the first lesson. If you recall, we had two dataframes, one called people and the other called test_info. We can recreate these datasets but change Alice to alice in the test_info_diff dataset and keep all other values the same.\n\npeople\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n32\n\n\n2\nCharlie\n45\n\n\n\n\n\n\n\n\ntest_info_diff\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nalice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n2023-05-02\nNegative\n\n\n\n\n\n\n\nNow let’s try a merge() on our two datasets.\n\npeople.merge(test_info_diff, on='name', how='left')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\nNaN\nNaN\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\nNaN\nNaN\n\n\n\n\n\n\n\n\npd.merge(people, test_info_diff, on=\"name\", how=\"inner\")\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nBob\n32\n2023-08-10\nPositive\n\n\n\n\n\n\n\nAs we can see, Python didn’t recognize Alice and alice as the same person, and it also could not match Charlie and Charlie! So we lose Alice and Charlie in the left join, and they are dropped in the inner join.\nHow can we fix this? We need to ensure that the names in both datasets are in the same format. For this, we can use str.title() to capitalize the first letter of each name.\n\ntest_info_diff['name'] = test_info_diff['name'].str.title()\ntest_info_diff\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n2023-05-02\nNegative\n\n\n\n\n\n\n\n\npeople.merge(test_info_diff, on='name', how='inner')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n\n\n\n\n\nHmm, Charlie is still not matched. It’s hard to see from the printout, but the string Charlie in test_info_diff has an extra space at the end.\nWe can spot this better by using .unique() to convert to an array:\n\ntest_info_diff['name'].unique()\n\narray(['Alice', 'Bob', 'Charlie '], dtype=object)\n\n\nWe can fix this by using str.strip() to remove the extra space.\n\ntest_info_diff['name'] = test_info_diff['name'].str.strip()\ntest_info_diff\n\n\n\n\n\n\n\n\nname\ntest_date\nresult\n\n\n\n\n0\nAlice\n2023-06-05\nNegative\n\n\n1\nBob\n2023-08-10\nPositive\n\n\n2\nCharlie\n2023-05-02\nNegative\n\n\n\n\n\n\n\nNow we can join the two datasets:\n\npeople.merge(test_info_diff, on='name', how='inner')\n\n\n\n\n\n\n\n\nname\nage\ntest_date\nresult\n\n\n\n\n0\nAlice\n25\n2023-06-05\nNegative\n\n\n1\nBob\n32\n2023-08-10\nPositive\n\n\n2\nCharlie\n45\n2023-05-02\nNegative\n\n\n\n\n\n\n\nPerfect!\n\n\n\n\n\n\nPractice\n\n\n\n22.10 Practice Q: Inner Join Countries\nThe following two datasets contain data for India, Indonesia, and the Philippines. However, an inner join of these datasets only returns 1 row.\n\nasia_countries\n\n\n\n\n\n\n\n\nCountry\nCapital\n\n\n\n\n0\nIndia\nNew Delhi\n\n\n1\nIndonesia\nJakarta\n\n\n2\nPhilippines\nManila\n\n\n\n\n\n\n\n\nasia_population\n\n\n\n\n\n\n\n\nCountry\nPopulation\nLife_Expectancy\n\n\n\n\n0\nIndia\n1393000000\n69.7\n\n\n1\nindonesia\n273500000\n71.7\n\n\n2\nPhilipines\n113000000\n72.7\n\n\n\n\n\n\n\n\npd.merge(asia_countries, asia_population)\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLife_Expectancy\n\n\n\n\n0\nIndia\nNew Delhi\n1393000000\n69.7\n\n\n\n\n\n\n\nWhat are the differences between the values in the key columns that would have to be changed before joining the datasets? Pay attention to capitalization and spelling.\nNow, fix the mismatched values in the Country column and try the join again.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#practice-q-inner-join-countries",
    "href": "p_untangled_joining_2.html#practice-q-inner-join-countries",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.10 Practice Q: Inner Join Countries",
    "text": "22.10 Practice Q: Inner Join Countries\nThe following two datasets contain data for India, Indonesia, and the Philippines. However, an inner join of these datasets only returns 1 row.\n\nasia_countries\n\n\n\n\n\n\n\n\nCountry\nCapital\n\n\n\n\n0\nIndia\nNew Delhi\n\n\n1\nIndonesia\nJakarta\n\n\n2\nPhilippines\nManila\n\n\n\n\n\n\n\n\nasia_population\n\n\n\n\n\n\n\n\nCountry\nPopulation\nLife_Expectancy\n\n\n\n\n0\nIndia\n1393000000\n69.7\n\n\n1\nindonesia\n273500000\n71.7\n\n\n2\nPhilipines\n113000000\n72.7\n\n\n\n\n\n\n\n\npd.merge(asia_countries, asia_population)\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLife_Expectancy\n\n\n\n\n0\nIndia\nNew Delhi\n1393000000\n69.7\n\n\n\n\n\n\n\nWhat are the differences between the values in the key columns that would have to be changed before joining the datasets? Pay attention to capitalization and spelling.\nNow, fix the mismatched values in the Country column and try the join again.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#key-mismatches-oil-consumption-example",
    "href": "p_untangled_joining_2.html#key-mismatches-oil-consumption-example",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.11 Key Mismatches: Oil Consumption Example",
    "text": "22.11 Key Mismatches: Oil Consumption Example\nLet’s now see a more realistic example of how mismatched keys can cause problems.\n\noil_consumption\n\n\n\n\n\n\n\n\ncountry\nyear\noil_consump\n\n\n\n\n0\nUnited Arab Emirates\n1995\n20800000\n\n\n1\nArgentina\n1995\n20300000\n\n\n2\nAustralia\n1995\n36500000\n\n\n3\nAustria\n1995\n11300000\n\n\n4\nAzerbaijan\n1995\n6580000\n\n\n...\n...\n...\n...\n\n\n1496\nUSA\n2013\n791000000\n\n\n1497\nUzbekistan\n2013\n2860000\n\n\n1498\nVenezuela\n2013\n36800000\n\n\n1499\nVietnam\n2013\n18200000\n\n\n1500\nSouth Africa\n2013\n26700000\n\n\n\n\n1501 rows × 3 columns\n\n\n\n\ntidyr_population\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4040\nZimbabwe\n2009\n12888918\n\n\n4041\nZimbabwe\n2010\n13076978\n\n\n4042\nZimbabwe\n2011\n13358738\n\n\n4043\nZimbabwe\n2012\n13724317\n\n\n4044\nZimbabwe\n2013\n14149648\n\n\n\n\n4045 rows × 3 columns\n\n\n\nAfter we attempt a join, we see that there are some countries that are not matched, such as Vietnam.\n\npd.merge(\n    oil_consumption, tidyr_population, on=[\"country\", \"year\"], how=\"left\"\n).sort_values([\"country\", \"year\"])\n\n\n\n\n\n\n\n\ncountry\nyear\noil_consump\npopulation\n\n\n\n\n19\nAlgeria\n1995\n8430000\n29315463.0\n\n\n98\nAlgeria\n1996\n8060000\n29845208.0\n\n\n177\nAlgeria\n1997\n7990000\n30345466.0\n\n\n256\nAlgeria\n1998\n8220000\n30820435.0\n\n\n335\nAlgeria\n1999\n8110000\n31276295.0\n\n\n...\n...\n...\n...\n...\n\n\n1183\nVietnam\n2009\n14200000\nNaN\n\n\n1262\nVietnam\n2010\n15300000\nNaN\n\n\n1341\nVietnam\n2011\n16700000\nNaN\n\n\n1420\nVietnam\n2012\n17000000\nNaN\n\n\n1499\nVietnam\n2013\n18200000\nNaN\n\n\n\n\n1501 rows × 4 columns\n\n\n\nThis is because the country names are not in the same format in the two datasets.\nBefore attempting to join these datasets, it’s a good idea to check for mismatches in the key columns. This can help you identify any discrepancies that might prevent a successful join.\nFirst, let’s identify the unique country names in both datasets.\n\noil_countries = set(oil_consumption['country'].unique())\npop_countries = set(tidyr_population['country'].unique())\n\nNow, to find countries in oil_consumption that are not in tidyr_population, we can use set arithmetic:\n\nmissing_in_pop = oil_countries - pop_countries\nmissing_in_pop\n\n{'Hong Kong, China',\n 'Iran',\n 'North Macedonia',\n 'Russia',\n 'Slovak Republic',\n 'South Korea',\n 'Taiwan',\n 'UK',\n 'USA',\n 'Venezuela',\n 'Vietnam'}\n\n\nAnd countries in tidyr_population that are not in oil_consumption:\n\nmissing_in_oil = pop_countries - oil_countries\nmissing_in_oil\n\n{'Afghanistan',\n 'Albania',\n 'American Samoa',\n 'Andorra',\n 'Angola',\n 'Anguilla',\n 'Antigua and Barbuda',\n 'Armenia',\n 'Aruba',\n 'Bahamas',\n 'Bahrain',\n 'Barbados',\n 'Belize',\n 'Benin',\n 'Bermuda',\n 'Bhutan',\n 'Bolivia (Plurinational State of)',\n 'Bonaire, Saint Eustatius and Saba',\n 'Bosnia and Herzegovina',\n 'Botswana',\n 'British Virgin Islands',\n 'Brunei Darussalam',\n 'Burkina Faso',\n 'Burundi',\n 'Cabo Verde',\n 'Cambodia',\n 'Cameroon',\n 'Cayman Islands',\n 'Central African Republic',\n 'Chad',\n 'China, Hong Kong SAR',\n 'China, Macao SAR',\n 'Comoros',\n 'Congo',\n 'Cook Islands',\n 'Costa Rica',\n 'Cuba',\n 'Curaçao',\n \"Côte d'Ivoire\",\n \"Democratic People's Republic of Korea\",\n 'Democratic Republic of the Congo',\n 'Djibouti',\n 'Dominica',\n 'Dominican Republic',\n 'El Salvador',\n 'Equatorial Guinea',\n 'Eritrea',\n 'Ethiopia',\n 'Fiji',\n 'French Polynesia',\n 'Gabon',\n 'Gambia',\n 'Georgia',\n 'Ghana',\n 'Greenland',\n 'Grenada',\n 'Guam',\n 'Guatemala',\n 'Guinea',\n 'Guinea-Bissau',\n 'Guyana',\n 'Haiti',\n 'Honduras',\n 'Iran (Islamic Republic of)',\n 'Jamaica',\n 'Jordan',\n 'Kenya',\n 'Kiribati',\n 'Kyrgyzstan',\n \"Lao People's Democratic Republic\",\n 'Lebanon',\n 'Lesotho',\n 'Liberia',\n 'Libya',\n 'Madagascar',\n 'Malawi',\n 'Maldives',\n 'Mali',\n 'Malta',\n 'Marshall Islands',\n 'Mauritania',\n 'Mauritius',\n 'Micronesia (Federated States of)',\n 'Monaco',\n 'Mongolia',\n 'Montenegro',\n 'Montserrat',\n 'Mozambique',\n 'Myanmar',\n 'Namibia',\n 'Nauru',\n 'Nepal',\n 'New Caledonia',\n 'Nicaragua',\n 'Niger',\n 'Nigeria',\n 'Niue',\n 'Northern Mariana Islands',\n 'Palau',\n 'Panama',\n 'Papua New Guinea',\n 'Paraguay',\n 'Puerto Rico',\n 'Republic of Korea',\n 'Republic of Moldova',\n 'Russian Federation',\n 'Rwanda',\n 'Saint Kitts and Nevis',\n 'Saint Lucia',\n 'Saint Vincent and the Grenadines',\n 'Samoa',\n 'San Marino',\n 'Sao Tome and Principe',\n 'Senegal',\n 'Serbia',\n 'Seychelles',\n 'Sierra Leone',\n 'Sint Maarten (Dutch part)',\n 'Slovakia',\n 'Solomon Islands',\n 'Somalia',\n 'South Sudan',\n 'Sudan',\n 'Suriname',\n 'Swaziland',\n 'Syrian Arab Republic',\n 'Tajikistan',\n 'The Former Yugoslav Republic of Macedonia',\n 'Timor-Leste',\n 'Togo',\n 'Tokelau',\n 'Tonga',\n 'Tunisia',\n 'Turks and Caicos Islands',\n 'Tuvalu',\n 'US Virgin Islands',\n 'Uganda',\n 'United Kingdom of Great Britain and Northern Ireland',\n 'United Republic of Tanzania',\n 'United States of America',\n 'Uruguay',\n 'Vanuatu',\n 'Venezuela (Bolivarian Republic of)',\n 'Viet Nam',\n 'Wallis and Futuna Islands',\n 'West Bank and Gaza Strip',\n 'Yemen',\n 'Zambia',\n 'Zimbabwe'}\n\n\nThese differences indicate mismatches in the key columns that need to be addressed before joining.\nYou might try to check manually. For example, we can see that Vietname is written as Vietnam in one dataset and Viet Nam in the other.\nHowever, in the case of countries, there is an even nicer solution: use country codes! We’ll see how to do this in the next section.\n\n\n\n\n\n\nSide Note\n\n\n\n22.12 Set Arithmetic\nA quick side note on set arithmetic for those who are unfamiliar.\nConsider two sets of the numbers 1:5, and 2:4.\n\nset_1 = set([1, 2, 3, 4, 5])\nset_2 = set([2, 3, 4])\n\nWe can check the values in set_1 that are not in set_2 by using set arithmetic:\n\nset_1 - set_2\n\n{1, 5}\n\n\nAnd the values in set_2 that are not in set_1 by using:\n\nset_2 - set_1\n\nset()\n\n\n\n\n\n22.12.1 Merging with Country Codes\nTo avoid country mismatches, it is often useful to use country codes rather than country names as the key.\nLet’s now add country codes to both datasets and try the join again.\n\n# How to use country_converter\ncc.convert(\"Nigeria\", to='ISO3')\n\n'NGA'\n\n\n\noil_consumption['country_code'] = cc.convert(oil_consumption['country'], to='ISO3')\ntidyr_population['country_code'] = cc.convert(tidyr_population['country'], to='ISO3')\n\n\noil_pop_code = oil_consumption.merge(tidyr_population, on=['country_code', 'year'], how='left')\n\n\n\n22.12.2 Identifying Remaining Mismatches\nLet’s see which countries still failed to find a match:\n\nset(oil_pop_code['country_code'].unique()) - set(tidyr_population['country_code'].unique())\n\n{'TWN'}\n\n\nIt seems ‘TWN’ (Taiwan) failed to find a match. We can manually look through the tidyr_population dataset to see if we can find it.\n\ntidyr_population.query(\"country.str.contains('Taiwan')\")\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\ncountry_code\n\n\n\n\n\n\n\n\n\nJust in case there is a mismatch in capitalization, we can also check for ‘taiwan’:\n\ntidyr_population.query(\"country.str.contains('taiwan')\")\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\ncountry_code\n\n\n\n\n\n\n\n\n\nAnd we can check for ‘China’ since there is currently conflict over whether Taiwan is part of China.\n\ntidyr_population.query(\"country.str.contains('China')\")\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\ncountry_code\n\n\n\n\n783\nChina\n1995\n1237531429\nCHN\n\n\n784\nChina\n1996\n1247897092\nCHN\n\n\n785\nChina\n1997\n1257021784\nCHN\n\n\n786\nChina\n1998\n1265222536\nCHN\n\n\n787\nChina\n1999\n1272915272\nCHN\n\n\n788\nChina\n2000\n1280428583\nCHN\n\n\n789\nChina\n2001\n1287890449\nCHN\n\n\n790\nChina\n2002\n1295322020\nCHN\n\n\n791\nChina\n2003\n1302810258\nCHN\n\n\n792\nChina\n2004\n1310414386\nCHN\n\n\n793\nChina\n2005\n1318176835\nCHN\n\n\n794\nChina\n2006\n1326146433\nCHN\n\n\n795\nChina\n2007\n1334343509\nCHN\n\n\n796\nChina\n2008\n1342732604\nCHN\n\n\n797\nChina\n2009\n1351247555\nCHN\n\n\n798\nChina\n2010\n1359821465\nCHN\n\n\n799\nChina\n2011\n1368440300\nCHN\n\n\n800\nChina\n2012\n1377064907\nCHN\n\n\n801\nChina\n2013\n1385566537\nCHN\n\n\n802\nChina, Hong Kong SAR\n1995\n6144498\nHKG\n\n\n803\nChina, Hong Kong SAR\n1996\n6275363\nHKG\n\n\n804\nChina, Hong Kong SAR\n1997\n6430651\nHKG\n\n\n805\nChina, Hong Kong SAR\n1998\n6591717\nHKG\n\n\n806\nChina, Hong Kong SAR\n1999\n6732627\nHKG\n\n\n807\nChina, Hong Kong SAR\n2000\n6835301\nHKG\n\n\n808\nChina, Hong Kong SAR\n2001\n6892752\nHKG\n\n\n809\nChina, Hong Kong SAR\n2002\n6912079\nHKG\n\n\n810\nChina, Hong Kong SAR\n2003\n6906631\nHKG\n\n\n811\nChina, Hong Kong SAR\n2004\n6896523\nHKG\n\n\n812\nChina, Hong Kong SAR\n2005\n6896686\nHKG\n\n\n813\nChina, Hong Kong SAR\n2006\n6910671\nHKG\n\n\n814\nChina, Hong Kong SAR\n2007\n6934748\nHKG\n\n\n815\nChina, Hong Kong SAR\n2008\n6967866\nHKG\n\n\n816\nChina, Hong Kong SAR\n2009\n7006930\nHKG\n\n\n817\nChina, Hong Kong SAR\n2010\n7049514\nHKG\n\n\n818\nChina, Hong Kong SAR\n2011\n7096359\nHKG\n\n\n819\nChina, Hong Kong SAR\n2012\n7148493\nHKG\n\n\n820\nChina, Hong Kong SAR\n2013\n7203836\nHKG\n\n\n821\nChina, Macao SAR\n1995\n398459\nMAC\n\n\n822\nChina, Macao SAR\n1996\n405231\nMAC\n\n\n823\nChina, Macao SAR\n1997\n412031\nMAC\n\n\n824\nChina, Macao SAR\n1998\n418810\nMAC\n\n\n825\nChina, Macao SAR\n1999\n425448\nMAC\n\n\n826\nChina, Macao SAR\n2000\n431907\nMAC\n\n\n827\nChina, Macao SAR\n2001\n438080\nMAC\n\n\n828\nChina, Macao SAR\n2002\n444150\nMAC\n\n\n829\nChina, Macao SAR\n2003\n450711\nMAC\n\n\n830\nChina, Macao SAR\n2004\n458542\nMAC\n\n\n831\nChina, Macao SAR\n2005\n468149\nMAC\n\n\n832\nChina, Macao SAR\n2006\n479808\nMAC\n\n\n833\nChina, Macao SAR\n2007\n493206\nMAC\n\n\n834\nChina, Macao SAR\n2008\n507528\nMAC\n\n\n835\nChina, Macao SAR\n2009\n521617\nMAC\n\n\n836\nChina, Macao SAR\n2010\n534626\nMAC\n\n\n837\nChina, Macao SAR\n2011\n546278\nMAC\n\n\n838\nChina, Macao SAR\n2012\n556783\nMAC\n\n\n839\nChina, Macao SAR\n2013\n566375\nMAC\n\n\n\n\n\n\n\nIt seems that Taiwan is not in the tidyr_population dataset.\nIn such a case, you might then try to find a dataset containing population data for Taiwan and add it to the tidyr_population dataset. But we’ll leave this for you to figure out.\n\n\n\n\n\n\nPractice\n\n\n\n22.13 Practice Q: Merging Oil Consumption with Geographic Data\nRun the code to view the two datasets.\nThe first, oil_2012, records the oil consumption for the year 2012:\n\noil_2012\n\n\n\n\n\n\n\n\ncountry\noil_consump\n\n\n\n\n1343\nUnited Arab Emirates\n35200000\n\n\n1344\nArgentina\n28600000\n\n\n1345\nAustralia\n46100000\n\n\n1346\nAustria\n11900000\n\n\n1347\nAzerbaijan\n4170000\n\n\n...\n...\n...\n\n\n1417\nUSA\n778000000\n\n\n1418\nUzbekistan\n3030000\n\n\n1419\nVenezuela\n37200000\n\n\n1420\nVietnam\n17000000\n\n\n1421\nSouth Africa\n26300000\n\n\n\n\n79 rows × 2 columns\n\n\n\nAnd country_regions lists countries along with their respective regions and continents:\n\ncountry_regions\n\n\n\n\n\n\n\n\ncountry_name\ncountry_code\ncontinent\n\n\n\n\n0\nAfghanistan\nAFG\nAsia\n\n\n1\nAlbania\nALB\nEurope\n\n\n2\nAlgeria\nDZA\nAfrica\n\n\n3\nAmerican Samoa\nASM\nOceania\n\n\n4\nAndorra\nAND\nEurope\n\n\n...\n...\n...\n...\n\n\n237\nWestern Sahara\nESH\nAfrica\n\n\n238\nYemen\nYEM\nAsia\n\n\n239\nZambia\nZMB\nAfrica\n\n\n240\nZimbabwe\nZWE\nAfrica\n\n\n241\nÅland Islands\nALA\nEurope\n\n\n\n\n242 rows × 3 columns\n\n\n\nJoin the two datasets using the country codes as the key. Then find the countries with the highest oil consumption in each continent. As a sanity check, your answer should include the US & China.\n\noil_2012['country_code'] = cc.convert(oil_2012['country'], to='ISO3')\n\n\noil_2012_regions = oil_2012.merge(country_regions, on='country_code', how='left')\n\nmax_oil_by_continent = oil_2012_regions.loc[\n    oil_2012_regions.groupby('continent')['oil_consump'].idxmax()\n]\n\nmax_oil_by_continent[['country', 'continent', 'oil_consump']]\n\n\n\n\n\n\n\n\ncountry\ncontinent\noil_consump\n\n\n\n\n21\nEgypt\nAfrica\n35300000\n\n\n74\nUSA\nAmericas\n778000000\n\n\n13\nChina\nAsia\n484000000\n\n\n62\nRussia\nEurope\n145000000\n\n\n2\nAustralia\nOceania\n46100000",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#set-arithmetic",
    "href": "p_untangled_joining_2.html#set-arithmetic",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.12 Set Arithmetic",
    "text": "22.12 Set Arithmetic\nA quick side note on set arithmetic for those who are unfamiliar.\nConsider two sets of the numbers 1:5, and 2:4.\n\nset_1 = set([1, 2, 3, 4, 5])\nset_2 = set([2, 3, 4])\n\nWe can check the values in set_1 that are not in set_2 by using set arithmetic:\n\nset_1 - set_2\n\n{1, 5}\n\n\nAnd the values in set_2 that are not in set_1 by using:\n\nset_2 - set_1\n\nset()",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_joining_2.html#practice-q-merging-oil-consumption-with-geographic-data",
    "href": "p_untangled_joining_2.html#practice-q-merging-oil-consumption-with-geographic-data",
    "title": "22  Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches",
    "section": "22.13 Practice Q: Merging Oil Consumption with Geographic Data",
    "text": "22.13 Practice Q: Merging Oil Consumption with Geographic Data\nRun the code to view the two datasets.\nThe first, oil_2012, records the oil consumption for the year 2012:\n\noil_2012\n\n\n\n\n\n\n\n\ncountry\noil_consump\n\n\n\n\n1343\nUnited Arab Emirates\n35200000\n\n\n1344\nArgentina\n28600000\n\n\n1345\nAustralia\n46100000\n\n\n1346\nAustria\n11900000\n\n\n1347\nAzerbaijan\n4170000\n\n\n...\n...\n...\n\n\n1417\nUSA\n778000000\n\n\n1418\nUzbekistan\n3030000\n\n\n1419\nVenezuela\n37200000\n\n\n1420\nVietnam\n17000000\n\n\n1421\nSouth Africa\n26300000\n\n\n\n\n79 rows × 2 columns\n\n\n\nAnd country_regions lists countries along with their respective regions and continents:\n\ncountry_regions\n\n\n\n\n\n\n\n\ncountry_name\ncountry_code\ncontinent\n\n\n\n\n0\nAfghanistan\nAFG\nAsia\n\n\n1\nAlbania\nALB\nEurope\n\n\n2\nAlgeria\nDZA\nAfrica\n\n\n3\nAmerican Samoa\nASM\nOceania\n\n\n4\nAndorra\nAND\nEurope\n\n\n...\n...\n...\n...\n\n\n237\nWestern Sahara\nESH\nAfrica\n\n\n238\nYemen\nYEM\nAsia\n\n\n239\nZambia\nZMB\nAfrica\n\n\n240\nZimbabwe\nZWE\nAfrica\n\n\n241\nÅland Islands\nALA\nEurope\n\n\n\n\n242 rows × 3 columns\n\n\n\nJoin the two datasets using the country codes as the key. Then find the countries with the highest oil consumption in each continent. As a sanity check, your answer should include the US & China.\n\noil_2012['country_code'] = cc.convert(oil_2012['country'], to='ISO3')\n\n\noil_2012_regions = oil_2012.merge(country_regions, on='country_code', how='left')\n\nmax_oil_by_continent = oil_2012_regions.loc[\n    oil_2012_regions.groupby('continent')['oil_consump'].idxmax()\n]\n\nmax_oil_by_continent[['country', 'continent', 'oil_consump']]\n\n\n\n\n\n\n\n\ncountry\ncontinent\noil_consump\n\n\n\n\n21\nEgypt\nAfrica\n35300000\n\n\n74\nUSA\nAmericas\n778000000\n\n\n13\nChina\nAsia\n484000000\n\n\n62\nRussia\nEurope\n145000000\n\n\n2\nAustralia\nOceania\n46100000",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joining 2: One-to-Many, Multi-Key Joins & Key Mismatches</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html",
    "href": "p_untangled_pivoting.html",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "",
    "text": "23.1 Packages\nimport pandas as pd\nimport plotly.express as px",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#data",
    "href": "p_untangled_pivoting.html#data",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.2 Data",
    "text": "23.2 Data\nRun the code below to load and define the datasets to be used in this lesson.\n\n# Temperatures dataset\ntemperatures = pd.DataFrame(\n    {\n        \"country\": [\"Sweden\", \"Denmark\", \"Norway\"],\n        \"1994\": [1, 2, 3],\n        \"1995\": [3, 4, 5],\n        \"1996\": [5, 6, 7],\n    }\n)\n\n# Fuels Wide dataset\nfuels_wide = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/oil_per_capita_wide.csv\"\n)\n\n# Eurostat Births Wide dataset\neuro_births_wide = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/euro_births_wide.csv\"\n)\n\n# Contracts dataset\ncontracts = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/chicago_contracts_20_23.csv\"\n)\n\n# Population dataset\npopulation = pd.read_csv(\n    \"https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/tidyr_population.csv\"\n)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#introduction",
    "href": "p_untangled_pivoting.html#introduction",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.3 Introduction",
    "text": "23.3 Introduction\nReshaping is a data manipulation technique that involves re-orienting the rows and columns of a dataset. This is often required to make data easier to analyze or understand.\nIn this lesson, we will cover how to effectively reshape data using pandas functions.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#learning-objectives",
    "href": "p_untangled_pivoting.html#learning-objectives",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.4 Learning Objectives",
    "text": "23.4 Learning Objectives\n\nUnderstand what wide data format is and what long data format is.\nLearn how to reshape wide data to long data using melt().\nLearn how to reshape long data to wide data using pivot().",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#what-do-wide-and-long-mean",
    "href": "p_untangled_pivoting.html#what-do-wide-and-long-mean",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.5 What Do “Wide” and “Long” Mean?",
    "text": "23.5 What Do “Wide” and “Long” Mean?\nThe terms “wide” and “long” are best understood in the context of example datasets. Let’s take a look at some now.\nImagine that you have three products for which you collect sales data over three months.\nWide Format:\n\n\n\nProduct\nJan\nFeb\nMar\n\n\n\n\nA\n100\n120\n110\n\n\nB\n90\n95\n100\n\n\nC\n80\n85\n90\n\n\n\n\nLong Format:\n\n\n\nProduct\nMonth\nSales\n\n\n\n\nA\nJan\n100\n\n\nA\nFeb\n120\n\n\nA\nMar\n110\n\n\nB\nJan\n90\n\n\nB\nFeb\n95\n\n\nB\nMar\n100\n\n\nC\nJan\n80\n\n\nC\nFeb\n85\n\n\nC\nMar\n90\n\n\n\nTake a minute to study the two datasets to make sure you understand the relationship between them.\nIn the wide dataset, each observational unit (each product) occupies only one row, and each measurement (sales in Jan, Feb, Mar) is in a separate column.\nIn the long dataset, on the other hand, each observational unit (each product) occupies multiple rows, with one row for each measurement.\n\nHere’s another example with mock data, where the observational units are countries:\nLong Format:\n\n\n\nCountry\nYear\nGDP\n\n\n\n\nUSA\n2020\n21433\n\n\nUSA\n2021\n22940\n\n\nChina\n2020\n14723\n\n\nChina\n2021\n17734\n\n\n\nWide Format:\n\n\n\nCountry\nGDP_2020\nGDP_2021\n\n\n\n\nUSA\n21433\n22940\n\n\nChina\n14723\n17734\n\n\n\n\nThe examples above are both time-series datasets because the measurements are repeated across time. But the concepts of wide and long are relevant to other kinds of data too.\nConsider the example below, showing the number of employees in different departments of three companies:\nWide Format:\n\n\n\nCompany\nHR\nSales\nIT\n\n\n\n\nA\n10\n20\n15\n\n\nB\n8\n25\n20\n\n\nC\n12\n18\n22\n\n\n\nLong Format:\n\n\n\nCompany\nDepartment\nEmployees\n\n\n\n\nA\nHR\n10\n\n\nA\nSales\n20\n\n\nA\nIT\n15\n\n\nB\nHR\n8\n\n\nB\nSales\n25\n\n\nB\nIT\n20\n\n\nC\nHR\n12\n\n\nC\nSales\n18\n\n\nC\nIT\n22\n\n\n\nIn the wide dataset, each observational unit (each company) occupies only one row, with the repeated measurements (number of employees in different departments) spread across multiple columns.\nIn the long dataset, each observational unit is spread over multiple rows.\n\n\n\n\n\n\nVocab\n\n\n\nThe observational units, sometimes called statistical units, are the primary entities or items described by the dataset.\nIn the first example, the observational units were products; in the second example, countries; and in the third example, companies.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n23.6 Practice Q: Wide or Long?\nConsider the temperatures dataset created earlier:\n\ntemperatures\n\n\n\n\n\n\n\n\ncountry\n1994\n1995\n1996\n\n\n\n\n0\nSweden\n1\n3\n5\n\n\n1\nDenmark\n2\n4\n6\n\n\n2\nNorway\n3\n5\n7\n\n\n\n\n\n\n\nIs this data in a wide or long format?",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#practice-q-wide-or-long",
    "href": "p_untangled_pivoting.html#practice-q-wide-or-long",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.6 Practice Q: Wide or Long?",
    "text": "23.6 Practice Q: Wide or Long?\nConsider the temperatures dataset created earlier:\n\ntemperatures\n\n\n\n\n\n\n\n\ncountry\n1994\n1995\n1996\n\n\n\n\n0\nSweden\n1\n3\n5\n\n\n1\nDenmark\n2\n4\n6\n\n\n2\nNorway\n3\n5\n7\n\n\n\n\n\n\n\nIs this data in a wide or long format?",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#when-should-you-use-wide-vs.-long-data",
    "href": "p_untangled_pivoting.html#when-should-you-use-wide-vs.-long-data",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.7 When Should You Use Wide vs. Long Data?",
    "text": "23.7 When Should You Use Wide vs. Long Data?\nThe truth is, it really depends on what you want to do! The wide format is great for displaying data because it’s easy to visually compare values this way. Long data is best for some data analysis tasks, like grouping and plotting.\nIt is essential to know how to switch from one format to the other easily. Switching from the wide to the long format, or the other way around, is called reshaping.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#from-wide-to-long-with-melt",
    "href": "p_untangled_pivoting.html#from-wide-to-long-with-melt",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.8 From Wide to Long with melt()",
    "text": "23.8 From Wide to Long with melt()\nTo practice reshaping from a wide to a long format, we’ll consider data from Our World in Data on fossil fuel consumption per capita. You can find the data here.\nBelow, we view this data on fossil fuel consumption per capita:\n\nfuels_wide\n\n\n\n\n\n\n\n\nEntity\nCode\ny_1970\ny_1980\ny_1990\ny_2000\ny_2010\ny_2020\n\n\n\n\n0\nAlgeria\nDZA\n1764.8470\n3532.7976\n4381.6636\n3351.2180\n5064.9863\n4877.2680\n\n\n1\nArgentina\nARG\n11677.9680\n10598.3990\n7046.2485\n7146.8154\n7966.7827\n6399.2114\n\n\n2\nAustralia\nAUS\n23040.4550\n25007.4380\n23046.9510\n23976.3550\n23584.3070\n20332.4100\n\n\n3\nAustria\nAUT\n14338.8090\n19064.0920\n16595.1930\n18189.0920\n18424.1170\n14934.0650\n\n\n4\nAzerbaijan\nAZE\nNaN\nNaN\n13516.0190\n9119.3470\n4031.9407\n5615.1157\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nUnited States\nUSA\n40813.9530\n42365.6500\n37525.5160\n37730.1600\n31791.3070\n26895.4770\n\n\n77\nUzbekistan\nUZB\nNaN\nNaN\n6324.8677\n3197.1330\n1880.1338\n1859.1548\n\n\n78\nVenezuela\nVEN\n11138.2210\n16234.0960\n12404.5570\n11239.9260\n14948.3070\n4742.6226\n\n\n79\nVietnam\nVNM\n1757.6117\n439.9465\n523.2565\n1280.3065\n2296.7590\n2927.7446\n\n\n80\nWorld\nOWID_WRL\n7217.8340\n8002.0854\n7074.2583\n6990.4272\n6879.6110\n6216.8060\n\n\n\n\n81 rows × 8 columns\n\n\n\nWe observe that each observational unit (each country) occupies only one row, with the repeated measurements of fossil fuel consumption (in kilowatt-hour equivalents) spread out across multiple columns. Hence, this dataset is in a wide format.\nTo convert it to a long format, we can use the convenient melt function. Within melt, can define the id variables, which we do not want to reshape:\n\nfuels_long = fuels_wide.melt(id_vars=[\"Entity\", \"Code\"])\nfuels_long\n\n\n\n\n\n\n\n\nEntity\nCode\nvariable\nvalue\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n1\nArgentina\nARG\ny_1970\n11677.9680\n\n\n2\nAustralia\nAUS\ny_1970\n23040.4550\n\n\n3\nAustria\nAUT\ny_1970\n14338.8090\n\n\n4\nAzerbaijan\nAZE\ny_1970\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n481\nUnited States\nUSA\ny_2020\n26895.4770\n\n\n482\nUzbekistan\nUZB\ny_2020\n1859.1548\n\n\n483\nVenezuela\nVEN\ny_2020\n4742.6226\n\n\n484\nVietnam\nVNM\ny_2020\n2927.7446\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nVery easy!\nLet’s sort it so it’s easier to read:\n\nfuels_long = fuels_long.sort_values(by=['Entity', 'variable'])\nfuels_long\n\n\n\n\n\n\n\n\nEntity\nCode\nvariable\nvalue\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n81\nAlgeria\nDZA\ny_1980\n3532.7976\n\n\n162\nAlgeria\nDZA\ny_1990\n4381.6636\n\n\n243\nAlgeria\nDZA\ny_2000\n3351.2180\n\n\n324\nAlgeria\nDZA\ny_2010\n5064.9863\n\n\n...\n...\n...\n...\n...\n\n\n161\nWorld\nOWID_WRL\ny_1980\n8002.0854\n\n\n242\nWorld\nOWID_WRL\ny_1990\n7074.2583\n\n\n323\nWorld\nOWID_WRL\ny_2000\n6990.4272\n\n\n404\nWorld\nOWID_WRL\ny_2010\n6879.6110\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nThe years are now indicated in the variable variable, and all the consumption values occupy a single variable, value. We may wish to rename the variable column to year, and the value column to oil_consumption. This can be done directly in the melt function:\n\nfuels_long = fuels_wide.melt(\n    id_vars=['Entity', 'Code'],\n    var_name='year',\n    value_name='oil_consumption'\n).sort_values(by=['Entity', 'year'])\nfuels_long\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n81\nAlgeria\nDZA\ny_1980\n3532.7976\n\n\n162\nAlgeria\nDZA\ny_1990\n4381.6636\n\n\n243\nAlgeria\nDZA\ny_2000\n3351.2180\n\n\n324\nAlgeria\nDZA\ny_2010\n5064.9863\n\n\n...\n...\n...\n...\n...\n\n\n161\nWorld\nOWID_WRL\ny_1980\n8002.0854\n\n\n242\nWorld\nOWID_WRL\ny_1990\n7074.2583\n\n\n323\nWorld\nOWID_WRL\ny_2000\n6990.4272\n\n\n404\nWorld\nOWID_WRL\ny_2010\n6879.6110\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nYou may also want to remove the y_ in front of each year. This can be achieved with a string operation.\n\nfuels_long['year'] = fuels_long['year'].str.replace('y_', '').astype(int)\nfuels_long\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\n1970\n1764.8470\n\n\n81\nAlgeria\nDZA\n1980\n3532.7976\n\n\n162\nAlgeria\nDZA\n1990\n4381.6636\n\n\n243\nAlgeria\nDZA\n2000\n3351.2180\n\n\n324\nAlgeria\nDZA\n2010\n5064.9863\n\n\n...\n...\n...\n...\n...\n\n\n161\nWorld\nOWID_WRL\n1980\n8002.0854\n\n\n242\nWorld\nOWID_WRL\n1990\n7074.2583\n\n\n323\nWorld\nOWID_WRL\n2000\n6990.4272\n\n\n404\nWorld\nOWID_WRL\n2010\n6879.6110\n\n\n485\nWorld\nOWID_WRL\n2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nHere’s what we did above:\n\nUsed str.replace() to remove the y_ prefix from each year.\nConverted the year column to integers using astype(int).\nSorted the data by “Entity” and “year” using sort_values().\n\n\n\n\n\n\n\nPractice\n\n\n\n23.9 Practice Q: Temperatures to Long\nConvert the temperatures dataset shown below into a long format. Your answer should have the following column names: “country”, “year”, and “avg_temp”.\n\n# Your code here\ntemperatures\n\n\n\n\n\n\n\n\ncountry\n1994\n1995\n1996\n\n\n\n\n0\nSweden\n1\n3\n5\n\n\n1\nDenmark\n2\n4\n6\n\n\n2\nNorway\n3\n5\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n23.10 Practice Q: Eurostat Births to Long\nFor this practice question, you will use the euro_births_wide dataset from Eurostat. It shows the annual number of births in 50 European countries:\n\neuro_births_wide.head()\n\n\n\n\n\n\n\n\ncountry\nx2015\nx2016\nx2017\nx2018\nx2019\nx2020\nx2021\n\n\n\n\n0\nBelgium\n122274.0\n121896.0\n119690.0\n118319.0\n117695.0\n114350.0\n118349.0\n\n\n1\nBulgaria\n65950.0\n64984.0\n63955.0\n62197.0\n61538.0\n59086.0\n58678.0\n\n\n2\nCzechia\n110764.0\n112663.0\n114405.0\n114036.0\n112231.0\n110200.0\n111793.0\n\n\n3\nDenmark\n58205.0\n61614.0\n61397.0\n61476.0\n61167.0\n60937.0\n63473.0\n\n\n4\nGermany\n737575.0\n792141.0\n784901.0\n787523.0\n778090.0\n773144.0\n795492.0\n\n\n\n\n\n\n\nThe data is in a wide format. Convert it to a long format DataFrame that has the following column names: “country”, “year”, and “births_count”.\nRemove the x prefix from the year columns and convert them to integers.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#practice-q-temperatures-to-long",
    "href": "p_untangled_pivoting.html#practice-q-temperatures-to-long",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.9 Practice Q: Temperatures to Long",
    "text": "23.9 Practice Q: Temperatures to Long\nConvert the temperatures dataset shown below into a long format. Your answer should have the following column names: “country”, “year”, and “avg_temp”.\n\n# Your code here\ntemperatures\n\n\n\n\n\n\n\n\ncountry\n1994\n1995\n1996\n\n\n\n\n0\nSweden\n1\n3\n5\n\n\n1\nDenmark\n2\n4\n6\n\n\n2\nNorway\n3\n5\n7",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#practice-q-eurostat-births-to-long",
    "href": "p_untangled_pivoting.html#practice-q-eurostat-births-to-long",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.10 Practice Q: Eurostat Births to Long",
    "text": "23.10 Practice Q: Eurostat Births to Long\nFor this practice question, you will use the euro_births_wide dataset from Eurostat. It shows the annual number of births in 50 European countries:\n\neuro_births_wide.head()\n\n\n\n\n\n\n\n\ncountry\nx2015\nx2016\nx2017\nx2018\nx2019\nx2020\nx2021\n\n\n\n\n0\nBelgium\n122274.0\n121896.0\n119690.0\n118319.0\n117695.0\n114350.0\n118349.0\n\n\n1\nBulgaria\n65950.0\n64984.0\n63955.0\n62197.0\n61538.0\n59086.0\n58678.0\n\n\n2\nCzechia\n110764.0\n112663.0\n114405.0\n114036.0\n112231.0\n110200.0\n111793.0\n\n\n3\nDenmark\n58205.0\n61614.0\n61397.0\n61476.0\n61167.0\n60937.0\n63473.0\n\n\n4\nGermany\n737575.0\n792141.0\n784901.0\n787523.0\n778090.0\n773144.0\n795492.0\n\n\n\n\n\n\n\nThe data is in a wide format. Convert it to a long format DataFrame that has the following column names: “country”, “year”, and “births_count”.\nRemove the x prefix from the year columns and convert them to integers.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#using-long-data-for-analysis",
    "href": "p_untangled_pivoting.html#using-long-data-for-analysis",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.11 Using Long Data for Analysis",
    "text": "23.11 Using Long Data for Analysis\nLet’s see why long data is often better for analysis.\nConsider again the fuels_wide dataset:\n\nfuels_wide.head()\n\n\n\n\n\n\n\n\nEntity\nCode\ny_1970\ny_1980\ny_1990\ny_2000\ny_2010\ny_2020\n\n\n\n\n0\nAlgeria\nDZA\n1764.847\n3532.7976\n4381.6636\n3351.2180\n5064.9863\n4877.2680\n\n\n1\nArgentina\nARG\n11677.968\n10598.3990\n7046.2485\n7146.8154\n7966.7827\n6399.2114\n\n\n2\nAustralia\nAUS\n23040.455\n25007.4380\n23046.9510\n23976.3550\n23584.3070\n20332.4100\n\n\n3\nAustria\nAUT\n14338.809\n19064.0920\n16595.1930\n18189.0920\n18424.1170\n14934.0650\n\n\n4\nAzerbaijan\nAZE\nNaN\nNaN\n13516.0190\n9119.3470\n4031.9407\n5615.1157\n\n\n\n\n\n\n\n\nfuels_long.head()\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\n1970\n1764.8470\n\n\n81\nAlgeria\nDZA\n1980\n3532.7976\n\n\n162\nAlgeria\nDZA\n1990\n4381.6636\n\n\n243\nAlgeria\nDZA\n2000\n3351.2180\n\n\n324\nAlgeria\nDZA\n2010\n5064.9863\n\n\n\n\n\n\n\nIf we want to find the average fossil fuel consumption per country, this is very easy to do with the long format:\n\nfuels_long.groupby('Entity')['oil_consumption'].mean()\n\nEntity\nAlgeria           3828.796750\nArgentina         8472.570833\nAustralia        23164.652667\nAustria          16924.228000\nAzerbaijan        8070.605600\n                     ...     \nUnited States    36187.010500\nUzbekistan        3315.322325\nVenezuela        11784.621600\nVietnam           1537.604133\nWorld             7063.503650\nName: oil_consumption, Length: 81, dtype: float64\n\n\nBut with the wide format, this is not so easy:\n\nfuels_wide[['y_1970', 'y_1980', 'y_1990', 'y_2000', 'y_2010', 'y_2020']].mean(axis=1)\n\n0      3828.796750\n1      8472.570833\n2     23164.652667\n3     16924.228000\n4      8070.605600\n          ...     \n76    36187.010500\n77     3315.322325\n78    11784.621600\n79     1537.604133\n80     7063.503650\nLength: 81, dtype: float64\n\n\nImagine if you had 100 years of data!\nAnd mean is a fairly simple operation. How would you calculate the standard deviation of fossil fuel consumption per country?\n\nLong data is also very useful for plotting.\nFor example, to plot the average fossil fuel consumption per country over time, we can use the following code:\n\nsubset = fuels_long.query('Entity in [\"Peru\", \"Iran\", \"China\"]')\npx.line(subset, x='year', y='oil_consumption', color='Entity', title='Average Fossil Fuel Consumption per Country')\n\n                                                \n\n\nTo create a plot like this with the wide format is not directly possible, since the data you want to plot is scattered across multiple columns.\nSo as you can see, while wide data is great for display, long data is very useful for analysis and plotting.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#from-long-to-wide",
    "href": "p_untangled_pivoting.html#from-long-to-wide",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.12 From Long to Wide",
    "text": "23.12 From Long to Wide\nNow you know how to reshape from wide to long with melt(). How about going the other way, from long to wide? For this, you can use the pivot() function.\nBut before we see how to use this function to manipulate long data, let’s first consider where you’re likely to run into long data.\nWhile wide data tends to come from external sources (as we have seen above), long data, on the other hand, is likely to be created by you while data wrangling, especially in the course of grouped aggregations.\nLet’s see an example of this now.\nWe will use a dataset of contracts granted by the city of Chicago from 2020 to 2023. You can find more information about the data here.\n\ncontracts\n\n\n\n\n\n\n\n\nyear\napproval_date\ndescription\ncontract_num\nrevision_num\nspecification_num\ncontract_type\nstart_date\nend_date\ndepartment\nvendor_name\nvendor_id\naddress_1\naddress_2\ncity\nstate\nzip\naward_amount\nprocurement_type\ncontract_pdf\n\n\n\n\n0\n2020\n2020-01-02\nLEASE\n24406\n32\n96136\nPROPERTY LEASE\nNaN\nNaN\nNaN\n8700 BUILDING LLC\n89123305A\n7300 S NARRAGANSETT\nNaN\nBEDFORD PARK\nIllinois\n60638\n321.1\nNaN\nNaN\n\n\n1\n2020\n2020-01-03\nDFSS-HHS-CS-CEL:\n113798\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nCATHOLIC CHARITIES OF THE ARCHDIOCESE OF CHICAGO\n102484615A\n1 E BANKS ST\nNaN\nCHICAGO\nIllinois\n60670\n17692515.0\nNaN\nNaN\n\n\n2\n2020\n2020-01-03\nDFSS-HHS-CS-CEL:\n113819\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nKIMBALL DAYCARE CENTER & KINDERGARTEN INC\n105458567Z\n1636-1638 N KIMBALL AVE\nNaN\nCHICAGO\nIllinois\n60647\n11461500.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n3\n2020\n2020-01-03\nDFSS-HHS-CS-CEL:\n113818\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nJUDAH INTERNATIONAL OUTREACH MINISTRIES, INC\n94219962X\n856 N PULASKI RD\nNaN\nCHICAGO\nIllinois\n60651\n2356515.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n4\n2020\n2020-01-03\nDFSS-HHS-CS-CEL:\n113820\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nMarillac St. Vincent Family Services Inc DBA S...\n97791861L\n212 S FRANCISCO AVENUE EFT\nNaN\nCHICAGO\nIllinois\n60612\n3666015.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n28823\n2023\n2023-12-29\nDFSS-CORP-HL-PSH:\n220413\n3\n1221503\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nINNER VOICE INC.\n6231926M\n1621 W WALNUT ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60612\n0.0\nNaN\nNaN\n\n\n28824\n2023\n2023-12-29\nDFSS-CORP-YS-OST:\n253846\n0\n1247493\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nAFTER-SCHOOL MATTERS, INC.|CLEANED-UP\n72580818P\n66 E RANDOLPH ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60601\n32000.0\nNaN\nNaN\n\n\n28825\n2023\n2023-12-29\nDFSS-IDHS-HL-INTHS:\n253843\n0\n1235949\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nBREAKTHROUGH URBAN MINISTRIES, INC.\n94722896V\n402 N ST LOUIS AVENUE EFT\nNaN\nCHICAGO\nIllinois\n60624\n14400.0\nNaN\nNaN\n\n\n28826\n2023\n2023-12-29\nCDPH-RW-PA: ESS-HRSA PO 116685 CHICAGO HOUSE A...\n192085\n1\n1095441\nDELEGATE AGENCY\nNaN\nNaN\nDEPARTMENT OF HEALTH\nCHICAGO HOUSE & SOCIAL SERVICE AGENCY\n105470138T\n2229 S MICHIGAN AVE 304 EFT\nNaN\nCHICAGO\nIllinois\n60616\n-32025.2\nNaN\nNaN\n\n\n28827\n2023\n2023-12-29\nDFSS-HHS-CS-CEL:\n222199\n1\n1070196\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nALLISON'S INFANT & TODDLER CENTER INC\n62751817Z\n234 E 115TH ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60628\n141923.0\nNaN\nNaN\n\n\n\n\n28828 rows × 20 columns\n\n\n\nEach row corresponds to one contract, and we have each contract’s ID number, the year in which it was granted, the amount of the contract, and the vendor’s name and address, among other variables.\nNow, consider the following grouped summary of the contracts dataset, which shows the number of contracts by state of the vendor in each year:\n\ncontracts_summary = contracts.groupby(\"state\")[\"year\"].value_counts().reset_index()\ncontracts_summary\n\n\n\n\n\n\n\n\nstate\nyear\ncount\n\n\n\n\n0\nAlabama\n2023\n7\n\n\n1\nAlabama\n2021\n2\n\n\n2\nAlabama\n2020\n1\n\n\n3\nAlabama\n2022\n1\n\n\n4\nArizona\n2020\n3\n\n\n...\n...\n...\n...\n\n\n128\nWashington\n2021\n1\n\n\n129\nWisconsin\n2023\n25\n\n\n130\nWisconsin\n2020\n18\n\n\n131\nWisconsin\n2022\n17\n\n\n132\nWisconsin\n2021\n15\n\n\n\n\n133 rows × 3 columns\n\n\n\nThe output of this grouped operation is a quintessentially “long” dataset. Each observational unit (each state) occupies multiple rows, with one row for each measurement (each year).\nNow, let’s see how to convert such long data into a wide format with pivot().\nThe code is quite straightforward:\n\ncontracts_wide = contracts_summary.pivot(\n    index=\"state\", columns=\"year\", values=\"count\"\n).reset_index()\ncontracts_wide.head()\n\n\n\n\n\n\n\nyear\nstate\n2020\n2021\n2022\n2023\n\n\n\n\n0\nAlabama\n1.0\n2.0\n1.0\n7.0\n\n\n1\nArizona\n3.0\n1.0\n3.0\n2.0\n\n\n2\nArkansas\n1.0\nNaN\n1.0\nNaN\n\n\n3\nBritish Columbia\nNaN\n1.0\nNaN\nNaN\n\n\n4\nCalifornia\n36.0\n42.0\n43.0\n38.0\n\n\n\n\n\n\n\nAs you can see, pivot() has three important arguments:\n\nindex defines which column(s) to use as the new index. In our case, it’s the “state” since we want each row to represent one state.\ncolumns identifies which variable to use to define column names in the wide format. In our case, it’s the “year”. You can see that the years are now the column names.\nvalues specifies which values will become the core of the wide data format. In our case, it’s the number of contracts “count”.\n\nYou might also want to have the years be your primary observational units, with each year occupying one row. This can be carried out similarly to the above example, but with year as the index and state as the columns:\n\ncontracts_wide_year = contracts_summary.pivot(\n    index=\"year\", columns=\"state\", values=\"count\"\n).reset_index()\ncontracts_wide_year\n\n\n\n\n\n\n\nstate\nyear\nAlabama\nArizona\nArkansas\nBritish Columbia\nCalifornia\nCanada\nColorado\nConnecticut\nDelaware\n...\nOregon\nPennsylvania\nRhode Island\nSouth Carolina\nTennessee\nTexas\nVermont\nVirginia\nWashington\nWisconsin\n\n\n\n\n0\n2020\n1.0\n3.0\n1.0\nNaN\n36.0\n1.0\n6.0\n1.0\nNaN\n...\n5.0\n20.0\n1.0\n2.0\n2.0\n25.0\nNaN\n4.0\nNaN\n18.0\n\n\n1\n2021\n2.0\n1.0\nNaN\n1.0\n42.0\n1.0\n2.0\n5.0\nNaN\n...\nNaN\n24.0\nNaN\n3.0\n2.0\n24.0\n1.0\n4.0\n1.0\n15.0\n\n\n2\n2022\n1.0\n3.0\n1.0\nNaN\n43.0\n1.0\n7.0\n3.0\n1.0\n...\nNaN\n31.0\nNaN\n2.0\n3.0\n37.0\nNaN\n7.0\nNaN\n17.0\n\n\n3\n2023\n7.0\n2.0\nNaN\nNaN\n38.0\nNaN\n6.0\n5.0\n2.0\n...\nNaN\n37.0\nNaN\n1.0\n3.0\n28.0\nNaN\n9.0\nNaN\n25.0\n\n\n\n\n4 rows × 44 columns\n\n\n\nHere, the unique observation units (our rows) are now the years (2020, 2021, 2022, 2023).\n\n\n\n\n\n\nPractice\n\n\n\n23.13 Practice Q: Temperatures back to Wide\nConvert the long temperatures_long dataset you created above back to a wide format. Your answer should have the following column names: “country”, “1994”, “1995”, and “1996”.\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n23.14 Practice Q: Population to Wide\nThe population dataset shows the populations of 219 countries over time.\nReshape this data into a wide format.\n\npopulation\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4040\nZimbabwe\n2009\n12888918\n\n\n4041\nZimbabwe\n2010\n13076978\n\n\n4042\nZimbabwe\n2011\n13358738\n\n\n4043\nZimbabwe\n2012\n13724317\n\n\n4044\nZimbabwe\n2013\n14149648\n\n\n\n\n4045 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#practice-q-temperatures-back-to-wide",
    "href": "p_untangled_pivoting.html#practice-q-temperatures-back-to-wide",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.13 Practice Q: Temperatures back to Wide",
    "text": "23.13 Practice Q: Temperatures back to Wide\nConvert the long temperatures_long dataset you created above back to a wide format. Your answer should have the following column names: “country”, “1994”, “1995”, and “1996”.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#practice-q-population-to-wide",
    "href": "p_untangled_pivoting.html#practice-q-population-to-wide",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.14 Practice Q: Population to Wide",
    "text": "23.14 Practice Q: Population to Wide\nThe population dataset shows the populations of 219 countries over time.\nReshape this data into a wide format.\n\npopulation\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4040\nZimbabwe\n2009\n12888918\n\n\n4041\nZimbabwe\n2010\n13076978\n\n\n4042\nZimbabwe\n2011\n13358738\n\n\n4043\nZimbabwe\n2012\n13724317\n\n\n4044\nZimbabwe\n2013\n14149648\n\n\n\n\n4045 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#reshaping-can-be-hard",
    "href": "p_untangled_pivoting.html#reshaping-can-be-hard",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.15 Reshaping Can Be Hard",
    "text": "23.15 Reshaping Can Be Hard\nWe have mostly looked at very simple examples of reshaping here, but in the wild, reshaping can be difficult to do accurately.\nWhen you run into such cases, we recommend looking at the official documentation of reshaping from the pandas team, as it is quite rich in examples.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#wrap-up",
    "href": "p_untangled_pivoting.html#wrap-up",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.16 Wrap-Up",
    "text": "23.16 Wrap-Up\nCongratulations! You’ve gotten the hang of reshaping data with pandas.\nYou now understand the differences between wide and long formats and can skillfully use melt() and pivot() to transform your data as needed.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#solution-q-wide-or-long",
    "href": "p_untangled_pivoting.html#solution-q-wide-or-long",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.17 Solution Q: Wide or Long?",
    "text": "23.17 Solution Q: Wide or Long?\nThe data is in a wide format.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#solution-q-temperatures-to-long",
    "href": "p_untangled_pivoting.html#solution-q-temperatures-to-long",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.18 Solution Q: Temperatures to Long",
    "text": "23.18 Solution Q: Temperatures to Long\n\n# Melt the wide data into long format\ntemperatures_long = temperatures.melt(\n    id_vars=[\"country\"], var_name=\"year\", value_name=\"avgtemp\"\n)\n\n# Display the long format data\ntemperatures_long\n\n\n\n\n\n\n\n\ncountry\nyear\navgtemp\n\n\n\n\n0\nSweden\n1994\n1\n\n\n1\nDenmark\n1994\n2\n\n\n2\nNorway\n1994\n3\n\n\n3\nSweden\n1995\n3\n\n\n4\nDenmark\n1995\n4\n\n\n5\nNorway\n1995\n5\n\n\n6\nSweden\n1996\n5\n\n\n7\nDenmark\n1996\n6\n\n\n8\nNorway\n1996\n7",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#solution-q-eurostat-births-to-long",
    "href": "p_untangled_pivoting.html#solution-q-eurostat-births-to-long",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.19 Solution Q: Eurostat Births to Long",
    "text": "23.19 Solution Q: Eurostat Births to Long\n\n# Melt the wide data into long format\nbirths_long = euro_births_wide.melt(\n    id_vars=[\"country\"], var_name=\"year\", value_name=\"births_count\"\n)\n\n# Display the long format data\nbirths_long\n\n\n\n\n\n\n\n\ncountry\nyear\nbirths_count\n\n\n\n\n0\nBelgium\nx2015\n122274.0\n\n\n1\nBulgaria\nx2015\n65950.0\n\n\n2\nCzechia\nx2015\n110764.0\n\n\n3\nDenmark\nx2015\n58205.0\n\n\n4\nGermany\nx2015\n737575.0\n\n\n...\n...\n...\n...\n\n\n345\nUkraine\nx2021\n212.0\n\n\n346\nArmenia\nx2021\n271983.0\n\n\n347\nAzerbaijan\nx2021\nNaN\n\n\n348\nGeorgia\nx2021\n112284.0\n\n\n349\nNaN\nx2021\n45946.0\n\n\n\n\n350 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#solution-q-temperatures-back-to-wide",
    "href": "p_untangled_pivoting.html#solution-q-temperatures-back-to-wide",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.20 Solution Q: Temperatures back to Wide",
    "text": "23.20 Solution Q: Temperatures back to Wide\n\n# Pivot the long data into wide format\ntemperatures_wide = temperatures_long.pivot(\n    index=\"country\", columns=\"year\", values=\"avgtemp\"\n).reset_index()\n\n# Display the wide format data\ntemperatures_wide\n\n\n\n\n\n\n\nyear\ncountry\n1994\n1995\n1996\n\n\n\n\n0\nDenmark\n2\n4\n6\n\n\n1\nNorway\n3\n5\n7\n\n\n2\nSweden\n1\n3\n5",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#solution-q-population-to-wide",
    "href": "p_untangled_pivoting.html#solution-q-population-to-wide",
    "title": "23  Reshaping Data with melt() and pivot()",
    "section": "23.21 Solution Q: Population to Wide",
    "text": "23.21 Solution Q: Population to Wide\n\n# Pivot the long data into wide format\npopulation_wide = population.pivot(\n    index=\"country\", columns=\"year\", values=\"population\"\n).reset_index()\n\n# Display the wide format data\npopulation_wide\n\n\n\n\n\n\n\nyear\ncountry\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n\n\n\n\n0\nAfghanistan\n17586073.0\n18415307.0\n19021226.0\n19496836.0\n19987071.0\n20595360.0\n21347782.0\n22202806.0\n23116142.0\n24018682.0\n24860855.0\n25631282.0\n26349243.0\n27032197.0\n27708187.0\n28397812.0\n29105480.0\n29824536.0\n30551674.0\n\n\n1\nAlbania\n3357858.0\n3341043.0\n3331317.0\n3325456.0\n3317941.0\n3304948.0\n3286084.0\n3263596.0\n3239385.0\n3216197.0\n3196130.0\n3179573.0\n3166222.0\n3156608.0\n3151185.0\n3150143.0\n3153883.0\n3162083.0\n3173271.0\n\n\n2\nAlgeria\n29315463.0\n29845208.0\n30345466.0\n30820435.0\n31276295.0\n31719449.0\n32150198.0\n32572977.0\n33003442.0\n33461345.0\n33960903.0\n34507214.0\n35097043.0\n35725377.0\n36383302.0\n37062820.0\n37762962.0\n38481705.0\n39208194.0\n\n\n3\nAmerican Samoa\n52874.0\n53926.0\n54942.0\n55899.0\n56768.0\n57522.0\n58176.0\n58729.0\n59117.0\n59262.0\n59117.0\n58652.0\n57919.0\n57053.0\n56245.0\n55636.0\n55274.0\n55128.0\n55165.0\n\n\n4\nAndorra\n63854.0\n64274.0\n64090.0\n63799.0\n64084.0\n65399.0\n68000.0\n71639.0\n75643.0\n79060.0\n81223.0\n81877.0\n81292.0\n79969.0\n78659.0\n77907.0\n77865.0\n78360.0\n79218.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n212\nWallis and Futuna Islands\n14143.0\n14221.0\n14309.0\n14394.0\n14460.0\n14497.0\n14501.0\n14476.0\n14422.0\n14344.0\n14246.0\n14126.0\n13988.0\n13840.0\n13697.0\n13565.0\n13451.0\n13353.0\n13272.0\n\n\n213\nWest Bank and Gaza Strip\n2598393.0\n2722497.0\n2851993.0\n2980563.0\n3099951.0\n3204572.0\n3291620.0\n3363542.0\n3426549.0\n3489743.0\n3559856.0\n3638829.0\n3725076.0\n3817551.0\n3914035.0\n4012880.0\n4114199.0\n4218771.0\n4326295.0\n\n\n214\nYemen\n15018201.0\n15578640.0\n16088019.0\n16564235.0\n17035531.0\n17522537.0\n18029989.0\n18551068.0\n19081306.0\n19612696.0\n20139661.0\n20661714.0\n21182162.0\n21703571.0\n22229625.0\n22763008.0\n23304206.0\n23852409.0\n24407381.0\n\n\n215\nZambia\n8841338.0\n9073311.0\n9320089.0\n9577483.0\n9839179.0\n10100981.0\n10362137.0\n10625423.0\n10894519.0\n11174650.0\n11470022.0\n11781612.0\n12109620.0\n12456527.0\n12825031.0\n13216985.0\n13633796.0\n14075099.0\n14538640.0\n\n\n216\nZimbabwe\n11639364.0\n11846110.0\n12045813.0\n12229500.0\n12384727.0\n12503652.0\n12586763.0\n12640922.0\n12673103.0\n12693047.0\n12710589.0\n12724308.0\n12740160.0\n12784041.0\n12888918.0\n13076978.0\n13358738.0\n13724317.0\n14149648.0\n\n\n\n\n217 rows × 20 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Reshaping Data with melt() and pivot()</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html",
    "href": "p_ai_LLM_functions.html",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "",
    "text": "24.1 Intro\nIn this tutorial, we’ll explore how to leverage Large Language Models (LLMs) to generate text using OpenAI’s API. We’ll use the gpt-4o-mini model to generate responses to fixed and variable prompts, optimize our code with helper functions and vectorization, and handle data using pandas DataFrames.",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#learning-objectives",
    "href": "p_ai_LLM_functions.html#learning-objectives",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.2 Learning Objectives",
    "text": "24.2 Learning Objectives\n\nSet up the OpenAI client\nDefine and use simple functions to generate text\nUse vectorization to apply functions to DataFrames",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#setting-up-the-openai-client",
    "href": "p_ai_LLM_functions.html#setting-up-the-openai-client",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.3 Setting Up the OpenAI Client",
    "text": "24.3 Setting Up the OpenAI Client\nFirst, we need to set up the OpenAI client using your API key. Here, we store the key in a file called local_settings.py, then import it into our script.\n\nfrom openai import OpenAI\nimport pandas as pd\nimport numpy as np\nfrom local_settings import OPENAI_KEY\n\n# Set up the OpenAI API key\n# Initialize the OpenAI client with your API key\nclient = OpenAI(api_key=OPENAI_KEY)\n\nAlternatively, you can pass your API key directly when setting the api_key, but be cautious not to expose it in your code, especially if you plan to share or publish it.",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#making-an-api-call",
    "href": "p_ai_LLM_functions.html#making-an-api-call",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.4 Making an API Call",
    "text": "24.4 Making an API Call\nLet’s make an API call to the gpt-4o-mini model to generate a response to a prompt.\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": \"What is the most tourist-friendly city in France?\"}]\n)\nprint(response.choices[0].message.content)\n\nParis is generally considered the most tourist-friendly city in France. Known as the \"City of Light,\" it attracts millions of visitors each year with its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris offers a well-developed public transportation system, a wide range of accommodations, and numerous dining options that cater to different budgets.\n\nAdditionally, the city is rich in culture, history, and art, making it a prime destination for tourists from around the world. While other cities in France, like Nice, Lyon, and Marseille, are also quite welcoming to tourists, Paris's allure, infrastructure, and amenities make it the top choice for many travelers.",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#defining-helper-functions",
    "href": "p_ai_LLM_functions.html#defining-helper-functions",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.5 Defining Helper Functions",
    "text": "24.5 Defining Helper Functions\nTo simplify our code and avoid repetition, we’ll define a helper function for making API calls. API calls contain a lot of boilerplate code, so encapsulating this logic in a function makes our code cleaner and more maintainable.\nIf you ever forget how to structure the API calls, refer to the OpenAI API documentation or search for “OpenAI Python API example” online.\nHere’s how we can define the llm_chat function:\n\ndef llm_chat(message):\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": message}]\n    )\n    return response.choices[0].message.content\n\nThis function takes a message as input, sends it to the LLM, and returns the generated response. The model parameter specifies which model to use—in this case, gpt-4o-mini. We use this model for its balance of quality, speed, and cost. If you want a more performant model, you can use gpt-4o but be careful not to exceed your API quota.",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#fixed-questions",
    "href": "p_ai_LLM_functions.html#fixed-questions",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.6 Fixed Questions",
    "text": "24.6 Fixed Questions\nLet’s start by sending a fixed question to the gpt-4o-mini model and retrieving a response.\n\n# Example usage\nresponse = llm_chat(\"What is the most tourist-friendly city in France?\")\nprint(response)\n\nParis is widely regarded as the most tourist-friendly city in France. The capital offers a wealth of iconic attractions, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral, making it a prime destination for visitors. Additionally, Paris boasts an extensive public transportation system, including buses and the Metro, which makes it easy for tourists to navigate the city. The city is also known for its plethora of restaurants, shops, and cultural experiences, catering to a variety of tastes and preferences. Other cities in France, like Nice, Lyon, and Marseille, are also popular with tourists, but Paris remains the most prominent and accessible for international visitors.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n24.7 Practice Q: Get tourist-friendly city in Brazil\nUse the llm_chat function to ask the model for the most tourist-friendly city in Brazil. Store the response in a variable called rec_brazil. Print the response.\n\n# Your code here",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#practice-q-get-tourist-friendly-city-in-brazil",
    "href": "p_ai_LLM_functions.html#practice-q-get-tourist-friendly-city-in-brazil",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.7 Practice Q: Get tourist-friendly city in Brazil",
    "text": "24.7 Practice Q: Get tourist-friendly city in Brazil\nUse the llm_chat function to ask the model for the most tourist-friendly city in Brazil. Store the response in a variable called rec_brazil. Print the response.\n\n# Your code here",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#variables-as-prompt-inputs",
    "href": "p_ai_LLM_functions.html#variables-as-prompt-inputs",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.8 Variables as Prompt Inputs",
    "text": "24.8 Variables as Prompt Inputs\nOften, you’ll want to generate responses based on varying inputs. Let’s create a function that takes a country as input and asks the model for the most tourist-friendly city in that country.\n\ndef city_rec(country):\n    prompt = f\"What is the most tourist-friendly city in {country}?\"\n    return llm_chat(prompt)\n\nNow, you can get recommendations for different countries by calling city_rec(\"Country Name\"):\n\ncity_rec(\"Nigeria\")\n\n'Lagos is often considered the most tourist-friendly city in Nigeria. As the largest city in the country, it offers a vibrant cultural scene, diverse cuisine, numerous entertainment options, and various attractions, such as the Nike Art Gallery, Lekki Conservation Centre, and beautiful beaches like Tarkwa Bay and Elegushi Beach. Lagos has a lively nightlife and a range of accommodations to suit different budgets, making it appealing for tourists. \\n\\nOther cities like Abuja, the capital, also attract visitors with its modern infrastructure, and cultural sites like the National Mosque and Aso Rock. Port Harcourt and Calabar are notable for their unique cultural heritage and festivals as well. Each city has its own character and appeal depending on what tourists are looking for.'\n\n\nHowever, if we try to use this function on a list of countries or a DataFrame column directly, it won’t process each country individually. Instead, it will attempt to concatenate the list into a single string, which isn’t the desired behavior.\n\n# Incorrect usage\ncountry_df = pd.DataFrame({\"country\": [\"Nigeria\", \"Chile\", \"France\", \"Canada\"]})\n\nresponse = city_rec(country_df[\"country\"])\n\nprint(response)\n\nDetermining the \"most tourist-friendly\" city can be subjective and depends on various factors, including infrastructure, hospitality, safety, attractions, and overall visitor experience. Here’s a brief overview of some key cities in the countries listed:\n\n0. **Nigeria**: Lagos is the largest city and a major economic hub, but it might not be as tourist-friendly compared to others due to safety and infrastructure concerns.\n\n1. **Chile**: Santiago, the capital, is often seen as a tourist-friendly city with a mix of modern and historic sites, good public transportation, and a rich cultural scene.\n\n2. **France**: Paris is renowned as a top tourist destination globally, famous for its attractions, dining, art, and overall hospitality.\n\n3. **Canada**: Cities like Toronto and Vancouver are known for their inclusivity, safety, and various attractions, making them tourist-friendly as well.\n\nBased on general consensus and global recognition, **Paris (France)** is often considered the most tourist-friendly city, highly regarded for its extensive services tailored for visitors, iconic landmarks, and vibrant atmosphere.\n\n\nTo process each country individually, we can use NumPy’s vectorize function. This function transforms city_rec so that it can accept arrays (like lists or NumPy arrays) and apply the function element-wise.\n\n# Vectorize the function\ncity_rec_vec = np.vectorize(city_rec)\n\n# Apply the function to each country\ncountry_df[\"city_rec\"] = city_rec_vec(country_df[\"country\"])\ncountry_df\n\n\n\n\n\n\n\n\ncountry\ncity_rec\n\n\n\n\n0\nNigeria\nLagos is often considered the most tourist-fri...\n\n\n1\nChile\nSantiago, the capital of Chile, is often consi...\n\n\n2\nFrance\nParis is often considered the most tourist-fri...\n\n\n3\nCanada\nWhile \"most tourist-friendly\" can be subjectiv...\n\n\n\n\n\n\n\nThis code will output a DataFrame with a new column city_rec containing city recommendations corresponding to each country.\n\n\n\n\n\n\nPractice\n\n\n\n24.9 Practice Q: Get local dishes\nCreate a function called get_local_dishes that takes a country name as input and returns some of the most famous local dishes from that country. Then, vectorize this function and apply it to the country_df DataFrame to add a column with local dish recommendations for each country.\n\n# Your code here",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#practice-q-get-local-dishes",
    "href": "p_ai_LLM_functions.html#practice-q-get-local-dishes",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.9 Practice Q: Get local dishes",
    "text": "24.9 Practice Q: Get local dishes\nCreate a function called get_local_dishes that takes a country name as input and returns some of the most famous local dishes from that country. Then, vectorize this function and apply it to the country_df DataFrame to add a column with local dish recommendations for each country.\n\n# Your code here",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#automated-summary-movies-dataset",
    "href": "p_ai_LLM_functions.html#automated-summary-movies-dataset",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.10 Automated Summary: Movies Dataset",
    "text": "24.10 Automated Summary: Movies Dataset\nIn this example, we’ll use the movies dataset from vega_datasets to generate automated summaries for each movie. We’ll convert each movie’s data into a dictionary and use it as input for the LLM to generate a one-paragraph performance summary.\nFirst, let’s load the movies dataset and preview the first few rows:\n\nimport pandas as pd\nimport vega_datasets as vd\n\n# Load the movies dataset\nmovies = vd.data.movies().head()  # Using only the first 5 rows to conserve API credits\nmovies\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNext, we’ll convert each row of the DataFrame into a dictionary. This will be useful for passing the data to the LLM.\n\n# Convert each movie's data into a dictionary\nmovies.to_dict(orient=\"records\")\n\n[{'Title': 'The Land Girls',\n  'US_Gross': 146083.0,\n  'Worldwide_Gross': 146083.0,\n  'US_DVD_Sales': nan,\n  'Production_Budget': 8000000.0,\n  'Release_Date': 'Jun 12 1998',\n  'MPAA_Rating': 'R',\n  'Running_Time_min': nan,\n  'Distributor': 'Gramercy',\n  'Source': None,\n  'Major_Genre': None,\n  'Creative_Type': None,\n  'Director': None,\n  'Rotten_Tomatoes_Rating': nan,\n  'IMDB_Rating': 6.1,\n  'IMDB_Votes': 1071.0},\n {'Title': 'First Love, Last Rites',\n  'US_Gross': 10876.0,\n  'Worldwide_Gross': 10876.0,\n  'US_DVD_Sales': nan,\n  'Production_Budget': 300000.0,\n  'Release_Date': 'Aug 07 1998',\n  'MPAA_Rating': 'R',\n  'Running_Time_min': nan,\n  'Distributor': 'Strand',\n  'Source': None,\n  'Major_Genre': 'Drama',\n  'Creative_Type': None,\n  'Director': None,\n  'Rotten_Tomatoes_Rating': nan,\n  'IMDB_Rating': 6.9,\n  'IMDB_Votes': 207.0},\n {'Title': 'I Married a Strange Person',\n  'US_Gross': 203134.0,\n  'Worldwide_Gross': 203134.0,\n  'US_DVD_Sales': nan,\n  'Production_Budget': 250000.0,\n  'Release_Date': 'Aug 28 1998',\n  'MPAA_Rating': None,\n  'Running_Time_min': nan,\n  'Distributor': 'Lionsgate',\n  'Source': None,\n  'Major_Genre': 'Comedy',\n  'Creative_Type': None,\n  'Director': None,\n  'Rotten_Tomatoes_Rating': nan,\n  'IMDB_Rating': 6.8,\n  'IMDB_Votes': 865.0},\n {'Title': \"Let's Talk About Sex\",\n  'US_Gross': 373615.0,\n  'Worldwide_Gross': 373615.0,\n  'US_DVD_Sales': nan,\n  'Production_Budget': 300000.0,\n  'Release_Date': 'Sep 11 1998',\n  'MPAA_Rating': None,\n  'Running_Time_min': nan,\n  'Distributor': 'Fine Line',\n  'Source': None,\n  'Major_Genre': 'Comedy',\n  'Creative_Type': None,\n  'Director': None,\n  'Rotten_Tomatoes_Rating': 13.0,\n  'IMDB_Rating': nan,\n  'IMDB_Votes': nan},\n {'Title': 'Slam',\n  'US_Gross': 1009819.0,\n  'Worldwide_Gross': 1087521.0,\n  'US_DVD_Sales': nan,\n  'Production_Budget': 1000000.0,\n  'Release_Date': 'Oct 09 1998',\n  'MPAA_Rating': 'R',\n  'Running_Time_min': nan,\n  'Distributor': 'Trimark',\n  'Source': 'Original Screenplay',\n  'Major_Genre': 'Drama',\n  'Creative_Type': 'Contemporary Fiction',\n  'Director': None,\n  'Rotten_Tomatoes_Rating': 62.0,\n  'IMDB_Rating': 3.4,\n  'IMDB_Votes': 165.0}]\n\n\nLet’s store this new column in the DataFrame:\n\nmovies[\"full_dict\"] = movies.to_dict(orient=\"records\")\nmovies\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\nfull_dict\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n{'Title': 'The Land Girls', 'US_Gross': 146083...\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n{'Title': 'First Love, Last Rites', 'US_Gross'...\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n{'Title': 'I Married a Strange Person', 'US_Gr...\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n{'Title': 'Let's Talk About Sex', 'US_Gross': ...\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n{'Title': 'Slam', 'US_Gross': 1009819.0, 'Worl...\n\n\n\n\n\n\n\nNow, let’s define a function movie_performance that takes a movie’s data dictionary, constructs a prompt, and calls the llm_chat function to get a summary:\n\ndef movie_performance(movie_data):\n    prompt = f\"Considering the following data on this movie {movie_data}, provide a one-paragraph summary of its performance for my report.\"\n    return llm_chat(prompt)\n\nWe’ll vectorize this function so we can apply it to the entire full_dict column:\n\nimport numpy as np\n\n# Vectorize the function to apply it to the DataFrame\nmovie_performance_vec = np.vectorize(movie_performance)\n\nLet’s test our function with an example:\n\n# Example usage\nmovie_performance(\"Name: Kene's Movie, Sales: 100,000 USD\")\n\n\"Kene's Movie has demonstrated a strong market performance, achieving impressive sales of $100,000. This noteworthy figure indicates a solid reception among audiences, reflecting effective marketing strategies and appealing content that resonated with viewers. The success of the film can be attributed to its engaging storyline and perhaps positive word-of-mouth or critical acclaim, contributing to its financial achievement in the competitive film industry. Overall, the sales performance positions Kene's Movie favorably within its genre and suggests potential for continued interest and revenue growth in the future.\"\n\n\nFinally, we’ll apply the vectorized function to generate summaries for each movie:\n\n# Generate summaries for each movie\nmovies[\"llm_summary\"] = movie_performance_vec(movies[\"full_dict\"])\n\nYou can now save the DataFrame with the generated summaries to a CSV file:\n\n# Save the results to a CSV file\nmovies.to_csv(\"movies_output.csv\", index=False)\n\nThis approach allows you to generate detailed summaries for each movie based on its full set of data, which can be incredibly useful for automated reporting and data analysis.\n\n\n\n\n\n\nPractice\n\n\n\n24.11 Practice Q: Weather Summary\nUsing the first 5 rows of the seattle_weather dataset from vega_datasets, create a function that takes all weather columns for a particular day and generates a summary of the weather conditions for that day. The function should use the LLM to generate a one-paragraph summary for a report, considering the data provided. Store the function in a column called weather_summary.\n\nweather = vd.data.seattle_weather().head()\nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\n\n\n\n\n\n\n\n\n# Your code here",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#practice-q-weather-summary",
    "href": "p_ai_LLM_functions.html#practice-q-weather-summary",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.11 Practice Q: Weather Summary",
    "text": "24.11 Practice Q: Weather Summary\nUsing the first 5 rows of the seattle_weather dataset from vega_datasets, create a function that takes all weather columns for a particular day and generates a summary of the weather conditions for that day. The function should use the LLM to generate a one-paragraph summary for a report, considering the data provided. Store the function in a column called weather_summary.\n\nweather = vd.data.seattle_weather().head()\nweather\n\n\n\n\n\n\n\n\ndate\nprecipitation\ntemp_max\ntemp_min\nwind\nweather\n\n\n\n\n0\n2012-01-01\n0.0\n12.8\n5.0\n4.7\ndrizzle\n\n\n1\n2012-01-02\n10.9\n10.6\n2.8\n4.5\nrain\n\n\n2\n2012-01-03\n0.8\n11.7\n7.2\n2.3\nrain\n\n\n3\n2012-01-04\n20.3\n12.2\n5.6\n4.7\nrain\n\n\n4\n2012-01-05\n1.3\n8.9\n2.8\n6.1\nrain\n\n\n\n\n\n\n\n\n# Your code here",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  },
  {
    "objectID": "p_ai_LLM_functions.html#wrap-up",
    "href": "p_ai_LLM_functions.html#wrap-up",
    "title": "24  Using LLMs in Python for Text Generation",
    "section": "24.12 Wrap-up",
    "text": "24.12 Wrap-up\nIn this tutorial, we learned the basics of using OpenAI’s LLMs in Python for text generation, created helper functions, and applied these functions to datasets using vectorization.\nIn the next lesson, we’ll look at structured outputs that allow us to specify the format of the response we want from the LLM. We’ll use this to extract structured data from unstructured text, a common task in data analysis.",
    "crumbs": [
      "Using LLMs in Python",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Using LLMs in Python for Text Generation</span>"
    ]
  }
]